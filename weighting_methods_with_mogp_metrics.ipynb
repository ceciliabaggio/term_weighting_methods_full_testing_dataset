{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epLwHJtZiUMx"
   },
   "source": [
    "# DMOZ weighting methods using Global Recall & Jaccard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1628721517208,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "8U6V_57HOBTY"
   },
   "outputs": [],
   "source": [
    "# testing - para mostrar calculos intermedios\n",
    "debug = True\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa-_NOPyMa4v"
   },
   "source": [
    "Dataset load and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1628721517210,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "Ngocd76rT2rW",
    "outputId": "948dfd04-2a52-4bba-f30d-9a84af3819b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::::::::::::::: Loaded topic_names ::::::::::::::::\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_number</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Top/Science/Math/Number_Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>Top/Sports/Golf/Instruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Top/Business/Industrial_Goods_and_Services/Fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>Top/Society/Issues/Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99</td>\n",
       "      <td>Top/Games/Board_Games/War_and_Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>134</td>\n",
       "      <td>Top/Home/Cooking/Fruits_and_Vegetables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>187</td>\n",
       "      <td>Top/Sports/Cycling/Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>215</td>\n",
       "      <td>Top/Recreation/Pets/Exotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>221</td>\n",
       "      <td>Top/Recreation/Outdoors/Camping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>259</td>\n",
       "      <td>Top/Business/Investing/News_and_Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>289</td>\n",
       "      <td>Top/Health/Conditions_and_Diseases/Skin_Disorders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>350</td>\n",
       "      <td>Top/Health/Reproductive_Health/Birth_Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>363</td>\n",
       "      <td>Top/Science/Environment/Air_Quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>403</td>\n",
       "      <td>Top/Science/Social_Sciences/Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>418</td>\n",
       "      <td>Top/Business/Hospitality/Restaurant_Chains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>428</td>\n",
       "      <td>Top/Shopping/Tobacco/Cigars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>465</td>\n",
       "      <td>Top/Shopping/Jewelry/Theme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>476</td>\n",
       "      <td>Top/Arts/Music/Collecting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>528</td>\n",
       "      <td>Top/Business/Agriculture_and_Forestry/Aquaculture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>529</td>\n",
       "      <td>Top/Science/Astronomy/Solar_System</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>537</td>\n",
       "      <td>Top/Recreation/Birding/Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>538</td>\n",
       "      <td>Top/Society/Religion_and_Spirituality/Pagan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>556</td>\n",
       "      <td>Top/Business/Management/Management_Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>561</td>\n",
       "      <td>Top/Arts/Music/Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>586</td>\n",
       "      <td>Top/Arts/Television/Networks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_number                                         topic_name\n",
       "0              1                     Top/Science/Math/Number_Theory\n",
       "1             25                        Top/Sports/Golf/Instruction\n",
       "2             37  Top/Business/Industrial_Goods_and_Services/Fac...\n",
       "3             58                     Top/Society/Issues/Environment\n",
       "4             99             Top/Games/Board_Games/War_and_Politics\n",
       "5            134             Top/Home/Cooking/Fruits_and_Vegetables\n",
       "6            187                          Top/Sports/Cycling/Travel\n",
       "7            215                         Top/Recreation/Pets/Exotic\n",
       "8            221                    Top/Recreation/Outdoors/Camping\n",
       "9            259              Top/Business/Investing/News_and_Media\n",
       "10           289  Top/Health/Conditions_and_Diseases/Skin_Disorders\n",
       "11           350       Top/Health/Reproductive_Health/Birth_Control\n",
       "12           363                Top/Science/Environment/Air_Quality\n",
       "13           403              Top/Science/Social_Sciences/Economics\n",
       "14           418         Top/Business/Hospitality/Restaurant_Chains\n",
       "15           428                        Top/Shopping/Tobacco/Cigars\n",
       "16           465                         Top/Shopping/Jewelry/Theme\n",
       "17           476                          Top/Arts/Music/Collecting\n",
       "18           528  Top/Business/Agriculture_and_Forestry/Aquaculture\n",
       "19           529                 Top/Science/Astronomy/Solar_System\n",
       "20           537                      Top/Recreation/Birding/Europe\n",
       "21           538        Top/Society/Religion_and_Spirituality/Pagan\n",
       "22           556         Top/Business/Management/Management_Science\n",
       "23           561                             Top/Arts/Music/Reviews\n",
       "24           586                       Top/Arts/Television/Networks"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(':::::::::::::::: Loaded topic_names ::::::::::::::::')\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "df_topics = pd.read_csv (r'topic_names_25.txt', header = None, delimiter = '\\t', dtype={'topic_number': \"Int64\", 'topic_name': str})\n",
    "df_topics.columns = ['topic_number','topic_name']\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1628721517211,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "E6mZrMuyvgQm",
    "outputId": "206c8b3f-fd2b-4926-e2c1-1db8a85f1b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    topic_number                                         topic_name\n",
      "18           528  Top/Business/Agriculture_and_Forestry/Aquaculture\n"
     ]
    }
   ],
   "source": [
    "if debug:\n",
    "  select_name = df_topics.loc[df_topics['topic_number'] == 528]\n",
    "  print(select_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX5OgmE_oyAr"
   },
   "source": [
    "### Dictionary\n",
    "dict_topic_numb_name={topic_number: topic_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1628721517211,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "3PivO1Lhy8rM",
    "outputId": "d50ca483-11cd-4e35-eef8-6071d21413c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Top/Science/Math/Number_Theory', 'Top/Sports/Golf/Instruction', 'Top/Business/Industrial_Goods_and_Services/Factory_Automation', 'Top/Society/Issues/Environment', 'Top/Games/Board_Games/War_and_Politics', 'Top/Home/Cooking/Fruits_and_Vegetables', 'Top/Sports/Cycling/Travel', 'Top/Recreation/Pets/Exotic', 'Top/Recreation/Outdoors/Camping', 'Top/Business/Investing/News_and_Media', 'Top/Health/Conditions_and_Diseases/Skin_Disorders', 'Top/Health/Reproductive_Health/Birth_Control', 'Top/Science/Environment/Air_Quality', 'Top/Science/Social_Sciences/Economics', 'Top/Business/Hospitality/Restaurant_Chains', 'Top/Shopping/Tobacco/Cigars', 'Top/Shopping/Jewelry/Theme', 'Top/Arts/Music/Collecting', 'Top/Business/Agriculture_and_Forestry/Aquaculture', 'Top/Science/Astronomy/Solar_System', 'Top/Recreation/Birding/Europe', 'Top/Society/Religion_and_Spirituality/Pagan', 'Top/Business/Management/Management_Science', 'Top/Arts/Music/Reviews', 'Top/Arts/Television/Networks']\n",
      "\n",
      " {'topic_name': 'Top/Science/Math/Number_Theory'} {'topic_name': 'Top/Business/Agriculture_and_Forestry/Aquaculture'}\n"
     ]
    }
   ],
   "source": [
    "df1 = df_topics[['topic_name']].to_numpy()\n",
    "list_of_topic_names = []\n",
    "for arr in df1:\n",
    "  for topic in arr:  \n",
    "    list_of_topic_names.append(topic)\n",
    "print(list_of_topic_names)\n",
    "\n",
    "\n",
    "\n",
    "### Dictionary of {topic_number: topic_name} \n",
    "dict_topic_numb_name = df_topics.set_index('topic_number').T.to_dict()\n",
    "print(\"\\n\",dict_topic_numb_name[1], dict_topic_numb_name[528])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dnmFMV4JtxV"
   },
   "source": [
    "### Test Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4853,
     "status": "ok",
     "timestamp": 1628721522053,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "7lFMDmaJJs_P",
    "outputId": "eb29d61a-facb-4e6f-e75c-8932f029d7ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (23.2.1)\n",
      "Requirement already satisfied: setuptools in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (68.0.0)\n",
      "Requirement already satisfied: wheel in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (0.41.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (3.5.0)\n",
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/1b/09/88bb47efcc6908d9272410a6f2966a01bb44585aae37c8d190e1bc0aee58/spacy-3.6.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached spacy-3.6.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/tljh/user/lib/python3.7/site-packages (from spacy) (1.21.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/tljh/user/lib/python3.7/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.7/site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.7/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/tljh/user/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/tljh/user/lib/python3.7/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/tljh/user/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/tljh/user/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/tljh/user/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.7/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/tljh/user/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy  \"spacy==3.5.0\"\n",
    "#!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11278,
     "status": "ok",
     "timestamp": 1628721533326,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "05cTm8sBxiZf",
    "outputId": "78e743bb-2c94-4125-ade3-58e60475250f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-07 15:37:47.139849: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-07 15:37:47.299677: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-07 15:37:47.303592: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-07 15:37:47.303620: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-09-07 15:37:48.033204: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-07 15:37:48.033280: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-07 15:37:48.033289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-09-07 15:37:48.970992: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-09-07 15:37:48.971026: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-09-07 15:37:48.971046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyterhub): /proc/driver/nvidia/version does not exist\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from en-core-web-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/tljh/user/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/tljh/user/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.25.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (20.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/tljh/user/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/tljh/user/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/tljh/user/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/tljh/user/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/tljh/user/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.5.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.7/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/tljh/user/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "#python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: protobuf~=3.19.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (3.19.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: streamlit==1.9.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (1.9.0)\n",
      "Requirement already satisfied: altair>=3.2.0 in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (5.0.1)\n",
      "Requirement already satisfied: attrs in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (21.2.0)\n",
      "Requirement already satisfied: blinker in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (1.6.2)\n",
      "Requirement already satisfied: cachetools>=4.0 in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (4.2.2)\n",
      "Requirement already satisfied: click<8.1,>=7.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from streamlit==1.9.0) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (4.5.0)\n",
      "Requirement already satisfied: numpy in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (1.21.6)\n",
      "Requirement already satisfied: packaging in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (20.9)\n",
      "Requirement already satisfied: pandas>=0.21.0 in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (1.2.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (9.2.0)\n",
      "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from streamlit==1.9.0) (3.19.6)\n",
      "Requirement already satisfied: pyarrow in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (12.0.1)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (0.8.1b0)\n",
      "Requirement already satisfied: pympler>=0.9 in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (2.8.1)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (2.25.1)\n",
      "Requirement already satisfied: toml in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (0.10.2)\n",
      "Requirement already satisfied: tornado>=5.0 in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (6.1)\n",
      "Requirement already satisfied: tzlocal in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (4.3.1)\n",
      "Requirement already satisfied: validators in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (0.20.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (3.1.32)\n",
      "Requirement already satisfied: semver in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from streamlit==1.9.0) (3.0.1)\n",
      "Requirement already satisfied: watchdog in /opt/tljh/user/lib/python3.7/site-packages (from streamlit==1.9.0) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from streamlit==1.9.0) (4.1.1)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.7/site-packages (from altair>=3.2.0->streamlit==1.9.0) (3.0.1)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/tljh/user/lib/python3.7/site-packages (from altair>=3.2.0->streamlit==1.9.0) (3.2.0)\n",
      "Requirement already satisfied: toolz in /opt/tljh/user/lib/python3.7/site-packages (from altair>=3.2.0->streamlit==1.9.0) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/tljh/user/lib/python3.7/site-packages (from gitpython!=3.1.19->streamlit==1.9.0) (4.0.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/tljh/user/lib/python3.7/site-packages (from importlib-metadata>=1.4->streamlit==1.9.0) (3.4.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/tljh/user/lib/python3.7/site-packages (from pandas>=0.21.0->streamlit==1.9.0) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.7/site-packages (from python-dateutil->streamlit==1.9.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/tljh/user/lib/python3.7/site-packages (from packaging->streamlit==1.9.0) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/tljh/user/lib/python3.7/site-packages (from requests->streamlit==1.9.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/tljh/user/lib/python3.7/site-packages (from requests->streamlit==1.9.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/tljh/user/lib/python3.7/site-packages (from requests->streamlit==1.9.0) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.7/site-packages (from requests->streamlit==1.9.0) (2021.5.30)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /opt/tljh/user/lib/python3.7/site-packages (from tzlocal->streamlit==1.9.0) (0.1.0.post0)\n",
      "Requirement already satisfied: backports.zoneinfo in /opt/tljh/user/lib/python3.7/site-packages (from tzlocal->streamlit==1.9.0) (0.2.1)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /opt/tljh/user/lib/python3.7/site-packages (from validators->streamlit==1.9.0) (5.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/tljh/user/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.9.0) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.7/site-packages (from jinja2->altair>=3.2.0->streamlit==1.9.0) (2.0.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/tljh/user/lib/python3.7/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.9.0) (0.17.3)\n",
      "Requirement already satisfied: setuptools in /home/jupyter-ceciliabgg@gmail.c-09713/.local/lib/python3.7/site-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.9.0) (68.0.0)\n",
      "Requirement already satisfied: tzdata in /opt/tljh/user/lib/python3.7/site-packages (from pytz-deprecation-shim->tzlocal->streamlit==1.9.0) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf~=3.19.0\n",
    "#!export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "!pip install streamlit==1.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYzXQgfWbd-3"
   },
   "source": [
    "### Clase Texto (Documento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4637,
     "status": "ok",
     "timestamp": 1628721537955,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "Vew9XqHRbcC0",
    "outputId": "90d86b62-b05b-4c04-9eda-35898cb18717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  OK   ] Texto Class loaded.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "def valid_token(token):\n",
    "    return not token.is_stop and token.is_alpha and token.lemma_.isalnum()\n",
    "\n",
    "class Texto(object):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    #categorias = df_topics[['topic_name']].to_numpy()  \n",
    "    # convert to an array of DMOZ topic names, named categorias\n",
    "    \n",
    "    # df1 = df_topics[['topic_name']].to_numpy()\n",
    "    # categorias = []\n",
    "    # for arr in df1:\n",
    "    #   for topic in arr:  \n",
    "    #    categorias.append(topic)\n",
    "    # print(categorias)               # pero tmb tengo dictionary de topic NUM: categoria!!!!!!!!!!!!!!!!!!! ver si no esta de mas esto\n",
    "\n",
    "    \n",
    "    def __init__(self, idx, path, spacy_doc=False):\n",
    "        self.idx = idx \n",
    "        self.path = path\n",
    "        self.topic_number = int(path.split('/')[-2])\n",
    "        self.raw_text = open(path, 'r', encoding='UTF-8').read().lower()           # str.lower() added by Ceci\n",
    "        # all the tokens in the doc, no filtering        \n",
    "        self.tokens = Texto.nlp(self.raw_text) \n",
    "        #self.tokens = Texto.nlp(self.raw_text, disable=['textcat','ner','tagger','parser'])         \n",
    "\n",
    "    # si category_idx no es None, entonces devuelve un array de 0s y 1s.\n",
    "    # 1s en las posiciones donde hay un texto en textos tal que el nro de topico es category_idx.\n",
    "    # si category_idx=None entonces devuelve un array con nros de topicos para cada texto (este array no es de 0s y 1s)\n",
    "    # category_idx DEBE ser ENTERO\n",
    "    @staticmethod\n",
    "    def target(textos, category_idx=None):        \n",
    "        if not category_idx is None:                    \n",
    "            #categoria = dict_topic_numb_name[Texto.topic_number]  # esto me da el nombre del topic            \n",
    "#             if debug:\n",
    "#               for texto in textos:\n",
    "#                 print(texto.topic_number, category_idx, int(texto.topic_number)==int(category_idx))\n",
    "#                 print(dict_topic_numb_name[int(texto.topic_number)])  # esto me da el nombre del topic            \n",
    "            target = np.array([int(texto.topic_number)==int(category_idx) for texto in textos])\n",
    "            # devuelve 1s donde hay doc para ese topico\n",
    "            return target[:,np.newaxis] # devuelve un arreglo de 0s y 1s con 1s si el doc es del topico category_idx\n",
    "        else:      \n",
    "            # el index deberia ser le nro de topico del DF de topic names, no el indice, no?\n",
    "            #categoria2index = dict([(categoria, index) for index,categoria in enumerate(Texto.categorias)])\n",
    "            #print(\"categoria2index\", categoria2index)\n",
    "            target = np.array([int(texto.topic_number) for texto in textos])  # devuelve un arreglo de TOPIC_IDs segun documentos en textos (IDS pueden ser distintos)\n",
    "            return target[:,np.newaxis]\n",
    "        \n",
    "print('[  OK   ] Texto Class loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG5zEjoKd0Ui"
   },
   "source": [
    "## load files from disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1628721537956,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "8dpJyOOF4cdD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files exist. Nothing to extract\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not (os.path.isfile('pickle_texts/dmoz_train_25.p') and os.path.isfile('pickle_texts/dmoz_test_25.p')):\n",
    "\n",
    "    !cp '/set12_train_25topics.tar.gz' .\n",
    "    #unzip dataset\n",
    "    !tar xzf 'set12_train_25topics.tar.gz'\n",
    "\n",
    "    print(':::::::::::::::: EXTRACTED dataset Set12 ::::::::::::::::')\n",
    "\n",
    "    !cp '/set3_test_all_topics.tar.gz' .\n",
    "    #unzip dataset\n",
    "    !tar xzf 'set3_test_all_topics.tar.gz'\n",
    "\n",
    "    print(':::::::::::::::: Extracted dataset Set3 ::::::::::::::::')\n",
    "else:\n",
    "        print(\"Files exist. Nothing to extract\")\n",
    "    \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-AuOi0BfzqL"
   },
   "source": [
    "\n",
    "## Load Train & Test from pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190556,
     "status": "ok",
     "timestamp": 1628721728473,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "uK5G2j0QfzWT",
    "outputId": "7a69ce0f-71bf-4c4b-fbff-45d4b2e22aa2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO  ] Retrieve train set from pickles\n",
      "[ INFO  ] Retrieve test set from pickles\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "#!mkdir -p db/cache\n",
    "if not (os.path.isfile('pickle_texts/dmoz_train_25.p')):\n",
    "    \n",
    "    !mkdir -p 'pickle_texts'\n",
    "    print('[ INFO  ] Reading raw text and transforming it into object Texto.')\n",
    "\n",
    "    #path_train = 'set12_train_3_topics/'\n",
    "    path_train = 'set12_train'\n",
    "\n",
    "\n",
    "    idx = 0\n",
    "    train = []\n",
    "    for dirpath,_,filenames in os.walk(path_train):\n",
    "        print(\"topic: \", dirpath)\n",
    "        for filename in filenames:\n",
    "              train.append(Texto(idx, os.path.join(dirpath, filename)))\n",
    "              idx += 1\n",
    "              print(filename)\n",
    "    pickle.dump(train, open('pickle_texts/dmoz_train_25.p','wb'))\n",
    "    print('[ INFO  ] Save train set into pickles')\n",
    "    print('train set size: {}'.format(len(train)))\n",
    "    \n",
    "else:\n",
    "    print('[ INFO  ] Retrieve train set from pickles')    \n",
    "    train = pickle.load(open('pickle_texts/dmoz_train_25.p', 'rb'))    \n",
    "    \n",
    "\n",
    "\n",
    "if not (os.path.isfile('pickle_texts/dmoz_test_25.p')):    \n",
    "    idx = 0\n",
    "    test = []\n",
    "    #path_test = 'set3_test_3_topics/'\n",
    "    path_test = 'set3_test'\n",
    "    for dirpath,_,filenames in os.walk(path_test):\n",
    "        print(\"topic: \", dirpath)\n",
    "        for filename in filenames:\n",
    "            test.append(Texto(idx, os.path.join(dirpath, filename)))\n",
    "            idx += 1\n",
    "    pickle.dump(test, open('pickle_texts/dmoz_test_25.p','wb'))\n",
    "    print('[ INFO  ] Save test set into pickles')\n",
    "    print('test set size:  {}'.format(len(test)))\n",
    "    \n",
    "else:\n",
    "    print('[ INFO  ] Retrieve test set from pickles')    \n",
    "    test = pickle.load(open('pickle_texts/dmoz_test_25.p', 'rb'))\n",
    "    \n",
    "print('[  OK   ] train/test sets loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FrFhg9x9f-q"
   },
   "source": [
    "# Matrix and vocabulary building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKeQcspf9z3I"
   },
   "source": [
    "**Trigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44113,
     "status": "ok",
     "timestamp": 1628721772578,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "Js4tZptz8zw2",
    "outputId": "c2302e80-4c39-474b-a4ff-25e6d7bd5fc3"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def valid_token(token):\n",
    "    return not token is None and not token.is_stop and token.is_alpha and token.lemma_.isalnum() and not Texto.nlp.vocab[token.lemma_].is_stop\n",
    "\n",
    "def build_trigrams(textos, umbral=15):\n",
    "    freq = defaultdict(int)\n",
    "    for texto in textos:\n",
    "        \n",
    "        visited = set()\n",
    "        for idx in range(len((texto.tokens[:-2]))):\n",
    "            t1 = texto.tokens[idx]\n",
    "            t2 = texto.tokens[idx+1]\n",
    "            t3 = texto.tokens[idx+2]\n",
    "            \n",
    "            if valid_token(t1) and valid_token(t2) and valid_token(t3) and not (t1.lemma_,t2.lemma_,t3.lemma_) in visited:\n",
    "                freq[(t1.lemma_,t2.lemma_,t3.lemma_)] +=1 \n",
    "                visited.add((t1.lemma_,t2.lemma_,t3.lemma_))\n",
    "    return [word for word in freq if freq[word]>umbral]\n",
    "            \n",
    "####### CON TRAIN Y TEST PARA ARMAR LA MATRIZ DE VOCABULARIO (INDICE)\n",
    "trigrams = build_trigrams(train+test, umbral =35)                            # TEST: umbral=1, aparece al menos 2 veces)\n",
    "print('[  OK   ] Trigrams computed: {}'.format(len(trigrams)))\n",
    "#print(\"some trigrams: \", trigrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpZzAygN-DuM"
   },
   "source": [
    "**Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45445,
     "status": "ok",
     "timestamp": 1628721818007,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "WWnXJ8M8-Br5",
    "outputId": "81e5fa42-5398-431a-fd1a-2892d5e60b91"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_bigrams(textos,vocab, umbral=15):\n",
    "    freq = defaultdict(int)\n",
    "    for texto in textos:\n",
    "        visited = set()\n",
    "        for idx in range(len((texto.tokens[:-1]))):\n",
    "            t1 = texto.tokens[idx] \n",
    "            t2 = texto.tokens[idx+1]\n",
    "            t3 = texto.tokens[idx+2] if idx+2<len(texto.tokens) else None\n",
    "            \n",
    "                \n",
    "            \n",
    "            if valid_token(t1) and valid_token(t2) and not (not t3 is None and (t1.lemma_,t2.lemma_,t3.lemma_) in vocab) and not (t1.lemma_,t2.lemma_) in visited:\n",
    "                freq[(t1.lemma_,t2.lemma_)] +=1 \n",
    "                visited.add((t1.lemma_,t2.lemma_))\n",
    "\n",
    "    return [word for word in freq if freq[word]>umbral]\n",
    "            \n",
    "vocab = set(trigrams)\n",
    "####### CON TRAIN Y TEST PARA ARMAR LA MATRIZ DE VOCABULARIO (INDICE)\n",
    "bigrams = build_bigrams(train+test,vocab, umbral =35)                            # TEST: umbral=1, aparece al menos 2 veces)\n",
    "print('[  OK   ] Bigrams computed: {}'.format(len(bigrams)))\n",
    "#print(\"some bigrams: \", bigrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycKq9dEC-NTG"
   },
   "source": [
    "**Unigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69640,
     "status": "ok",
     "timestamp": 1628721887639,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "SoPTQXGV-QIu",
    "outputId": "513df88e-97ee-41d1-f57c-3551be569987"
   },
   "outputs": [],
   "source": [
    "def build_unigrams(textos,vocab, umbral=15):\n",
    "    freq = defaultdict(int)\n",
    "    for texto in textos:\n",
    "        visited = set()\n",
    "        for idx in range(len((texto.tokens))):\n",
    "            t1 = texto.tokens[idx]             \n",
    "            t2 = texto.tokens[idx+1] if idx+1<len(texto.tokens) else None            \n",
    "            t3 = texto.tokens[idx+2] if idx+2<len(texto.tokens) else None            \n",
    "            \n",
    "            if not t2 is None and (t1.lemma_,t2.lemma_) in vocab:\n",
    "                valid_bigram = True\n",
    "            else:\n",
    "                valid_bigram = False\n",
    "            \n",
    "            if not t3 is None and (t1.lemma_,t2.lemma_,t3.lemma_) in vocab:\n",
    "                valid_trigram=True\n",
    "            else:\n",
    "                valid_trigram=False                \n",
    "            \n",
    "            if valid_token(t1) and valid_token(t2) and not valid_bigram and not valid_trigram and not (t1.lemma_,) in visited:\n",
    "                freq[(t1.lemma_,)] +=1 \n",
    "                visited.add((t1.lemma_,))\n",
    "\n",
    "    return [word for word in freq if freq[word]>umbral]\n",
    "vocab = set(trigrams+bigrams)\n",
    "\n",
    "####### CON TRAIN Y TEST PARA ARMAR LA MATRIZ DE VOCABULARIO (INDICE)\n",
    "unigrams = build_unigrams(train+test, vocab,  umbral =35)        # TEST: umbral=1, aparece al menos 1 vez\n",
    "\n",
    "#unigrams = unigrams[0:20]\n",
    "print('[  OK   ] Unigrams computed: {}'.format(len(unigrams)))\n",
    "#print(\"some unigrams:\", unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5AqI1E6_V-g"
   },
   "source": [
    "**vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1628721887639,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "NL2inBuC_VPc",
    "outputId": "9f68597d-a097-4096-e981-8403db2255a6"
   },
   "outputs": [],
   "source": [
    "vocab = list(set(unigrams+bigrams+trigrams))\n",
    "print('[  OK   ] Vocab computed: {}'.format(len(vocab)))\n",
    "\n",
    "# if debug:  \n",
    "#   print(\"\\n\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75bs9Jr6_sjw"
   },
   "source": [
    "**Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48543,
     "status": "ok",
     "timestamp": 1628721936173,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "HFNGF69d_u-4",
    "outputId": "81fd9b85-7539-4937-caab-ca82853346c4"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def build_matrix(textos, vocab):\n",
    "    m = np.zeros(shape=(len(textos),len(vocab)))\n",
    "    word2index = dict([(word,index) for index,word in enumerate(vocab)])    \n",
    "    # dict_df[termino]= nro de ocurrencias en docs del corpus. \n",
    "    dict_df = dict([(word,0) for word in vocab])\n",
    "        \n",
    "    for idx, text in enumerate(textos):\n",
    "        \n",
    "        set_tokens=set([]) # elimina repetidos - para saber si el token esta o no en el doc - calcular DF\n",
    "        \n",
    "        for tkn_idx in range(len(text.tokens)):\n",
    "            t1 = text.tokens[tkn_idx].lemma_\n",
    "            t2 = text.tokens[tkn_idx + 1].lemma_ if tkn_idx+1<len(text.tokens) else None\n",
    "            t3 = text.tokens[tkn_idx + 2].lemma_ if tkn_idx+2<len(text.tokens) else None\n",
    "            \n",
    "            unigram = (t1,)\n",
    "            bigram = (t1,t2) if not t2 is None else None\n",
    "            trigram = (t1,t2,t3) if not t3 is None and not t2 is None else None\n",
    "            \n",
    "            if not trigram is None and trigram in word2index:\n",
    "                m[idx,word2index[trigram]] += 1\n",
    "                set_tokens.add(trigram)\n",
    "            elif not bigram is None and bigram in word2index:                \n",
    "                m[idx, word2index[bigram]]+=1\n",
    "                set_tokens.add(bigram)\n",
    "            elif unigram in word2index:\n",
    "                m[idx,word2index[unigram]]+=1\n",
    "                set_tokens.add(unigram)\n",
    "        \n",
    "        for t in set_tokens:\n",
    "            dict_df[t]+=1\n",
    "    \n",
    "    ################################\n",
    "    ########## calculo matriz TF-IDF\n",
    "    ################################\n",
    "\n",
    "    # print(dict_df)\n",
    "    #print (\"dict_df[\",vocab[0],\"]=\",dict_df[vocab[0]], \"dict_df[\",vocab[1],\"]=\",dict_df[vocab[1]],\"vocab[2]=\",dict_df[vocab[2]])\n",
    "    \n",
    "    m_tfIdf = copy.deepcopy(m)\n",
    "    \n",
    "    corpus_size= len(textos)\n",
    "    \n",
    "    # multiplica cada columna (que tiene el TF) por el IDF del termino. \n",
    "    # El termino es la columna de la matriz\n",
    "    for indice in range(0, len(vocab)):\n",
    "        m_tfIdf[:, indice] = m_tfIdf[:,indice] * np.log(corpus_size / (dict_df[vocab[indice]] + 1 ) )\n",
    "        #m_tfIdf[:, indice] = m_tfIdf[:,indice] * 2  ## test              \n",
    "    \n",
    "    \n",
    "    return m, m_tfIdf\n",
    "\n",
    "m_test, m_tfIdf_test = build_matrix(test, vocab)\n",
    "m_train, m_tfIdf_train = build_matrix(train, vocab)\n",
    "t_testing = Texto.target(test)\n",
    "t_training = Texto.target(train)\n",
    "print('training matrix: {}'.format(m_train.shape))\n",
    "print('testing matrix: {}'.format(m_test.shape))\n",
    "\n",
    "# print('training matrix columna 0: {} equivale al primer termino de la matriz'.format(m_train[:, 0].shape))\n",
    "# f = open(\"original.txt\", \"w+\")\n",
    "# print(\"original=\")\n",
    "# print(m_train[0,:])\n",
    "\n",
    "# for a in range(0,len(m_train[0,:])):    \n",
    "#     f.write(str(m_train[0,a]))\n",
    "#     f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# f.close()\n",
    "\n",
    "# print(\"tfidf=\")\n",
    "\n",
    "# f1 = open(\"tfidf.txt\", \"w+\")\n",
    "# for a in range(0,len(m_tfIdf_train[0,:])):    \n",
    "#     f1.write(str(m_tfIdf_train[0,a]))\n",
    "#     f1.write(\"\\n\")\n",
    "    \n",
    "# f1.close()\n",
    "print(m_tfIdf_train[0,:])\n",
    "print(\"matriz con columna 0 ordenada de mayor a menor:\")\n",
    "\n",
    "## ordenamiento de columna (devuelve indices ordenados):\n",
    "indicesDeDocus = np.argsort(m_tfIdf_train[:,0])\n",
    "n=10\n",
    "#los primeros 10 de mayor a menor\n",
    "indicesDeDocus = indicesDeDocus[::-1][:n]\n",
    "\n",
    "print (\"TERMINO \", vocab[0])\n",
    "for i in range(0,9):\n",
    "    print(\"indice del documento recuperado \", indicesDeDocus[i])      \n",
    "    print(\"valor tf-idf \", m_tfIdf_train[indicesDeDocus[i], 0])\n",
    "    print(\"topico al que pertenece del train \", t_training[indicesDeDocus[i]])\n",
    "    print(\" ----------- \")\n",
    "\n",
    "print()\n",
    "print(\"vocab[0]=\", vocab[0])\n",
    "\n",
    "print('training target: {}'.format(t_training.shape))\n",
    "print('testing target: {}'.format(t_testing.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1628721937403,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "oXxpwGXGDGzE",
    "outputId": "f8ab94cc-7ee3-4f89-e2e7-7076724873ae"
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "  print(\"vector de topicos de cada texto/Documento en la matriz TRAIN\")\n",
    "  #print(t_training)\n",
    "  print(\"\\nmatriz de TRAIN\")\n",
    "  #print(m_train)\n",
    "  print(\"\\nterminos en archivos de TRAIN a traves de la matriz\")\n",
    "  non_zero = np.argwhere(m_train != 0)\n",
    "  print(non_zero)\n",
    "  print(\"\\nterminos en archivos de TEST a traves de la matriz\")\n",
    "  non_zero = np.argwhere(m_test != 0)\n",
    "\n",
    "  print(non_zero)\n",
    "\n",
    "    \n",
    "\n",
    "  print(vocab[0])\n",
    "  print(vocab[1])\n",
    "  print(vocab[2])\n",
    "  print(vocab[3])\n",
    "  print(vocab[4])\n",
    "  print(vocab[5])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl-7xQ1IMdPi"
   },
   "source": [
    "# Term Weighting schemes\n",
    "\n",
    "### The source code of the term-weighting methods has been provided by Mariano Maisonnave (https://cs.uns.edu.ar/~mmaisonnave/)\n",
    "\n",
    "• A denotes the number of documents that belong to class c_k and **contain** term t_i\n",
    "\n",
    "• B denotes the number of documents that belong to class c_k **but do not contain** the term t_i\n",
    "\n",
    "• C denotes the number of documents that **do not belong** to class c_k but **contain** the term t_i\n",
    "\n",
    "• D denotes the number of documents that **do not belong** to class c_k and **do not contain** the term t_i\n",
    "\n",
    "• N denotes the total number of documents in the collection (i.e., N = A + B + C + D).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1628721937404,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "yWXpjT2lMc3z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "eps = np.finfo(np.float64).eps\n",
    "eps\n",
    "\n",
    "# param matrix, matriz de Train o Test\n",
    "# param target: arreglo booleano. cant elementos como textos en matriz\n",
    "# TRUE, si el indice de la matrix corresponde a un docum del topico en cuestion, \n",
    "# FALSE en caso contrario\n",
    "def get_abcd(matrix, target):\n",
    "    m_bool = matrix.copy() > 0\n",
    "    \n",
    "    a = np.sum( m_bool &  target, axis=0)\n",
    "    b = np.sum(~m_bool &  target, axis=0)\n",
    "    c = np.sum( m_bool & ~target, axis=0)\n",
    "    d = np.sum(~m_bool & ~target, axis=0)\n",
    "    return a.reshape(1,len(a)),b.reshape(1,len(b)),c.reshape(1,len(c)),d.reshape(1,len(d))\n",
    "\n",
    "\n",
    "def get_best_word_method(matrix,target, category_idx, vocab, method, beta=0.477, testing=False):\n",
    "    # a y c son arreglos de longitud cantidad de terminos (vocab)-y cada elemento del arreglo representa el nro de documentos del tema y que tienen el termino (en A) \n",
    "    # C docus de otros topicos pero que si tienen el termibno\n",
    "    a,b,c,d=get_abcd(matrix,target==category_idx)\n",
    "    \n",
    "    score = []\n",
    "    if method.__name__ == 'MOGP':\n",
    "        print(\"no calcular nada, ya esta almacenado! -- ver que hacer\")\n",
    "    elif method.__name__ == 'fdd_05':\n",
    "        score = fdd(a,b,c,d,beta=0.5)\n",
    "    elif method.__name__ == 'fdd_1':\n",
    "        score = fdd(a,b,c,d,beta=1.0)\n",
    "    elif method.__name__ == 'fdd_10':\n",
    "        score = fdd(a,b,c,d,beta=10)        \n",
    "    elif \"igm\" in method.__name__:        \n",
    "        if testing:\n",
    "            t_testing = Texto.target(test) #al no pasar 2do argumento a target, devuelve los nros de topicos de cada doc\n",
    "            score = method(matrix,t_testing, category_idx )  # checked, usa category que es topic_number\n",
    "        else:                    \n",
    "            t_training = Texto.target(train)\n",
    "            score = method(matrix,t_training, category_idx )  # checked, usa category que es topic_number\n",
    "    else:\n",
    "        score = method(a,b,c,d)  \n",
    "    \n",
    "    words_100 = []\n",
    "    \n",
    "    print(\"score: \", score)\n",
    "    \n",
    "    \n",
    "    # score son punteros a los indices del arreglo de palabras\n",
    "    cant = 100\n",
    "    #cant=5\n",
    "    \n",
    "    score = np.argsort(score)[0]\n",
    "    print(\"np.argsort(score)[0,:]: \", score)\n",
    "    \n",
    "    for s in range(1, cant+1):\n",
    "        # score es de la forma [[ 1,2,3]] por eso el primer indice es 0\n",
    "        words_100.append(vocab[score[-s]])\n",
    "\n",
    "        #print(\"palabra \",-s, \" \",  vocab[np.argsort(score)[0,-s]], \" score \", np.argsort(score)[0,-s])\n",
    "    \n",
    "    \n",
    "    # las palabras ya las tengo en el orden correcto. El score esta desordenado\n",
    "   \n",
    "    return words_100, score\n",
    "\n",
    "\n",
    "# SOLO SIRVE PARA FDD\n",
    "def get_best_word(matrix,target, category_idx, vocab,beta=0.477):\n",
    "\n",
    "    a,b,c,d = get_abcd(matrix,target==category_idx)\n",
    "    score = fdd(a,b,c,d,beta=beta)\n",
    "    \n",
    "    # score es un arreglo con tantas filas como documentos del topico C_k que contienen al termino t_i\n",
    "    # print(\"lo que tiene score = \", score)\n",
    "\n",
    "    words_100 = []\n",
    "    \n",
    "    cant = 100\n",
    "    #cant=5\n",
    "    for s in range(1, cant+1):\n",
    "        words_100.append(vocab[np.argsort(score)[0,-s]])\n",
    "        #print(\"palabra \",-s, \" \",  vocab[np.argsort(score)[0,-s]], \" score \", np.argsort(score)[0,-s])\n",
    "    \n",
    "    #return vocab[np.argsort(score)[0,-1]]\n",
    "    return words_100\n",
    "\n",
    "#en este caso, target es un arreglo de nro de topicos, corresponde al topic_id de cada doc de la matriz\n",
    "def get_ir_metrics(word, matrix, m_tfIdf, target, category_idx, vocab):\n",
    "    word_idx = vocab.index(word)\n",
    "    \n",
    "    vec = matrix[:,word_idx]\n",
    "    \n",
    "    #     for indi in range(0, len(vec)):\n",
    "    #         if vec[indi]>1:\n",
    "    #             print (vec[indi])\n",
    "    \n",
    "    cant_recuperados = np.sum(vec>0)\n",
    "    cant_relevantes = np.sum(target==category_idx)\n",
    "    \n",
    "\n",
    "    cant_relevantes_recuperados = np.sum((vec>0) & (target[:,0]==category_idx))\n",
    "    relevantes_recuperados = (vec>0) & (target[:,0]==category_idx)\n",
    "    \n",
    "    # para chequear si se estaban calculando bien los relevantes recuperados\n",
    "    #     relev_recuperados_a_mano=0\n",
    "    #     for t in range(0, vec.shape[0]):\n",
    "    #         if(vec[t]) and ((target[t,0]==category_idx)):\n",
    "    #             relev_recuperados_a_mano+=1\n",
    "            \n",
    "    ## ordenamiento de columna (devuelve indices ordenados):\n",
    "    indicesDeDocusA10 = np.argsort(m_tfIdf[:,word_idx])\n",
    "    n=10\n",
    "    #los primeros 10 de mayor a menor\n",
    "    indicesDeDocusA10 = indicesDeDocusA10[::-1][:n]\n",
    "\n",
    "    # print (\"TERMINO \", word)\n",
    "    \n",
    "#     for i in range(0,n-1):\n",
    "#         print(\"indice del documento recuperado \", indicesDeDocusA10[i])      \n",
    "#         print(\"valor tf-idf \", m_tfIdf[indicesDeDocusA10[i], word_idx])\n",
    "#         print(\"topico al que pertenece \", target[indicesDeDocusA10[i]])\n",
    "#         print(\" ----------- \")    \n",
    "    \n",
    "    cant_relevantes_recuperados_at_10 = 0\n",
    "    cant_recuperados_at_10 = 0\n",
    "    for i in range(0,n):\n",
    "        if (m_tfIdf[indicesDeDocusA10[i], word_idx]>0):\n",
    "            cant_recuperados_at_10 += 1\n",
    "            # si el TF-IDF no es cero entonces me fijo si es del topico o no...\n",
    "            if (target[indicesDeDocusA10[i]]==category_idx):\n",
    "                cant_relevantes_recuperados_at_10 += 1\n",
    "  \n",
    "       \n",
    "    # precision clasica\n",
    "    #precision = cant_relevantes_recuperados/max(cant_recuperados,1.0)\n",
    "    \n",
    "    #precision a 10\n",
    "    precision = cant_relevantes_recuperados_at_10/max(cant_recuperados_at_10, 1.0)\n",
    "    \n",
    "    recall = cant_relevantes_recuperados/cant_relevantes\n",
    "    f1 = 2.0*((precision*recall)/(precision+recall+eps))\n",
    "\n",
    "    #     print(\"Precision = \", precision)\n",
    "    #     print(\"Recall = \", recall)\n",
    "    \n",
    "    # devuelvo cuales son los relevantes_recuperados para calcular GLOBAL RECALL\n",
    "    return precision, recall, f1, relevantes_recuperados\n",
    "\n",
    "# los relevantes_recuperados son de todas las 100 queries\n",
    "def globalRecall(relevantes_recuperados, target, category, vocab):    \n",
    "    cant_relevantes = np.sum(target==category)    \n",
    "    if cant_relevantes!=0:\n",
    "        global_recall = np.sum(relevantes_recuperados) / cant_relevantes\n",
    "    else:\n",
    "        global_recall = 0.0\n",
    "    return global_recall\n",
    "\n",
    "\n",
    "# best_100_words: las mejores 100 palabras (--> QUERIES) elegidas segun el metodo a probar \n",
    "# matrix: o bien es la matriz de train o la de test. cada celda i,j tiene la cant de veces que el termino j aparece en el doc i\n",
    "# relevantes recuperados: vector booleano-indica cuales son los docs relev recuperados\n",
    "# target (train o test): arreglo de numeros de topicos, para saber a que topico pertenece cada True/false del arreglo anterior\n",
    "# category: topic_number\n",
    "# vocab: todos los posibles terminos del dataset\n",
    "def get_ir_poblational_metrics(best_100_words, matrix, relevantes_recuperados, target, category, vocab):\n",
    "     \n",
    "    #print(\"calculando metrica poblacional GLOBAL RECALL\")\n",
    "    global_recall = globalRecall(relevantes_recuperados, target, category, vocab)\n",
    "    #print(\"calculando metrica poblacional JACCARD\")\n",
    "    jaccard_similarity = JaccardSimilarity(best_100_words, matrix, target, category, vocab)\n",
    "    \n",
    "    return global_recall, jaccard_similarity\n",
    "    \n",
    "# calcula average final\n",
    "def JaccardSimilarity(queries, matrix, target, category_idx, vocab):\n",
    "    \n",
    "    jaccard_sim = 0.0\n",
    "    \n",
    "    num=0\n",
    "    \n",
    "    # calcular indiceSimJaccard entre cada par de queries\n",
    "    for indx_i, q_i in enumerate(queries):\n",
    "        for indx_j, q_j in enumerate(queries):\n",
    "            num+=1\n",
    "            #print(\"par de queries: \", num)\n",
    "            #print(\"query i: \", q_i, \"\\nquery j: \", q_j )\n",
    "            \n",
    "            jaccard_sim += JaccardSimilarityIndex(q_i, q_j, matrix, target, category_idx, vocab)                \n",
    "            \n",
    "            #print(\"indice sim Jaccard ACUMULADO: \", jaccard_sim)         \n",
    "    #return jaccard_sim/(len(queries)*(len(queries)-1))            # no estoy omitiendo las queries cuando i=j\n",
    "    return jaccard_sim/(len(queries)*len(queries))\n",
    "\n",
    "def JaccardSimilarityIndex(q_i, q_j, matrix, target,category_idx, vocab):\n",
    "    # genero vector de documentos recuperados por cada query\n",
    "    word_idx_q_i = vocab.index(q_i)    \n",
    "    vec_q_i = matrix[:,word_idx_q_i]\n",
    "    \n",
    "    word_idx_q_j = vocab.index(q_j)    \n",
    "    vec_q_j = matrix[:,word_idx_q_j]\n",
    "    \n",
    "    RR_q_i = (vec_q_i>0) & (target[:,0]==category_idx)\n",
    "    relevantes_recuperados_q_i = np.array(RR_q_i)      #conversion a arreglo Numpy\n",
    "           \n",
    "    RR_q_j = (vec_q_j>0) & (target[:,0]==category_idx)\n",
    "    relevantes_recuperados_q_j = np.array(RR_q_j)\n",
    "                       \n",
    "    \n",
    "    # FALTARIA quedarme con el TOPIC ID tomandolo desde train[index].path y ver la interseccion de los topic_ids\n",
    "    # pero esto se hace hacer como el AND entre los vectores booleanos!\n",
    "    \n",
    "    #print(\"arreglo relevantes recuperados q_i y q_j: \")\n",
    "    #print (relevantes_recuperados_q_i & relevantes_recuperados_q_j)\n",
    "    interseccion_relevantes_recuperados = np.sum(relevantes_recuperados_q_i & relevantes_recuperados_q_j)\n",
    "    union_relevantes_recuperados = np.sum(relevantes_recuperados_q_i | relevantes_recuperados_q_j)\n",
    "    #print (\"interseccion = \", interseccion_relevantes_recuperados)\n",
    "    #print (\"union = \", union_relevantes_recuperados)\n",
    "    \n",
    "    \n",
    "    if (interseccion_relevantes_recuperados == 0 | union_relevantes_recuperados==0):\n",
    "        return 1.0\n",
    "    else:\n",
    "        return float(interseccion_relevantes_recuperados/union_relevantes_recuperados)\n",
    "\n",
    "    \n",
    "def fdd(a,b,c,d,beta=0.477):\n",
    "    toReturn = np.zeros(shape=(1,a.shape[1]))\n",
    "    p = discr(a,b,c,d)\n",
    "    r = descr(a,b,c,d)\n",
    "    num = (1+beta**2)*p*r\n",
    "    den = (beta**2)*p+r\n",
    "    mask = (p!=0) & (r!=0)\n",
    "    toReturn[mask] = num[mask]/den[mask]\n",
    "    return toReturn#/np.max(toReturn)\n",
    "def discr(a,b,c,d):\n",
    "    solucion = np.zeros(shape=a.shape)\n",
    "    mask = (a+c)!=0\n",
    "    solucion[mask] = a[mask]/(a[mask]+c[mask])\n",
    "    return solucion\n",
    "    #return a/(a+c)\n",
    "def descr(a,b,c,d):\n",
    "    solucion = np.zeros(shape=a.shape)\n",
    "    mask = (a+b)!=0\n",
    "    solucion[(a+b)!=0] = a[mask]/(a[mask]+b[mask])\n",
    "    return solucion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZDWRtPvvxKT"
   },
   "source": [
    "# FDD definition and auxiliary function - Pre-compute data and store in FDD-CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9731,
     "status": "ok",
     "timestamp": 1628721947125,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "NMBamBKNSyoF",
    "outputId": "37e76d61-f193-4b30-a96e-622e3cf15186"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "!mkdir 'FDD-CSVs'\n",
    "\n",
    "all_topics = dict_topic_numb_name.keys()\n",
    "print(all_topics)\n",
    "some_topics = [215, 1, 529, 528]                           #### NOT ALL TOPICS\n",
    "\n",
    "topics = all_topics\n",
    "\n",
    "for category in topics:\n",
    "    topic_name = dict_topic_numb_name[category]['topic_name']\n",
    "    topic_name = topic_name.replace(\"/\", \"-\")  \n",
    "    #topic_name = Texto.categorias[category].replace(\"/\", \"-\")   \n",
    "    print(\"Topic name \", topic_name, \"\\n\")\n",
    "\n",
    "    writer = open('FDD-CSVs/{}_{}.csv'.format(topic_name, category), \"w\")\n",
    "    #writer = open('FDD-CSVs/{}_{}.csv'.format(Texto.categorias[category],category), 'w')\n",
    "    writer.write('{},{},{},{},{},{},{},{}\\n'.format(\n",
    "        'beta',\n",
    "        'termino',\n",
    "        'precision_train',\n",
    "        'recall_train',\n",
    "        'f1_train',\n",
    "        'precision_test',\n",
    "        'recall_test',\n",
    "        'f1_test'\n",
    "    ))\n",
    "    \n",
    "    writer_poblational = open('FDD-CSVs/{}_{}_poblational.csv'.format(topic_name, category), \"w\")\n",
    "    writer_poblational.write('{},{},{},{},{},{}\\n'.format(\n",
    "        'topic_name',\n",
    "        'topic_number',\n",
    "        'global_recall_train',\n",
    "        'global_recall_test',\n",
    "        'jaccard_sim_train',\n",
    "        'jaccard_sim_test'\n",
    "    ))\n",
    "\n",
    "    beta = 0.477\n",
    "    best_100_words = get_best_word(m_train,t_training, category, vocab,beta=beta)\n",
    "        \n",
    "    total_RR_train = []\n",
    "    total_RR_test = []\n",
    "    pr1 = 0.0\n",
    "    re1 = 0.0\n",
    "    fs1 = 0.0\n",
    "    pr2 = 0.0\n",
    "    re2 = 0.0\n",
    "    fs2 = 0.0\n",
    "    RR1 = []\n",
    "    RR2 = []\n",
    "    \n",
    "    for word in best_100_words:                \n",
    "        #RR: relevantes_recuperados por cada termino-vectores de Bool - tantas filas como docus en train o test\n",
    "        pr1,re1,fs1, RR1 = get_ir_metrics(word, m_train, m_tfIdf_train, t_training, category, vocab)\n",
    "        \n",
    "        if len(total_RR_train)==0:\n",
    "            total_RR_train = RR1\n",
    "        else:            \n",
    "            total_RR_train = (total_RR_train | RR1)            \n",
    "        #print(\"termino= \", word, \" total_RR_train \\t\" ,np.sum(total_RR_train) )\n",
    "        \n",
    "\n",
    "        # Para el TEST son las mismas queries\n",
    "        pr2,re2,fs2, RR2 = get_ir_metrics(word, m_test, m_tfIdf_test, t_testing, category, vocab)\n",
    "        \n",
    "        if len(total_RR_test) == 0:\n",
    "            total_RR_test = RR2\n",
    "        else:            \n",
    "            total_RR_test = (total_RR_test | RR2)\n",
    "        #print(\"termino= \", word,\" total_RR_test \\t\", np.sum(total_RR_test)) \n",
    "        \n",
    "        writer.write('{},{},{},{},{},{},{},{}\\n'.format(\n",
    "            beta,\n",
    "            ' '.join([w for w in word]),\n",
    "            pr1,\n",
    "            re1,\n",
    "            fs1,\n",
    "            pr2,\n",
    "            re2,\n",
    "            fs2\n",
    "        ))\n",
    "    \n",
    "    # global recall & jaccard similarity(ploblational)\n",
    "    \n",
    "    gr1, js1 = get_ir_poblational_metrics(best_100_words, m_train, total_RR_train, t_training, category, vocab)    \n",
    "    \n",
    "    gr2, js2 = get_ir_poblational_metrics(best_100_words, m_test, total_RR_test, t_testing, category, vocab)\n",
    "        \n",
    "    writer_poblational.write('{},{},{},{},{},{}\\n'.format(\n",
    "        topic_name,\n",
    "        category,        \n",
    "        gr1,\n",
    "        gr2,\n",
    "        js1,\n",
    "        js2\n",
    "\n",
    "    ))        \n",
    "\n",
    "    writer.close()\n",
    "    writer_poblational.close()\n",
    "    gr1 = 0.0\n",
    "    gr2 = 0.0\n",
    "    js1 = 0.0\n",
    "    js2 = 0.0\n",
    "    print(\"end topic \", category, \" --------------------------------------- \")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida = False\n",
    "if salida:\n",
    "    a = np.array([True, False, True])\n",
    "    print(np.sum(a))\n",
    "\n",
    "    b=  np.array([True, True, True])\n",
    "\n",
    "    print(float(np.sum(a)/np.sum(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1628722279928,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "YQM5RnBHxvkW",
    "outputId": "5d546191-90fe-4fea-b178-7e208868c858"
   },
   "outputs": [],
   "source": [
    "tupla = ('number', 'theory')\n",
    "print( vocab.index(tupla)) # indice de la tupla \"number theory\"\n",
    "\n",
    "indice = vocab.index(tupla)\n",
    "vector = m_test[:,indice]\n",
    "\n",
    "# for i in range(1, len(vector)):\n",
    "#     if vector[i] !=0.0:\n",
    "#         print(i, vector[i])\n",
    "\n",
    "cant_recu = np.sum(vector>0)\n",
    "cant_relev = np.sum(t_testing==1)\n",
    "cant_relev_recu = np.sum((vector>0) & (t_testing[:,0]== 1))\n",
    "print(\"cant_recu > 0) de la tupla 'Number theory' en TEST\" , cant_recu)\n",
    "print(\"cant_relev de la tupla 'Number theory' en TEST\" , cant_relev)\n",
    "print(\"cant_relev_recu de la tupla 'Number theory' en TEST\" , cant_relev_recu)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(dict_topic_numb_name.keys() )\n",
    "# print(dict_topic_numb_name[586])\n",
    "# print(dict_topic_numb_name[561])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oCR9dsqegEY"
   },
   "source": [
    "Build tuplas dictionary from CSVs (more preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1628722453905,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "dFXIil0AeiYS",
    "outputId": "5666a23a-2e94-4106-cd06-555639d47cfa"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ejemplo de tuplas\n",
    "# {\n",
    "#   'Top-Business-Food_and_Related_Products-Sweeteners': [{'word': 'Home', \n",
    "#                                                         'Beta_range': [0.0, 9.9], \n",
    "#                                                         'prec_train': 1.0, \n",
    "#                                                         'recall_train': 1.0, \n",
    "#                                                         'f1_train': 1.0, \n",
    "#                                                         'prec_test': 0.0, \n",
    "#                                                         'recall_test': 0.0, \n",
    "#                                                         'f1_test': 0.0}], \n",
    "#  'Top-Society-Subcultures-Gothic': [{'word': 'Computational', \n",
    "#                                      'Beta_range': [0.0, 9.9], \n",
    "#                                      'prec_train': 0.0, \n",
    "#                                      'recall_train': 0.0, \n",
    "#                                      'f1_train': 0.0, \n",
    "#                                      'prec_test': 1.0, \n",
    "#                                      'recall_test': 1.0, \n",
    "#                                      'f1_test': 1.0}]\n",
    "#  }\n",
    "\n",
    "\n",
    "tuplas = {}\n",
    "for category in topics:\n",
    "    #topic_name = Texto.categorias[category].replace(\"/\", \"-\")\n",
    "    topic_name = dict_topic_numb_name[category]['topic_name']\n",
    "    topic_name = topic_name.replace(\"/\", \"-\")  \n",
    "    tuplas[topic_name] = []\n",
    "    \n",
    "    lineas = open('FDD-CSVs/{}_{}.csv'.format(topic_name,category), 'r').read().splitlines()\n",
    "    lineas = lineas[1:]\n",
    "    xs = [ float(linea.split(',')[0]) for linea in lineas]\n",
    "\n",
    "    \n",
    "    precs_train =[ float(linea.split(',')[2]) for linea in lineas]\n",
    "    recalls_train = [ float(linea.split(',')[3]) for linea in lineas]\n",
    "    f1s_train = [ float(linea.split(',')[4]) for linea in lineas]\n",
    "\n",
    "    words = [ (linea.split(',')[1]) for linea in lineas]\n",
    "    precs_test =[ float(linea.split(',')[5]) for linea in lineas]\n",
    "    recalls_test = [ float(linea.split(',')[6]) for linea in lineas]\n",
    "    f1s_test = [ float(linea.split(',')[7]) for linea in lineas]\n",
    "    \n",
    "    inicio = xs[0]\n",
    "    fin = None\n",
    "    prec_train = precs_train[0]\n",
    "    recall_train = recalls_train[0]\n",
    "    f1_train = f1s_train[0]\n",
    "    prec_test = precs_test[0]\n",
    "    recall_test = recalls_test[0]\n",
    "    f1_test = f1s_test[0]\n",
    "    ultima = words[0]\n",
    "\n",
    "    for idx,x in enumerate(xs):\n",
    "        word = words[idx]\n",
    "        if (word!=ultima):\n",
    "            fin = xs[idx-1]\n",
    "            t = {'word':ultima,'Beta_range':[inicio,fin],'prec_train':prec_train,\n",
    "                 'recall_train':recall_train,\n",
    "                 'f1_train':f1_train,\n",
    "                 'prec_test':prec_test,\n",
    "                 'recall_test':recall_test,\n",
    "                 'f1_test':f1_test}\n",
    "            topic_name = dict_topic_numb_name[category]['topic_name']\n",
    "            topic_name = topic_name.replace(\"/\", \"-\")  \n",
    "            #topic_name = Texto.categorias[category].replace(\"/\", \"-\")     \n",
    "            tuplas[topic_name].append(t)\n",
    "            inicio = xs[idx]\n",
    "            ultima = words[idx]    \n",
    "            prec_train = precs_train[idx]\n",
    "            recall_train = recalls_train[idx]\n",
    "            f1_train = f1s_train[idx]\n",
    "            prec_test = precs_test[idx]\n",
    "            recall_test = recalls_test[idx]\n",
    "            f1_test = f1s_test[idx]\n",
    "\n",
    "    fin = xs[idx]\n",
    "    t = {'word':ultima,'Beta_range':[inicio,fin],'prec_train':prec_train,\n",
    "         'recall_train':recall_train,\n",
    "         'f1_train':f1_train,\n",
    "         'prec_test':prec_test,\n",
    "         'recall_test':recall_test,\n",
    "         'f1_test':f1_test}\n",
    "    tuplas[topic_name].append(t)\n",
    "    \n",
    "# if debug:\n",
    "#     for key, value in tuplas.items():\n",
    "#         print(key, ' : ', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT TRAINING & QUERY SAVING\n",
    "**Histogram plot per method**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "\n",
    "!mkdir 'queries-CSVs'\n",
    "    \n",
    "# names ceci\n",
    "names = ['MOGP','TGF', 'IDF', 'TGF*',   'TGF*-IDFEC', '$\\chi^2$', 'OR','IG', 'GR',  r'FDD$_{0.5}$',r'FDD$_{10}$', r'FDD$_{1.0}$']\n",
    "\n",
    "def tf(a,b,c,d):    \n",
    "    return a + c\n",
    "def tf_idf_ast(a,b,c,d):\n",
    "\treturn tf(a,b,c,d)*idfec(a,b,c,d)\n",
    "def tf_ast(a,b,c,d):\n",
    "\treturn a\n",
    "\n",
    "def tf_ast_idf_ast(a,b,c,d):\n",
    "\treturn tf_ast(a,b,c,d) * idfec(a,b,c,d)#np.log10((c+d)/(c+eps))\n",
    "def discr(a,b,c,d):\n",
    "\tsolucion = np.zeros(shape=a.shape)\n",
    "\tmask = (a+c)!=0\n",
    "\tsolucion[mask] = a[mask]/(a[mask]+c[mask])\n",
    "\treturn solucion\n",
    "\t#return a/(a+c)\n",
    "def descr(a,b,c,d):\n",
    "\tsolucion = np.zeros(shape=a.shape)\n",
    "\tmask = (a+b)!=0\n",
    "\tsolucion[(a+b)!=0] = a[mask]/(a[mask]+b[mask])\n",
    "\treturn solucion\n",
    "\n",
    "def fscore(a,b,c,d, alpha=0.82):\n",
    "\tdi = discr(a,b,c,d)\n",
    "\tde = descr(a,b,c,d)\n",
    "\tbeta = 1 - alpha\n",
    "\treturn 2*((alpha*de*beta*di)/(eps+alpha*de+beta*di))\n",
    "\n",
    "\n",
    "def delete_zeros(c):\n",
    "\tj = c.copy()\n",
    "\tj[j==0]=1\n",
    "\treturn j\n",
    "\n",
    "def prob_based(a,b,c,d):\n",
    "\treturn np.log10(1+(a/b)*(a/(c+eps))) # fix delete_zeros\n",
    "\n",
    "def gss(a,b,c,d):\n",
    "    #checkeado Domeneconi\n",
    "\tN = a + b + c + d\n",
    "\treturn (a*d - b*c)/(N**2)\n",
    "\n",
    "def x_sqr(a,b,c,d):\n",
    "\tN = a + b + c + d\n",
    "\tnum = (a*d - b*c) ** 2\n",
    "\tden = (a + c)* (b + d) * (a + b) * (c + d)\n",
    "\treturn N / ((num/(den+eps))+eps)\n",
    "\n",
    "def odds_ratio(a,b,c,d):\n",
    "\treturn np.log10((delete_zeros(a)*d)/(b*c+eps))\n",
    "\n",
    "def information_gain(a,b,c,d):\n",
    "\tN = a + b + c + d\n",
    "\ttermino1 = -((a+b)/N)  * np.log10((a+b)/N)\n",
    "\ttermino2 = (a / N) * np.log10((a+eps)/(a+c+eps))\n",
    "\ttermino3 = (b/N) * np.log10((b+eps)/(b+d+eps))\n",
    "\treturn (termino1 + termino2 + termino3)\n",
    "\n",
    "def gain_ratio(a,b,c,d):\n",
    "\tN = a + b + c + d\n",
    "\tig = information_gain(a,b,c,d)\n",
    "\tterm1 = -((a+b)/N)*np.log10((a+b+eps)/N)\n",
    "\tterm2 = -((c+d)/N)*np.log10((c+d+eps)/N)\n",
    "\treturn ig/(term1+term2+eps)\n",
    "\n",
    "def mutual_information(a,b,c,d):\n",
    "\tN = a + b + c + d\n",
    "\treturn np.log10(  ((eps+a)*N)  /   (eps +(a+b) * (a+c))   )\n",
    "def idf(a,b,c,d):\n",
    "\tN = a + b + c + d\n",
    "\treturn np.log10(N/(a+c+eps))\n",
    "def idf_prob(a,b,c,d):\n",
    "\treturn np.log10((b+d+eps)/(a+c+eps))\n",
    "def rf(a,b,c,d):\n",
    "\treturn np.log10(2+(a/(c+eps) ) )\n",
    "\n",
    "def idfec(a,b,c,d):\n",
    "\treturn np.log10(  (eps+c+d)/  ((eps+c))  )\n",
    "\n",
    "def idfec_b(a,b,c,d):\n",
    "\treturn np.log10(2 + (  (a+c+d)   /   ((c+eps))  ))\n",
    " \n",
    "def fdd_05(a,b,c,d):\n",
    "    return  fdd(a,b,c,d,beta=0.5)\n",
    "def fdd_10(a,b,c,d):\n",
    "    return fdd(a,b,c,d,beta=10)\n",
    "\n",
    "#ceci\n",
    "def fdd_1(a,b,c,d):\n",
    "    return fdd(a,b,c,d,beta=1.0)\n",
    "\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# # # # # # # # # #       IGM       # # # # # # # # # # # # # # # # \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# ESWA 2016\n",
    "# Generally, the\n",
    "# coefficient is set empirically to be 5.0 ∼ 9.0. The default value of λ\n",
    "# is set to be 7.0\n",
    "def igm(matriz, target, lambda_=7):\n",
    "  cant_docs = matriz.shape[0]\n",
    "  cant_terms = matriz.shape[1]\n",
    "  assert target.shape == (cant_docs,1)\n",
    "\n",
    "  igm_ = np.zeros(shape=(1,cant_terms))\n",
    "  \n",
    "  for term_idx in range(cant_terms):\n",
    "    cant_categorias = len(set(target[:,0]))\n",
    "    \n",
    "#     print(\"cant_categorias\", cant_categorias)\n",
    "#     print(\"np.sum(target > cant_categorias) \", np.sum(target>cant_categorias))\n",
    "#     print(\"target\", target)\n",
    "\n",
    "#    assert np.sum(target>cant_categorias)==0, 'Los indices de la categorias van\\ desde cero hasta cant_categorias-1'\n",
    "    f_kr = np.zeros(shape=(cant_categorias))\n",
    "    for categoria in range(cant_categorias):\n",
    "      freq = matriz[target[:,0]==categoria,term_idx]\n",
    "      f_kr[categoria] = np.sum(freq)\n",
    "\n",
    "\n",
    "    f_kr = np.sort(f_kr)[::-1]\n",
    "\n",
    "    igm_[0,term_idx] = f_kr[0]/(np.sum(f_kr*np.arange(1,1+len(f_kr))))\n",
    "      \n",
    "  return 1 + lambda_ *  igm_\n",
    "\n",
    "\n",
    "def tf_ast_igm(matrix,target, category):\n",
    "  a, b, c, d = get_abcd(matrix,target==category)\n",
    "\n",
    "#   print( \"\\n\\nen tf_ast_igm llamare a tf_ast(a,b,c,d)\")\n",
    "\n",
    "#   tf_ast_ = tf_ast(a,b,c,d)\n",
    "#   print(tf_ast_)\n",
    "#   print(\"\\n\\nllamare a igm(matrix, target) con target =\")\n",
    "#   print(target, \"\\n\\n\")\n",
    "    \n",
    "  igm_ = igm(matrix, target)\n",
    "  return tf_ast_*igm_\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# # # # # # # # # #    END IGM        # # # # # # # # # # # # # # # \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# # # # # # # # #       IGM_imp       # # # # # # # # # # # # # # # \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "\n",
    "def igm_imp(matriz, target, lambda_=7):\n",
    "  cant_docs = matriz.shape[0]\n",
    "  cant_terms = matriz.shape[1]\n",
    "  assert target.shape == (cant_docs,1)\n",
    "\n",
    "  igm_imp_ = np.zeros(shape=(1,cant_terms))\n",
    "  \n",
    "  for term_idx in range(cant_terms):\n",
    "    cant_categorias = len(set(target[:,0]))\n",
    "#    assert np.sum(target>cant_categorias)==0, 'Los indices de la categorias van\\ desde cero hasta cant_categorias-1'\n",
    "    f_kr = np.zeros(shape=(cant_categorias))\n",
    "    for categoria in range(cant_categorias):\n",
    "      freq = matriz[target[:,0]==categoria,term_idx]\n",
    "      f_kr[categoria] = np.sum(freq)\n",
    "\n",
    "\n",
    "    doc_per_cat = np.array([np.sum(target[:,0]==cat) for cat in range(cant_categorias)])\n",
    "    Dtotal = doc_per_cat[np.argsort(f_kr)[::-1][0]]\n",
    "\n",
    "\n",
    "\n",
    "    f_kr = np.sort(f_kr)[::-1]\n",
    "\n",
    "    f_k1 = f_kr[0]\n",
    "\n",
    "\n",
    "\n",
    "    igm_imp_[0,term_idx] = f_kr[0]/(np.sum(f_kr*np.arange(1,1+len(f_kr))) + np.log10(Dtotal/f_k1))\n",
    "      \n",
    "  return 1 + lambda_ * igm_imp_\n",
    "\n",
    "def tf_ast_igm_imp(matrix,target, category):\n",
    "  a, b, c, d = get_abcd(matrix,target==category)\n",
    "  tf_ast_ = tf_ast(a,b,c,d)\n",
    "  igm_imp_ = igm_imp(matrix, target)\n",
    "  return tf_ast_*igm_imp_\n",
    "\n",
    "def sqrt_tf_ast_igm_imp(matrix,target, category):\n",
    "  a, b, c, d = get_abcd(matrix,target==category)\n",
    "  tf_ast_ = tf_ast(a,b,c,d)\n",
    "  igm_imp_ = igm_imp(matrix, target)\n",
    "  return np.sqrt(tf_ast_)*igm_imp_\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# # # # # # # #       end IGM_imp       # # # # # # # # # # # # # # \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "\n",
    "def MOGP(a,b,c,d):\n",
    "    \n",
    "    # a denotes the number of documents that belong to class c_k and contain term t_i\n",
    "    solucion = np.zeros(shape=a.shape)    \n",
    "    return solucion\n",
    "    \n",
    "\n",
    "def get_GR_JIS_MOGP(topic_id):\n",
    "    \n",
    "    ## abrir CSV y devolver para GR la fila de promedios para todas las 5 corridas del topicId - es la ultima columna\n",
    "    ## idem JIS\n",
    "\n",
    "    # CSV:\n",
    "    # Topic_number  Run_1  Run_2  Run_3  Run_4  Run_5\n",
    "    #          1     0.96     1.00     1.00     0.94 \n",
    "    #         134    1.00     0.86     0.96     0.94 \n",
    "    #         ... \n",
    "\n",
    "    root = 'MOGP/'\n",
    "    file = 'Co3_globalRecall_allRun_lastGen_per_topic_TRAINING.csv'\n",
    "    \n",
    "    # GR LAST GEN - MEAN ALL RUN\n",
    "    df_GR = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "    gr_topicId = df_GR.loc[df_GR['Topic_number'] == topic_id]    \n",
    "    GR = gr_topicId[\"mean\"]\n",
    "    \n",
    "    \n",
    "    file = 'Co3_JaccardIndex_allRun_lastGen_per_topic_TRAINING.csv'\n",
    "    \n",
    "    # GR LAST GEN - MEAN ALL RUN\n",
    "    df_JIS = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "    jis_topicId = df_JIS.loc[df_JIS['Topic_number'] == topic_id]\n",
    "    JIS = jis_topicId[\"mean\"]\n",
    "    \n",
    "\n",
    "    return float(GR), float(JIS)\n",
    "\n",
    "def get_GR_JIS_MOGP_testing(topic_id):\n",
    "    \n",
    "    ## abrir CSV y devolver para GR la fila de promedios para todas las 5 corridas del topicId - es la ultima columna\n",
    "    ## idem JIS\n",
    "\n",
    "    # CSV:\n",
    "    # Topic_number  Run_1  Run_2  Run_3  Run_4  Run_5\n",
    "    #          1     0.96     1.00     1.00     0.94 \n",
    "    #         134    1.00     0.86     0.96     0.94 \n",
    "    #         ... \n",
    "\n",
    "    root = 'MOGP/'\n",
    "    file = 'Co3_MEAN_GLOBAL_RECALL_FROM_TESTING_ALL_TOPICS_LAST_GEN_ordered.csv'\n",
    "    \n",
    "    # GR LAST GEN - MEAN ALL RUN\n",
    "    df_GR = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "    gr_topicId = df_GR.loc[df_GR['Topic_number'] == topic_id]    \n",
    "    GR = gr_topicId[\"mean\"]\n",
    "    \n",
    "    file = 'Co3_MEAN_JACCARD_SIM_FROM_TESTING_ALL_TOPICS_LAST_GEN_ordered.csv'\n",
    "    \n",
    "    # GR LAST GEN - MEAN ALL RUN\n",
    "    df_JIS = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "    jis_topicId = df_JIS.loc[df_JIS['Topic_number'] == topic_id]\n",
    "    JIS = jis_topicId[\"mean\"]\n",
    "    \n",
    "\n",
    "    return float(GR), float(JIS)\n",
    "\n",
    "        \n",
    "def getPrecisionRecallMOGP(topic_id):\n",
    "\n",
    "    ## abrir CSV y devolver para precision la fila de promedios para todas las 5 corridas del topicId\n",
    "    ## idem Recall\n",
    "\n",
    "    # CSV:\n",
    "    # Topic_number  query_0  query_1  query_2  query_3  query_4  query_5  ... query_99\n",
    "    #          1     0.96     1.00     1.00     0.94     0.94     0.90          0.915\n",
    "    #         134    1.00     0.86     0.96     0.94     1.00     0.92          0.86\n",
    "    #         ... \n",
    "\n",
    "    root = 'MOGP/'\n",
    "    file = 'Co3_precision_allRun_lastGen_per_topic_TRAINING.csv'\n",
    "\n",
    "    # PRECISION LAST GEN - MEAN ALL RUN\n",
    "    df_P = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "\n",
    "    precision_topicId = df_P.loc[df_P['Topic_number'] == topic_id]\n",
    "    precision_topicId = precision_topicId.drop('Topic_number', 1) # elimina el numero de topico - devuelve arreglo limpio de promedios\n",
    "\n",
    "\n",
    "    file = 'Co3_recall_allRun_lastGen_per_topic_TRAINING.csv'\n",
    "    \n",
    "    # RECALL LAST GEN - MEAN ALL RUN\n",
    "    df_R = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "\n",
    "    recall_topicId = df_R.loc[df_R['Topic_number'] == topic_id]\n",
    "    recall_topicId = recall_topicId.drop('Topic_number', 1) # elimina el numero de topico - devuelve arreglo limpio de promedios\n",
    "\n",
    "    return precision_topicId.to_numpy(), recall_topicId.to_numpy()\n",
    "\n",
    "def getPrecisionRecallMOGP_testing(topic_id):\n",
    "\n",
    "    ## abrir CSV y devolver para precision la fila de promedios para todas las 5 corridas del topicId\n",
    "    ## idem Recall\n",
    "\n",
    "    # CSV:\n",
    "    # Topic_number  query_0  query_1  query_2  query_3  query_4  query_5  ... query_99\n",
    "    #          1     0.96     1.00     1.00     0.94     0.94     0.90          0.915\n",
    "    #         134    1.00     0.86     0.96     0.94     1.00     0.92          0.86\n",
    "    #         ... \n",
    "\n",
    "    root = 'MOGP/'\n",
    "    file = 'Co3_MEAN_PRECISION@10_FROM_TESTING_ALL_TOPICS_LAST_GEN_ordered.csv'\n",
    "\n",
    "    # PRECISION LAST GEN - MEAN ALL RUN\n",
    "    df_P = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "\n",
    "    precision_topicId = df_P.loc[df_P['Topic_number'] == topic_id]\n",
    "    precision_topicId = precision_topicId.drop('Topic_number', 1) # elimina el numero de topico - devuelve arreglo limpio de promedios\n",
    "\n",
    "    precision_topicId = precision_topicId[\"mean\"]\n",
    "    \n",
    "    \n",
    "    # EL RECALL esta almacenado como\n",
    "    # topic number  mean all run\n",
    "    #      1           0.703889\n",
    "    #      134         0.874884\n",
    "    ######\n",
    "    \n",
    "    \n",
    "    file = 'Co3_RECALL_FROM_TESTING_ALL_TOPICS_LAST_GEN_ordered.csv'\n",
    "    \n",
    "    # RECALL LAST GEN - MEAN ALL RUN\n",
    "    df_R = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores    \n",
    "    recall_topicId = df_R.loc[df_R['Topic_number'] == topic_id]\n",
    "    recall_topicId = recall_topicId.drop('Topic_number', 1) # elimina el numero de topico - devuelve arreglo limpio de promedios\n",
    "    \n",
    "    return precision_topicId.to_numpy(), recall_topicId.to_numpy()\n",
    "\n",
    "# metodos ceci\n",
    "methods = [MOGP, tf, idf, tf_ast, tf_ast_idf_ast, x_sqr, odds_ratio, information_gain, gain_ratio, fdd_05, fdd_10, fdd_1]\n",
    "\n",
    "\n",
    "# usados por Maiso\n",
    "#methods = [tf, idf, tf_ast,tf_ast_idf_ast , x_sqr, odds_ratio,tf_ast_igm,  information_gain, gain_ratio, tf_ast_igm_imp, gss, prob_based, sqrt_tf_ast_igm_imp,rf, idfec, tf_idf_ast, mutual_information , idfec_b, fdd_05,fdd_10]\n",
    "\n",
    "\n",
    "#testing = False\n",
    "\n",
    "topic_names = []\n",
    "#print(topics)\n",
    "for cat in topics:\n",
    "    topic_name = dict_topic_numb_name[cat]['topic_name']\n",
    "    topic_name = topic_name.replace(\"/\", \"-\")  \n",
    "    topic_names.append(topic_name)\n",
    "    print(topic_name)\n",
    "print(\"----------------------------------------------------------------- \\n\\n\")    \n",
    "\n",
    "\n",
    "categ2index = dict([(cat,idx) for idx,cat in enumerate(topic_names)])\n",
    "index2categ = dict([(idx,cat) for idx,cat in enumerate(topic_names)])\n",
    "\n",
    "# matrix to store metrics from training\n",
    "recalls = np.zeros(shape=(len(categ2index),len(methods)))   \n",
    "precisions = np.zeros((len(categ2index),len(methods))) \n",
    "f1_score = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "globalRecalls = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "jaccardIndexes = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "\n",
    "# matrix to store metrics from testing\n",
    "recalls_testing = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "precisions_testing = np.zeros((len(categ2index),len(methods)))\n",
    "f1_score_testing = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "globalRecalls_testing = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "jaccardIndexes_testing = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "\n",
    "\n",
    "# RECALL:\n",
    "#            FDD     TGF     TGF*\n",
    "# cat 1      x.xx    x.xx   x.xx\n",
    "# cat 2      x.xx    x.xx   x.xx\n",
    "# cat 3      x.xx    x.xx   x.xx\n",
    "# cat 4      x.xx    x.xx   x.xx\n",
    "\n",
    "# precision:\n",
    "#            FDD     TGF     TGF*\n",
    "# cat 1      x.xx    x.xx   x.xx\n",
    "# cat 2      x.xx    x.xx   x.xx\n",
    "# cat 3      x.xx    x.xx   x.xx\n",
    "# cat 4      x.xx    x.xx   x.xx\n",
    "\n",
    "\n",
    "i = np.array(vocab)\n",
    "\n",
    "\n",
    "for idx, category in enumerate(topics):            ##################################### CECI\n",
    "    t_testing = Texto.target(test, category_idx=category)\n",
    "    t_training = Texto.target(train, category_idx=category)   # vector de T o F si el doc es relev para el topico o no\n",
    "    \n",
    "    #     A = [i for i, x in enumerate(t_training) if x]\n",
    "    #     print(\"las posiciones de docus relevantes para el topico \", category)\n",
    "    #     print (A)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"TOPICO = \", category)\n",
    "    \n",
    "    # almacena un csv por topico, con las 100 queries de cada metodo\n",
    "    writer = open('queries-CSVs/{}_queries.csv'.format(category), \"w\")\n",
    "\n",
    "    a, b, c, d = get_abcd(m_train,t_training)   \n",
    "\n",
    "    a_testing, b_testing, c_testing, d_testing = get_abcd(m_test,t_testing)\n",
    "    \n",
    "    recall = descr(a,b,c,d)\n",
    "    precision = discr(a,b,c,d)\n",
    "    \n",
    "    recall_testing = descr(a_testing, b_testing, c_testing, d_testing)\n",
    "    precision_testing = discr(a_testing, b_testing ,c_testing , d_testing)\n",
    "    \n",
    "    f = open('queries-CSVs/{}_queries.csv'.format(category), \"w\")\n",
    "    \n",
    "    # create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header = [\"method\"] + [\"query_\"+str(i) for i in range(0,100)]\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for k in range(len(methods)):\n",
    "    \n",
    "        method = methods[k]\n",
    "        \n",
    "        print(\"topico= \", category, \" - Metodo= \", method.__name__)        \n",
    "        \n",
    "        sum_recall_value = 0.0\n",
    "        sum_recall_value_testing = 0.0\n",
    "        \n",
    "        sum_prec_value = 0.0\n",
    "        sum_prec_value_testing = 0.0                     \n",
    "        \n",
    "        gr_per_method = 0.0\n",
    "        gr_per_method_testing = 0.0\n",
    "        \n",
    "        jis_per_method = 0.0\n",
    "        jis_per_method_testing = 0.0\n",
    "        \n",
    "        queries_method_k = []        \n",
    "        \n",
    "        if method.__name__ == \"MOGP\":\n",
    "            # los valores de las metricas de MOGP ya estan calculados en CSVs\n",
    "            precisions_topicId, recalls_topicId = getPrecisionRecallMOGP(int(category))\n",
    "            precisions_topicId_testing, recalls_topicId_testing = getPrecisionRecallMOGP_testing(int(category))\n",
    "            \n",
    "            sum_prec_value = np.sum(precisions_topicId)\n",
    "            sum_prec_value_testing = np.sum(precisions_topicId_testing)\n",
    "            \n",
    "            sum_recall_value = np.sum(recalls_topicId)\n",
    "            sum_recall_value_testing = np.sum(recalls_topicId_testing)           \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            best_100_words_with_k, scores_with_k = get_best_word_method(m_train, Texto.target(train), category, vocab, method)\n",
    "            print(scores_with_k) \n",
    "            \n",
    "            total_RR = []\n",
    "            RR1 = []\n",
    "            \n",
    "            total_RR_testing = []\n",
    "            RR2 = []\n",
    "            \n",
    "            for indx in range(1, len(best_100_words_with_k)+1):                            \n",
    "                \n",
    "                index_topword = scores_with_k[-1*indx]                        \n",
    "                                   \n",
    "                #top_word = (i[np.argsort(scores_with_k)])[-1][-1*indx]\n",
    "                top_word = (i[index_topword])\n",
    "                \n",
    "                queries_method_k.append(top_word)\n",
    "\n",
    "                # estan comentados F1 ya se calculan aparte. comprobado que da igual\n",
    "                #RR: relevantes_recuperados por cada termino-vectores de Bool - tantas filas como docus en train o test\n",
    "                precision_value, recall_value , _ , RR1 = get_ir_metrics(top_word, m_train, m_tfIdf_train, Texto.target(train), category, vocab)\n",
    "                \n",
    "                sum_recall_value+= recall_value\n",
    "                sum_prec_value+= precision_value\n",
    "                \n",
    "                \n",
    "                # total de Relevantes Recuperados para GR en TRAIN\n",
    "                if len(total_RR)==0:\n",
    "                    total_RR = RR1\n",
    "                else:            \n",
    "                    total_RR = (total_RR | RR1)            \n",
    "                #print(\"termino= \", top_word, \" total_RR \\t\" ,np.sum(total_RR) )\n",
    "                \n",
    "                \n",
    "                \n",
    "                # metricas para TESTING\n",
    "                precision_value_testing, recall_value_testing , f1_value_testing, RR2 = get_ir_metrics(top_word, m_test, m_tfIdf_test, Texto.target(test), category, vocab)                                                                    \n",
    "                \n",
    "                # total de Relevantes Recuperados para GR en TEST\n",
    "                if len(total_RR_testing)==0:\n",
    "                    total_RR_testing = RR2\n",
    "                else:            \n",
    "                    total_RR_testing = (total_RR_testing | RR2)                      \n",
    "                #print(\"termino= \", top_word, \" total_RR_TESTING \\t\" ,np.sum(total_RR_testing) )                    \n",
    "\n",
    "                sum_recall_value_testing+= recall_value_testing\n",
    "                sum_prec_value_testing+= precision_value_testing\n",
    "                \n",
    "                top_word = None\n",
    "                \n",
    "            \n",
    "            #almaceno las queries en CSVs segun el metodo                \n",
    "            line = [method.__name__] + queries_method_k\n",
    "\n",
    "            writer.writerow(line)\n",
    "            \n",
    "\n",
    "            gr_per_method, jis_per_method = get_ir_poblational_metrics(queries_method_k, m_train, total_RR, Texto.target(train), category, vocab)\n",
    "            gr_per_method_testing, jis_per_method_testing = get_ir_poblational_metrics(queries_method_k, m_test, total_RR_testing, Texto.target(test), category, vocab)\n",
    "        \n",
    "        if method.__name__ == \"MOGP\":\n",
    "            #print (\"mean RECALL \", sum_recall_value/100)\n",
    "            #print (\"mean PRECISION \", sum_prec_value/100)\n",
    "            \n",
    "            recalls[idx,k] = sum_recall_value/100\n",
    "            precisions[idx,k] = sum_prec_value/100             # CON MOGP SON 100 queries\n",
    "            globalRecalls[idx,k], jaccardIndexes[idx,k] = get_GR_JIS_MOGP(int(category))                 # TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR   \n",
    "            \n",
    "            recalls_testing[idx,k] = sum_recall_value_testing\n",
    "            precisions_testing[idx,k] = sum_prec_value_testing             # YA SON PROMEDIOS\n",
    "            globalRecalls_testing[idx,k], jaccardIndexes_testing[idx,k] = get_GR_JIS_MOGP_testing(int(category))\n",
    "\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            recalls[idx,k] = sum_recall_value/len(best_100_words_with_k)\n",
    "            recalls_testing[idx,k] = sum_recall_value_testing/len(best_100_words_with_k)\n",
    "            \n",
    "            precisions[idx,k] = sum_prec_value/len(best_100_words_with_k)\n",
    "            precisions_testing[idx,k] = sum_prec_value_testing/len(best_100_words_with_k)\n",
    "            \n",
    "            globalRecalls[idx,k] = gr_per_method\n",
    "            globalRecalls_testing[idx,k] = gr_per_method_testing\n",
    "            \n",
    "            jaccardIndexes[idx,k] = jis_per_method\n",
    "            jaccardIndexes_testing[idx,k] = jis_per_method_testing\n",
    "                                                   \n",
    "            \n",
    "            \n",
    "\n",
    "    # se queda con una top word en RECALL y otra en PREC para cada topico - WHY? creo que no se usa\n",
    "    top_word_r = (i[np.argsort(recall)])[-1][-1]\n",
    "    top_word_p = (i[np.argsort(precision)])[-1][-1]\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "from scipy.stats import sem, t\n",
    "from scipy import mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# testing = False\n",
    "\n",
    "confidence = 0.95\n",
    "\n",
    "# promedia metricas por TODOS LOS TOPICOS\n",
    "r = np.average(recalls,axis=0)\n",
    "p = np.average(precisions,axis=0)\n",
    "f1scores = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "mask = (precisions+recalls)!=0\n",
    "f1scores[mask] = 2.0 * ((precisions[mask] * recalls[mask]) / (precisions[mask]+recalls[mask]))\n",
    "f1 = np.average(f1scores,axis=0)\n",
    "\n",
    "gr = np.average(globalRecalls,axis=0)\n",
    "jis = np.average(jaccardIndexes,axis=0)\n",
    "\n",
    "deltas_f1score = np.zeros(shape=(len(methods)))\n",
    "deltas_recall = np.zeros(shape=(len(methods)))\n",
    "deltas_precision = np.zeros(shape=(len(methods)))\n",
    "deltas_gr = np.zeros(shape=(len(methods)))\n",
    "deltas_jis = np.zeros(shape=(len(methods)))\n",
    "\n",
    "for columna in range(len(recalls[0,:])):\n",
    "    datos = recalls[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_recall[columna] = h\n",
    "    \n",
    "    datos = precisions[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_precision[columna] = h\n",
    "    \n",
    "    datos = f1scores[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_f1score[columna] = h\n",
    "    \n",
    "    # NUEVO GR Y JIS\n",
    "    datos = globalRecalls[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_gr[columna] = h\n",
    "    \n",
    "    datos = jaccardIndexes[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_jis[columna] = h\n",
    "\n",
    "#for cate in range(len(methods)):\n",
    "#    print('method: {:15} recall: {:5,.4f} precision {:5,.4f} f1-score: {:5,.4f}'.format(names[cate],r[cate],p[cate],f1[cate]))\n",
    "#print('method: {:15} recall: {:5,.4f} precision {:5,.4f} f1-score: {:5,.4f}'.format('FDD',r[cate+1],p[cate+1],f1[cate+1]))\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [15, 15]\n",
    "fig,ax = plt.subplots(5)\n",
    "\n",
    "\n",
    "## PLOT PRECISION\n",
    "label = [f'{value:4.3f}' for value in p]\n",
    "for i in range(len(p)):\n",
    "  ax[0].text(x =i+0.1 , y = p[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[0].bar(np.arange(len(methods)), height=p,tick_label=names)\n",
    "ax[0].errorbar(np.arange(len(methods)),p,yerr=deltas_precision,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT RECALL\n",
    "label = [f'{value:4.3f}' for value in r]\n",
    "for i in range(len(r)):\n",
    "  ax[1].text(x =i+0.1 , y = r[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[1].bar(np.arange(len(methods)), height=r,tick_label=names)\n",
    "ax[1].errorbar(np.arange(len(methods)),r,yerr=deltas_recall,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT F1\n",
    "label = [f'{value:4.3f}' for value in f1]\n",
    "for i in range(len(f1)):  \n",
    "  ax[2].text(x =i+0.1 , y = f1[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[2].bar(np.arange(len(methods)), height=f1,tick_label=names)\n",
    "ax[2].errorbar(np.arange(len(methods)),f1,yerr=deltas_f1score,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT GLOBAL RECALL\n",
    "label = [f'{value:4.3f}' for value in gr]\n",
    "for i in range(len(gr)):  \n",
    "  ax[3].text(x =i+0.1 , y = gr[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[3].bar(np.arange(len(methods)), height = gr,tick_label=names)\n",
    "ax[3].errorbar(np.arange(len(methods)),gr,yerr=deltas_gr,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT JACCARD INDEX SIM\n",
    "label = [f'{value:4.3f}' for value in jis]\n",
    "for i in range(len(jis)):  \n",
    "  ax[4].text(x =i+0.1 , y = jis[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[4].bar(np.arange(len(methods)), height= jis ,tick_label=names)\n",
    "ax[4].errorbar(np.arange(len(methods)),jis,yerr=deltas_jis,fmt='.k',capsize=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[1].set_ylabel('$recall$ $(avg)$',fontsize=15)\n",
    "ax[0].set_ylabel('$precision$ $(avg)$',fontsize=15)\n",
    "ax[2].set_ylabel('$F_1-score$ $(avg)$',fontsize=15)\n",
    "ax[3].set_ylabel('$G. Recall$ $(avg)$',fontsize=15)\n",
    "ax[4].set_ylabel('$J. S. Index$ $(avg)$',fontsize=15)\n",
    "\n",
    "\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[1].set_ylim(0,1)\n",
    "ax[2].set_ylim(0,1)\n",
    "ax[3].set_ylim(0,1)\n",
    "ax[4].set_ylim(0,1)\n",
    "\n",
    "\n",
    "for plot in ax:\n",
    "\n",
    "    for tick in plot.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(10) \n",
    "    for tick in plot.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(10) \n",
    "\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "\n",
    "\n",
    "# horizontal line in MOGP value\n",
    "ax[0].axhline(y=p[-12], c='k',ls=':')\n",
    "ax[1].axhline(y=r[-12], c='k',ls=':')\n",
    "ax[2].axhline(y=f1[-12], c='k',ls=':')\n",
    "ax[3].axhline(y=gr[-12], c='k',ls=':')\n",
    "ax[4].axhline(y=jis[-12], c='k',ls=':')\n",
    "\n",
    "#fig.savefig(f'performanceComparisonDMOZ{\"Testing\" if testing else \"Training\"}.pdf')\n",
    "fig.savefig(f'performanceComparisonDMOZ_Training.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT TESTING\n",
    "**Results ofthe queries generated avobe but over a new data set** (using the testing matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# renombro a nombres usados en plot de mas arriba para no introducir bugs, ya que el calculo es el mismo\n",
    "recalls = recalls_testing\n",
    "precisions = precisions_testing\n",
    "globalRecalls = globalRecalls_testing\n",
    "jaccardIndexes = jaccardIndexes_testing\n",
    "\n",
    "# promedia metricas por TODOS LOS TOPICOS\n",
    "r = np.average(recalls,axis=0)\n",
    "p = np.average(precisions,axis=0)\n",
    "f1scores = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "mask = (precisions+recalls)!=0\n",
    "f1scores[mask] = 2.0 * ((precisions[mask] * recalls[mask]) / (precisions[mask]+recalls[mask]))\n",
    "f1 = np.average(f1scores,axis=0)\n",
    "\n",
    "gr = np.average(globalRecalls,axis=0)\n",
    "jis = np.average(jaccardIndexes,axis=0)\n",
    "\n",
    "deltas_f1score = np.zeros(shape=(len(methods)))\n",
    "deltas_recall = np.zeros(shape=(len(methods)))\n",
    "deltas_precision = np.zeros(shape=(len(methods)))\n",
    "deltas_gr = np.zeros(shape=(len(methods)))\n",
    "deltas_jis = np.zeros(shape=(len(methods)))\n",
    "\n",
    "for columna in range(len(recalls[0,:])):\n",
    "    datos = recalls[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_recall[columna] = h\n",
    "    \n",
    "    datos = precisions[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_precision[columna] = h\n",
    "    \n",
    "    datos = f1scores[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_f1score[columna] = h\n",
    "    \n",
    "    # NUEVO GR Y JIS\n",
    "    datos = globalRecalls[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_gr[columna] = h\n",
    "    \n",
    "    datos = jaccardIndexes[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_jis[columna] = h\n",
    "\n",
    "#for cate in range(len(methods)):\n",
    "#    print('method: {:15} recall: {:5,.4f} precision {:5,.4f} f1-score: {:5,.4f}'.format(names[cate],r[cate],p[cate],f1[cate]))\n",
    "#print('method: {:15} recall: {:5,.4f} precision {:5,.4f} f1-score: {:5,.4f}'.format('FDD',r[cate+1],p[cate+1],f1[cate+1]))\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [15, 15]\n",
    "fig,ax = plt.subplots(5)\n",
    "\n",
    "\n",
    "## PLOT PRECISION\n",
    "label = [f'{value:4.3f}' for value in p]\n",
    "for i in range(len(p)):\n",
    "  ax[0].text(x =i+0.1 , y = p[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[0].bar(np.arange(len(methods)), height=p,tick_label=names)\n",
    "ax[0].errorbar(np.arange(len(methods)),p,yerr=deltas_precision,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT RECALL\n",
    "label = [f'{value:4.3f}' for value in r]\n",
    "for i in range(len(r)):\n",
    "  ax[1].text(x =i+0.1 , y = r[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[1].bar(np.arange(len(methods)), height=r,tick_label=names)\n",
    "ax[1].errorbar(np.arange(len(methods)),r,yerr=deltas_recall,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT F1\n",
    "label = [f'{value:4.3f}' for value in f1]\n",
    "for i in range(len(f1)):  \n",
    "  ax[2].text(x =i+0.1 , y = f1[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[2].bar(np.arange(len(methods)), height=f1,tick_label=names)\n",
    "ax[2].errorbar(np.arange(len(methods)),f1,yerr=deltas_f1score,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT GLOBAL RECALL\n",
    "label = [f'{value:4.3f}' for value in gr]\n",
    "for i in range(len(gr)):  \n",
    "  ax[3].text(x =i+0.1 , y = gr[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[3].bar(np.arange(len(methods)), height = gr,tick_label=names)\n",
    "ax[3].errorbar(np.arange(len(methods)),gr,yerr=deltas_gr,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT JACCARD INDEX SIM\n",
    "label = [f'{value:4.3f}' for value in jis]\n",
    "for i in range(len(jis)):  \n",
    "  ax[4].text(x =i+0.1 , y = jis[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[4].bar(np.arange(len(methods)), height= jis ,tick_label=names)\n",
    "ax[4].errorbar(np.arange(len(methods)),jis,yerr=deltas_jis,fmt='.k',capsize=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[1].set_ylabel('$recall$ $(avg)$',fontsize=15)\n",
    "ax[0].set_ylabel('$precision$ $(avg)$',fontsize=15)\n",
    "ax[2].set_ylabel('$F_1-score$ $(avg)$',fontsize=15)\n",
    "ax[3].set_ylabel('$G. Recall$ $(avg)$',fontsize=15)\n",
    "ax[4].set_ylabel('$J. S. Index$ $(avg)$',fontsize=15)\n",
    "\n",
    "\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[1].set_ylim(0,1)\n",
    "ax[2].set_ylim(0,1)\n",
    "ax[3].set_ylim(0,1)\n",
    "ax[4].set_ylim(0,1)\n",
    "\n",
    "for plot in ax:\n",
    "\n",
    "    for tick in plot.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(10) \n",
    "    for tick in plot.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(10) \n",
    "\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "\n",
    "\n",
    "# horizontal line in MOGP value\n",
    "ax[0].axhline(y=p[-12], c='k',ls=':')\n",
    "ax[1].axhline(y=r[-12], c='k',ls=':')\n",
    "ax[2].axhline(y=f1[-12], c='k',ls=':')\n",
    "ax[3].axhline(y=gr[-12], c='k',ls=':')\n",
    "ax[4].axhline(y=jis[-12], c='k',ls=':')\n",
    "\n",
    "fig.savefig(f'performanceComparisonDMOZ_Testing.pdf')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DMOZ weighting methods.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
