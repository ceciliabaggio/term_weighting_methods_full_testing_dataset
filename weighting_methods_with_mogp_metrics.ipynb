{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epLwHJtZiUMx"
   },
   "source": [
    "# DMOZ weighting methods using Global Recall & Jaccard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1628721517208,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "8U6V_57HOBTY"
   },
   "outputs": [],
   "source": [
    "# testing - para mostrar calculos intermedios\n",
    "debug = True\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa-_NOPyMa4v"
   },
   "source": [
    "Dataset load and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1628721517210,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "Ngocd76rT2rW",
    "outputId": "948dfd04-2a52-4bba-f30d-9a84af3819b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":::::::::::::::: Loaded topic_names ::::::::::::::::\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_number</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Top/Science/Math/Number_Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>Top/Sports/Golf/Instruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Top/Business/Industrial_Goods_and_Services/Fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>Top/Society/Issues/Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99</td>\n",
       "      <td>Top/Games/Board_Games/War_and_Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>134</td>\n",
       "      <td>Top/Home/Cooking/Fruits_and_Vegetables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>187</td>\n",
       "      <td>Top/Sports/Cycling/Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>215</td>\n",
       "      <td>Top/Recreation/Pets/Exotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>221</td>\n",
       "      <td>Top/Recreation/Outdoors/Camping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>259</td>\n",
       "      <td>Top/Business/Investing/News_and_Media</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>289</td>\n",
       "      <td>Top/Health/Conditions_and_Diseases/Skin_Disorders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>350</td>\n",
       "      <td>Top/Health/Reproductive_Health/Birth_Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>363</td>\n",
       "      <td>Top/Science/Environment/Air_Quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>403</td>\n",
       "      <td>Top/Science/Social_Sciences/Economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>418</td>\n",
       "      <td>Top/Business/Hospitality/Restaurant_Chains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>428</td>\n",
       "      <td>Top/Shopping/Tobacco/Cigars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>465</td>\n",
       "      <td>Top/Shopping/Jewelry/Theme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>476</td>\n",
       "      <td>Top/Arts/Music/Collecting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>528</td>\n",
       "      <td>Top/Business/Agriculture_and_Forestry/Aquaculture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>529</td>\n",
       "      <td>Top/Science/Astronomy/Solar_System</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>537</td>\n",
       "      <td>Top/Recreation/Birding/Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>538</td>\n",
       "      <td>Top/Society/Religion_and_Spirituality/Pagan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>556</td>\n",
       "      <td>Top/Business/Management/Management_Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>561</td>\n",
       "      <td>Top/Arts/Music/Reviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>586</td>\n",
       "      <td>Top/Arts/Television/Networks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_number                                         topic_name\n",
       "0              1                     Top/Science/Math/Number_Theory\n",
       "1             25                        Top/Sports/Golf/Instruction\n",
       "2             37  Top/Business/Industrial_Goods_and_Services/Fac...\n",
       "3             58                     Top/Society/Issues/Environment\n",
       "4             99             Top/Games/Board_Games/War_and_Politics\n",
       "5            134             Top/Home/Cooking/Fruits_and_Vegetables\n",
       "6            187                          Top/Sports/Cycling/Travel\n",
       "7            215                         Top/Recreation/Pets/Exotic\n",
       "8            221                    Top/Recreation/Outdoors/Camping\n",
       "9            259              Top/Business/Investing/News_and_Media\n",
       "10           289  Top/Health/Conditions_and_Diseases/Skin_Disorders\n",
       "11           350       Top/Health/Reproductive_Health/Birth_Control\n",
       "12           363                Top/Science/Environment/Air_Quality\n",
       "13           403              Top/Science/Social_Sciences/Economics\n",
       "14           418         Top/Business/Hospitality/Restaurant_Chains\n",
       "15           428                        Top/Shopping/Tobacco/Cigars\n",
       "16           465                         Top/Shopping/Jewelry/Theme\n",
       "17           476                          Top/Arts/Music/Collecting\n",
       "18           528  Top/Business/Agriculture_and_Forestry/Aquaculture\n",
       "19           529                 Top/Science/Astronomy/Solar_System\n",
       "20           537                      Top/Recreation/Birding/Europe\n",
       "21           538        Top/Society/Religion_and_Spirituality/Pagan\n",
       "22           556         Top/Business/Management/Management_Science\n",
       "23           561                             Top/Arts/Music/Reviews\n",
       "24           586                       Top/Arts/Television/Networks"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(':::::::::::::::: Loaded topic_names ::::::::::::::::')\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "df_topics = pd.read_csv (r'topic_names_25.txt', header = None, delimiter = '\\t', dtype={'topic_number': \"Int64\", 'topic_name': str})\n",
    "df_topics.columns = ['topic_number','topic_name']\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1628721517211,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "E6mZrMuyvgQm",
    "outputId": "206c8b3f-fd2b-4926-e2c1-1db8a85f1b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    topic_number                                         topic_name\n",
      "18           528  Top/Business/Agriculture_and_Forestry/Aquaculture\n"
     ]
    }
   ],
   "source": [
    "if debug:\n",
    "  select_name = df_topics.loc[df_topics['topic_number'] == 528]\n",
    "  print(select_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DX5OgmE_oyAr"
   },
   "source": [
    "### Dictionary\n",
    "dict_topic_numb_name={topic_number: topic_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1628721517211,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "3PivO1Lhy8rM",
    "outputId": "d50ca483-11cd-4e35-eef8-6071d21413c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Top/Science/Math/Number_Theory', 'Top/Sports/Golf/Instruction', 'Top/Business/Industrial_Goods_and_Services/Factory_Automation', 'Top/Society/Issues/Environment', 'Top/Games/Board_Games/War_and_Politics', 'Top/Home/Cooking/Fruits_and_Vegetables', 'Top/Sports/Cycling/Travel', 'Top/Recreation/Pets/Exotic', 'Top/Recreation/Outdoors/Camping', 'Top/Business/Investing/News_and_Media', 'Top/Health/Conditions_and_Diseases/Skin_Disorders', 'Top/Health/Reproductive_Health/Birth_Control', 'Top/Science/Environment/Air_Quality', 'Top/Science/Social_Sciences/Economics', 'Top/Business/Hospitality/Restaurant_Chains', 'Top/Shopping/Tobacco/Cigars', 'Top/Shopping/Jewelry/Theme', 'Top/Arts/Music/Collecting', 'Top/Business/Agriculture_and_Forestry/Aquaculture', 'Top/Science/Astronomy/Solar_System', 'Top/Recreation/Birding/Europe', 'Top/Society/Religion_and_Spirituality/Pagan', 'Top/Business/Management/Management_Science', 'Top/Arts/Music/Reviews', 'Top/Arts/Television/Networks']\n",
      "\n",
      " {'topic_name': 'Top/Science/Math/Number_Theory'} {'topic_name': 'Top/Business/Agriculture_and_Forestry/Aquaculture'}\n"
     ]
    }
   ],
   "source": [
    "df1 = df_topics[['topic_name']].to_numpy()\n",
    "list_of_topic_names = []\n",
    "for arr in df1:\n",
    "  for topic in arr:  \n",
    "    list_of_topic_names.append(topic)\n",
    "print(list_of_topic_names)\n",
    "\n",
    "\n",
    "\n",
    "### Dictionary of {topic_number: topic_name} \n",
    "dict_topic_numb_name = df_topics.set_index('topic_number').T.to_dict()\n",
    "print(\"\\n\",dict_topic_numb_name[1], dict_topic_numb_name[528])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dnmFMV4JtxV"
   },
   "source": [
    "### Test Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4853,
     "status": "ok",
     "timestamp": 1628721522053,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "7lFMDmaJJs_P",
    "outputId": "eb29d61a-facb-4e6f-e75c-8932f029d7ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/cecilia/.local/lib/python3.8/site-packages (22.3)\n",
      "Collecting pip\n",
      "  Downloading pip-23.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m237.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/cecilia/.local/lib/python3.8/site-packages (65.5.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-67.2.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m476.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /home/cecilia/.local/lib/python3.8/site-packages (0.37.1)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.37.1\n",
      "    Uninstalling wheel-0.37.1:\n",
      "      Successfully uninstalled wheel-0.37.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 65.5.0\n",
      "    Uninstalling setuptools-65.5.0:\n",
      "      Successfully uninstalled setuptools-65.5.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3\n",
      "    Uninstalling pip-22.3:\n",
      "      Successfully uninstalled pip-22.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "launchpadlib 1.10.13 requires testresources, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pip-23.0 setuptools-67.2.0 wheel-0.38.4\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/cecilia/.local/lib/python3.8/site-packages (3.4.1)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m655.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (67.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: jinja2 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (0.3.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/cecilia/.local/lib/python3.8/site-packages (from spacy) (4.62.1)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /home/cecilia/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cecilia/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/cecilia/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /home/cecilia/.local/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/lib/python3/dist-packages (from jinja2->spacy) (1.1.0)\n",
      "Installing collected packages: spacy-legacy, smart-open, pathy, spacy\n",
      "  Attempting uninstall: spacy-legacy\n",
      "    Found existing installation: spacy-legacy 3.0.9\n",
      "    Uninstalling spacy-legacy-3.0.9:\n",
      "      Successfully uninstalled spacy-legacy-3.0.9\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.2.0\n",
      "    Uninstalling smart-open-5.2.0:\n",
      "      Successfully uninstalled smart-open-5.2.0\n",
      "  Attempting uninstall: pathy\n",
      "    Found existing installation: pathy 0.6.0\n",
      "    Uninstalling pathy-0.6.0:\n",
      "      Successfully uninstalled pathy-0.6.0\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.1\n",
      "    Uninstalling spacy-3.4.1:\n",
      "      Successfully uninstalled spacy-3.4.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pathy-0.10.1 smart-open-6.3.0 spacy-3.5.0 spacy-legacy-3.0.12\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11278,
     "status": "ok",
     "timestamp": 1628721533326,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "05cTm8sBxiZf",
    "outputId": "78e743bb-2c94-4125-ade3-58e60475250f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python: No module named spacy\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYzXQgfWbd-3"
   },
   "source": [
    "### Clase Texto (Documento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4637,
     "status": "ok",
     "timestamp": 1628721537955,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "Vew9XqHRbcC0",
    "outputId": "90d86b62-b05b-4c04-9eda-35898cb18717"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cecilia/.local/lib/python3.8/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.4.0) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  OK   ] Texto Class loaded.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "def valid_token(token):\n",
    "    return not token.is_stop and token.is_alpha and token.lemma_.isalnum()\n",
    "\n",
    "class Texto(object):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    #categorias = df_topics[['topic_name']].to_numpy()  \n",
    "    # convert to an array of DMOZ topic names, named categorias\n",
    "    \n",
    "    # df1 = df_topics[['topic_name']].to_numpy()\n",
    "    # categorias = []\n",
    "    # for arr in df1:\n",
    "    #   for topic in arr:  \n",
    "    #    categorias.append(topic)\n",
    "    # print(categorias)               # pero tmb tengo dictionary de topic NUM: categoria!!!!!!!!!!!!!!!!!!! ver si no esta de mas esto\n",
    "\n",
    "    \n",
    "    def __init__(self, idx, path, spacy_doc=False):\n",
    "        self.idx = idx \n",
    "        self.path = path\n",
    "        self.topic_number = int(path.split('/')[-2])\n",
    "        self.raw_text = open(path, 'r', encoding='UTF-8').read().lower()           # str.lower() added by Ceci\n",
    "        # all the tokens in the doc, no filtering        \n",
    "        self.tokens = Texto.nlp(self.raw_text) \n",
    "        #self.tokens = Texto.nlp(self.raw_text, disable=['textcat','ner','tagger','parser'])         \n",
    "\n",
    "    # si category_idx no es None, entonces devuelve un array de 0s y 1s.\n",
    "    # 1s en las posiciones donde hay un texto en textos tal que el nro de topico es category_idx.\n",
    "    # si category_idx=None entonces devuelve un array con nros de topicos para cada texto (este array no es de 0s y 1s)\n",
    "    # category_idx DEBE ser ENTERO\n",
    "    @staticmethod\n",
    "    def target(textos, category_idx=None):        \n",
    "        if not category_idx is None:                    \n",
    "            #categoria = dict_topic_numb_name[Texto.topic_number]  # esto me da el nombre del topic            \n",
    "#             if debug:\n",
    "#               for texto in textos:\n",
    "#                 print(texto.topic_number, category_idx, int(texto.topic_number)==int(category_idx))\n",
    "#                 print(dict_topic_numb_name[int(texto.topic_number)])  # esto me da el nombre del topic            \n",
    "            target = np.array([int(texto.topic_number)==int(category_idx) for texto in textos])\n",
    "            # devuelve 1s donde hay doc para ese topico\n",
    "            return target[:,np.newaxis] # devuelve un arreglo de 0s y 1s con 1s si el doc es del topico category_idx\n",
    "        else:      \n",
    "            # el index deberia ser le nro de topico del DF de topic names, no el indice, no?\n",
    "            #categoria2index = dict([(categoria, index) for index,categoria in enumerate(Texto.categorias)])\n",
    "            #print(\"categoria2index\", categoria2index)\n",
    "            target = np.array([int(texto.topic_number) for texto in textos])  # devuelve un arreglo de TOPIC_IDs segun documentos en textos (IDS pueden ser distintos)\n",
    "            return target[:,np.newaxis]\n",
    "        \n",
    "print('[  OK   ] Texto Class loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG5zEjoKd0Ui"
   },
   "source": [
    "## load files from disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1628721537956,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "8dpJyOOF4cdD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/set12_train_25topics.tar.gz': No such file or directory\n",
      ":::::::::::::::: EXTRACTED dataset Set12 ::::::::::::::::\n",
      "cp: cannot stat '/set3_test_all_topics.tar.gz': No such file or directory\n",
      ":::::::::::::::: Extracted dataset Set3 ::::::::::::::::\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not (os.path.isfile('pickle_texts/dmoz_train_25.p') and os.path.isfile('pickle_texts/dmoz_test_25.p')):\n",
    "\n",
    "    !cp '/set12_train_25topics.tar.gz' .\n",
    "    #unzip dataset\n",
    "    !tar xzf 'set12_train_25topics.tar.gz'\n",
    "\n",
    "    print(':::::::::::::::: EXTRACTED dataset Set12 ::::::::::::::::')\n",
    "\n",
    "    !cp '/set3_test_all_topics.tar.gz' .\n",
    "    #unzip dataset\n",
    "    !tar xzf 'set3_test_all_topics.tar.gz'\n",
    "\n",
    "    print(':::::::::::::::: Extracted dataset Set3 ::::::::::::::::')\n",
    "else:\n",
    "        print(\"Files exist. Nothing to extract\")\n",
    "    \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-AuOi0BfzqL"
   },
   "source": [
    "\n",
    "## Load Train & Test from pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190556,
     "status": "ok",
     "timestamp": 1628721728473,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "uK5G2j0QfzWT",
    "outputId": "7a69ce0f-71bf-4c4b-fbff-45d4b2e22aa2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO  ] Reading raw text and transforming it into object Texto.\n",
      "topic:  set12_train\n",
      "topic:  set12_train/403\n",
      "534.txt\n",
      "633.txt\n",
      "1432.txt\n",
      "1410.txt\n",
      "824.txt\n",
      "941.txt\n",
      "2500.txt\n",
      "1962.txt\n",
      "1662.txt\n",
      "113.txt\n",
      "2015.txt\n",
      "571.txt\n",
      "2435.txt\n",
      "771.txt\n",
      "434.txt\n",
      "1744.txt\n",
      "2324.txt\n",
      "2478.txt\n",
      "1479.txt\n",
      "1856.txt\n",
      "2301.txt\n",
      "168.txt\n",
      "2454.txt\n",
      "11.txt\n",
      "790.txt\n",
      "629.txt\n",
      "1569.txt\n",
      "1618.txt\n",
      "2511.txt\n",
      "1078.txt\n",
      "1731.txt\n",
      "1843.txt\n",
      "1816.txt\n",
      "286.txt\n",
      "1509.txt\n",
      "1467.txt\n",
      "2462.txt\n",
      "308.txt\n",
      "1241.txt\n",
      "1934.txt\n",
      "1710.txt\n",
      "1776.txt\n",
      "2503.txt\n",
      "765.txt\n",
      "2069.txt\n",
      "445.txt\n",
      "1638.txt\n",
      "431.txt\n",
      "1411.txt\n",
      "1624.txt\n",
      "1488.txt\n",
      "235.txt\n",
      "40.txt\n",
      "1430.txt\n",
      "2092.txt\n",
      "582.txt\n",
      "1313.txt\n",
      "2470.txt\n",
      "475.txt\n",
      "321.txt\n",
      "90.txt\n",
      "1767.txt\n",
      "1740.txt\n",
      "2459.txt\n",
      "1034.txt\n",
      "18.txt\n",
      "2474.txt\n",
      "1210.txt\n",
      "897.txt\n",
      "1595.txt\n",
      "2036.txt\n",
      "1490.txt\n",
      "1081.txt\n",
      "1300.txt\n",
      "955.txt\n",
      "277.txt\n",
      "2416.txt\n",
      "2521.txt\n",
      "2419.txt\n",
      "147.txt\n",
      "1968.txt\n",
      "1769.txt\n",
      "2550.txt\n",
      "1860.txt\n",
      "874.txt\n",
      "2122.txt\n",
      "604.txt\n",
      "621.txt\n",
      "1759.txt\n",
      "550.txt\n",
      "483.txt\n",
      "1862.txt\n",
      "469.txt\n",
      "1522.txt\n",
      "1868.txt\n",
      "367.txt\n",
      "429.txt\n",
      "2119.txt\n",
      "156.txt\n",
      "1758.txt\n",
      "636.txt\n",
      "2310.txt\n",
      "542.txt\n",
      "1973.txt\n",
      "1386.txt\n",
      "2560.txt\n",
      "1854.txt\n",
      "615.txt\n",
      "1930.txt\n",
      "458.txt\n",
      "834.txt\n",
      "783.txt\n",
      "262.txt\n",
      "723.txt\n",
      "307.txt\n",
      "151.txt\n",
      "2024.txt\n",
      "227.txt\n",
      "1591.txt\n",
      "1992.txt\n",
      "1701.txt\n",
      "2135.txt\n",
      "1443.txt\n",
      "2228.txt\n",
      "1516.txt\n",
      "294.txt\n",
      "97.txt\n",
      "1180.txt\n",
      "62.txt\n",
      "257.txt\n",
      "1310.txt\n",
      "651.txt\n",
      "2410.txt\n",
      "1565.txt\n",
      "1779.txt\n",
      "699.txt\n",
      "2517.txt\n",
      "1186.txt\n",
      "1179.txt\n",
      "2213.txt\n",
      "305.txt\n",
      "1821.txt\n",
      "452.txt\n",
      "722.txt\n",
      "1167.txt\n",
      "1446.txt\n",
      "472.txt\n",
      "1783.txt\n",
      "759.txt\n",
      "1114.txt\n",
      "583.txt\n",
      "1804.txt\n",
      "527.txt\n",
      "442.txt\n",
      "66.txt\n",
      "2164.txt\n",
      "768.txt\n",
      "2256.txt\n",
      "2422.txt\n",
      "5.txt\n",
      "1135.txt\n",
      "1460.txt\n",
      "1305.txt\n",
      "1359.txt\n",
      "1043.txt\n",
      "6.txt\n",
      "726.txt\n",
      "784.txt\n",
      "291.txt\n",
      "2025.txt\n",
      "152.txt\n",
      "2168.txt\n",
      "1958.txt\n",
      "682.txt\n",
      "1941.txt\n",
      "1040.txt\n",
      "1533.txt\n",
      "1097.txt\n",
      "2531.txt\n",
      "580.txt\n",
      "2565.txt\n",
      "1100.txt\n",
      "1076.txt\n",
      "1983.txt\n",
      "870.txt\n",
      "1906.txt\n",
      "1175.txt\n",
      "2398.txt\n",
      "1271.txt\n",
      "1426.txt\n",
      "193.txt\n",
      "75.txt\n",
      "2095.txt\n",
      "114.txt\n",
      "1198.txt\n",
      "1397.txt\n",
      "1128.txt\n",
      "1164.txt\n",
      "465.txt\n",
      "1507.txt\n",
      "2466.txt\n",
      "1990.txt\n",
      "1222.txt\n",
      "597.txt\n",
      "1056.txt\n",
      "1535.txt\n",
      "925.txt\n",
      "2033.txt\n",
      "1182.txt\n",
      "645.txt\n",
      "2413.txt\n",
      "407.txt\n",
      "2433.txt\n",
      "1037.txt\n",
      "988.txt\n",
      "837.txt\n",
      "654.txt\n",
      "1835.txt\n",
      "1283.txt\n",
      "2369.txt\n",
      "1234.txt\n",
      "397.txt\n",
      "1414.txt\n",
      "1622.txt\n",
      "952.txt\n",
      "1470.txt\n",
      "991.txt\n",
      "2524.txt\n",
      "627.txt\n",
      "1185.txt\n",
      "2417.txt\n",
      "2089.txt\n",
      "1345.txt\n",
      "750.txt\n",
      "2339.txt\n",
      "1580.txt\n",
      "266.txt\n",
      "73.txt\n",
      "1741.txt\n",
      "1586.txt\n",
      "72.txt\n",
      "762.txt\n",
      "230.txt\n",
      "2210.txt\n",
      "841.txt\n",
      "1138.txt\n",
      "2492.txt\n",
      "48.txt\n",
      "1824.txt\n",
      "1610.txt\n",
      "296.txt\n",
      "911.txt\n",
      "673.txt\n",
      "2559.txt\n",
      "1352.txt\n",
      "1111.txt\n",
      "2191.txt\n",
      "301.txt\n",
      "1500.txt\n",
      "2153.txt\n",
      "190.txt\n",
      "2571.txt\n",
      "2480.txt\n",
      "1206.txt\n",
      "2000.txt\n",
      "1629.txt\n",
      "1367.txt\n",
      "16.txt\n",
      "2288.txt\n",
      "1602.txt\n",
      "2318.txt\n",
      "1904.txt\n",
      "2188.txt\n",
      "1614.txt\n",
      "626.txt\n",
      "1399.txt\n",
      "2184.txt\n",
      "1032.txt\n",
      "958.txt\n",
      "1312.txt\n",
      "1035.txt\n",
      "2011.txt\n",
      "2045.txt\n",
      "666.txt\n",
      "1298.txt\n",
      "1734.txt\n",
      "2267.txt\n",
      "1456.txt\n",
      "68.txt\n",
      "718.txt\n",
      "15.txt\n",
      "864.txt\n",
      "528.txt\n",
      "1236.txt\n",
      "700.txt\n",
      "960.txt\n",
      "1142.txt\n",
      "561.txt\n",
      "1092.txt\n",
      "2225.txt\n",
      "219.txt\n",
      "2220.txt\n",
      "1474.txt\n",
      "694.txt\n",
      "1594.txt\n",
      "94.txt\n",
      "2573.txt\n",
      "162.txt\n",
      "891.txt\n",
      "1501.txt\n",
      "2192.txt\n",
      "91.txt\n",
      "1130.txt\n",
      "869.txt\n",
      "916.txt\n",
      "830.txt\n",
      "1145.txt\n",
      "863.txt\n",
      "1260.txt\n",
      "1442.txt\n",
      "1137.txt\n",
      "1162.txt\n",
      "1099.txt\n",
      "2553.txt\n",
      "2034.txt\n",
      "2283.txt\n",
      "413.txt\n",
      "2113.txt\n",
      "1827.txt\n",
      "134.txt\n",
      "851.txt\n",
      "382.txt\n",
      "1266.txt\n",
      "1151.txt\n",
      "770.txt\n",
      "1682.txt\n",
      "924.txt\n",
      "1353.txt\n",
      "1482.txt\n",
      "178.txt\n",
      "2194.txt\n",
      "1388.txt\n",
      "504.txt\n",
      "2494.txt\n",
      "440.txt\n",
      "2243.txt\n",
      "297.txt\n",
      "810.txt\n",
      "2445.txt\n",
      "1459.txt\n",
      "2055.txt\n",
      "1366.txt\n",
      "906.txt\n",
      "1269.txt\n",
      "2377.txt\n",
      "1844.txt\n",
      "2356.txt\n",
      "1699.txt\n",
      "2381.txt\n",
      "375.txt\n",
      "32.txt\n",
      "111.txt\n",
      "2160.txt\n",
      "2463.txt\n",
      "241.txt\n",
      "254.txt\n",
      "2360.txt\n",
      "2576.txt\n",
      "222.txt\n",
      "2303.txt\n",
      "753.txt\n",
      "2294.txt\n",
      "1127.txt\n",
      "558.txt\n",
      "1857.txt\n",
      "973.txt\n",
      "2510.txt\n",
      "396.txt\n",
      "1435.txt\n",
      "1641.txt\n",
      "2312.txt\n",
      "2071.txt\n",
      "1996.txt\n",
      "921.txt\n",
      "703.txt\n",
      "1119.txt\n",
      "2211.txt\n",
      "708.txt\n",
      "990.txt\n",
      "1790.txt\n",
      "2174.txt\n",
      "3.txt\n",
      "85.txt\n",
      "2232.txt\n",
      "1739.txt\n",
      "132.txt\n",
      "1394.txt\n",
      "2139.txt\n",
      "802.txt\n",
      "1704.txt\n",
      "1358.txt\n",
      "974.txt\n",
      "2101.txt\n",
      "1583.txt\n",
      "1369.txt\n",
      "2490.txt\n",
      "384.txt\n",
      "1527.txt\n",
      "1079.txt\n",
      "284.txt\n",
      "437.txt\n",
      "919.txt\n",
      "2003.txt\n",
      "2279.txt\n",
      "404.txt\n",
      "671.txt\n",
      "1376.txt\n",
      "2065.txt\n",
      "1493.txt\n",
      "2446.txt\n",
      "123.txt\n",
      "2509.txt\n",
      "930.txt\n",
      "1817.txt\n",
      "107.txt\n",
      "1649.txt\n",
      "643.txt\n",
      "2364.txt\n",
      "1789.txt\n",
      "1512.txt\n",
      "1604.txt\n",
      "922.txt\n",
      "2274.txt\n",
      "1285.txt\n",
      "2037.txt\n",
      "149.txt\n",
      "487.txt\n",
      "1965.txt\n",
      "2515.txt\n",
      "484.txt\n",
      "1566.txt\n",
      "847.txt\n",
      "608.txt\n",
      "1853.txt\n",
      "572.txt\n",
      "2390.txt\n",
      "1562.txt\n",
      "1319.txt\n",
      "1803.txt\n",
      "2046.txt\n",
      "894.txt\n",
      "2229.txt\n",
      "1646.txt\n",
      "787.txt\n",
      "1408.txt\n",
      "1428.txt\n",
      "2522.txt\n",
      "587.txt\n",
      "1263.txt\n",
      "1725.txt\n",
      "1189.txt\n",
      "879.txt\n",
      "2163.txt\n",
      "886.txt\n",
      "109.txt\n",
      "1334.txt\n",
      "1075.txt\n",
      "1173.txt\n",
      "122.txt\n",
      "1919.txt\n",
      "895.txt\n",
      "917.txt\n",
      "2328.txt\n",
      "2402.txt\n",
      "1848.txt\n",
      "1777.txt\n",
      "646.txt\n",
      "701.txt\n",
      "1574.txt\n",
      "1156.txt\n",
      "191.txt\n",
      "544.txt\n",
      "2156.txt\n",
      "106.txt\n",
      "2290.txt\n",
      "2214.txt\n",
      "1451.txt\n",
      "970.txt\n",
      "1680.txt\n",
      "1949.txt\n",
      "1995.txt\n",
      "202.txt\n",
      "1339.txt\n",
      "744.txt\n",
      "474.txt\n",
      "904.txt\n",
      "1317.txt\n",
      "1066.txt\n",
      "676.txt\n",
      "1476.txt\n",
      "208.txt\n",
      "2171.txt\n",
      "507.txt\n",
      "887.txt\n",
      "1976.txt\n",
      "2313.txt\n",
      "598.txt\n",
      "574.txt\n",
      "1440.txt\n",
      "290.txt\n",
      "774.txt\n",
      "426.txt\n",
      "1267.txt\n",
      "2260.txt\n",
      "2393.txt\n",
      "655.txt\n",
      "1787.txt\n",
      "1636.txt\n",
      "664.txt\n",
      "845.txt\n",
      "807.txt\n",
      "2185.txt\n",
      "2543.txt\n",
      "175.txt\n",
      "1986.txt\n",
      "2581.txt\n",
      "1799.txt\n",
      "124.txt\n",
      "964.txt\n",
      "2451.txt\n",
      "2287.txt\n",
      "1121.txt\n",
      "2293.txt\n",
      "438.txt\n",
      "416.txt\n",
      "2167.txt\n",
      "2484.txt\n",
      "2201.txt\n",
      "393.txt\n",
      "1912.txt\n",
      "2203.txt\n",
      "2317.txt\n",
      "2395.txt\n",
      "1341.txt\n",
      "480.txt\n",
      "2354.txt\n",
      "1094.txt\n",
      "1275.txt\n",
      "1910.txt\n",
      "1245.txt\n",
      "316.txt\n",
      "1390.txt\n",
      "857.txt\n",
      "1292.txt\n",
      "1924.txt\n",
      "997.txt\n",
      "153.txt\n",
      "1073.txt\n",
      "2436.txt\n",
      "1572.txt\n",
      "2111.txt\n",
      "2467.txt\n",
      "324.txt\n",
      "2080.txt\n",
      "1401.txt\n",
      "1944.txt\n",
      "1361.txt\n",
      "135.txt\n",
      "859.txt\n",
      "1228.txt\n",
      "369.txt\n",
      "2424.txt\n",
      "1104.txt\n",
      "618.txt\n",
      "315.txt\n",
      "1955.txt\n",
      "194.txt\n",
      "667.txt\n",
      "1405.txt\n",
      "1523.txt\n",
      "2272.txt\n",
      "435.txt\n",
      "1503.txt\n",
      "2086.txt\n",
      "1506.txt\n",
      "1890.txt\n",
      "1977.txt\n",
      "1497.txt\n",
      "1360.txt\n",
      "34.txt\n",
      "1254.txt\n",
      "0.txt\n",
      "1752.txt\n",
      "391.txt\n",
      "446.txt\n",
      "815.txt\n",
      "1928.txt\n",
      "508.txt\n",
      "41.txt\n",
      "1363.txt\n",
      "2421.txt\n",
      "2263.txt\n",
      "2380.txt\n",
      "1756.txt\n",
      "2208.txt\n",
      "871.txt\n",
      "23.txt\n",
      "424.txt\n",
      "1115.txt\n",
      "1294.txt\n",
      "510.txt\n",
      "740.txt\n",
      "460.txt\n",
      "1830.txt\n",
      "1598.txt\n",
      "1295.txt\n",
      "10.txt\n",
      "421.txt\n",
      "2307.txt\n",
      "2181.txt\n",
      "2309.txt\n",
      "947.txt\n",
      "1536.txt\n",
      "1668.txt\n",
      "379.txt\n",
      "1029.txt\n",
      "1735.txt\n",
      "1643.txt\n",
      "1069.txt\n",
      "2320.txt\n",
      "1749.txt\n",
      "1690.txt\n",
      "1554.txt\n",
      "557.txt\n",
      "1051.txt\n",
      "2536.txt\n",
      "934.txt\n",
      "912.txt\n",
      "1239.txt\n",
      "1278.txt\n",
      "1695.txt\n",
      "304.txt\n",
      "1355.txt\n",
      "1576.txt\n",
      "610.txt\n",
      "1863.txt\n",
      "714.txt\n",
      "272.txt\n",
      "735.txt\n",
      "471.txt\n",
      "985.txt\n",
      "249.txt\n",
      "1392.txt\n",
      "2075.txt\n",
      "2549.txt\n",
      "2404.txt\n",
      "1016.txt\n",
      "51.txt\n",
      "987.txt\n",
      "1209.txt\n",
      "545.txt\n",
      "1450.txt\n",
      "361.txt\n",
      "400.txt\n",
      "2009.txt\n",
      "613.txt\n",
      "878.txt\n",
      "1461.txt\n",
      "187.txt\n",
      "181.txt\n",
      "2002.txt\n",
      "1589.txt\n",
      "2150.txt\n",
      "1874.txt\n",
      "732.txt\n",
      "1117.txt\n",
      "61.txt\n",
      "2093.txt\n",
      "567.txt\n",
      "2158.txt\n",
      "637.txt\n",
      "259.txt\n",
      "1375.txt\n",
      "2556.txt\n",
      "1782.txt\n",
      "717.txt\n",
      "715.txt\n",
      "84.txt\n",
      "1202.txt\n",
      "196.txt\n",
      "538.txt\n",
      "1232.txt\n",
      "1342.txt\n",
      "889.txt\n",
      "1383.txt\n",
      "1453.txt\n",
      "200.txt\n",
      "505.txt\n",
      "1237.txt\n",
      "2030.txt\n",
      "1416.txt\n",
      "327.txt\n",
      "237.txt\n",
      "2554.txt\n",
      "2264.txt\n",
      "720.txt\n",
      "1998.txt\n",
      "2432.txt\n",
      "2007.txt\n",
      "822.txt\n",
      "236.txt\n",
      "2546.txt\n",
      "1828.txt\n",
      "1597.txt\n",
      "2296.txt\n",
      "555.txt\n",
      "494.txt\n",
      "1878.txt\n",
      "443.txt\n",
      "2353.txt\n",
      "1208.txt\n",
      "909.txt\n",
      "1143.txt\n",
      "355.txt\n",
      "57.txt\n",
      "333.txt\n",
      "1557.txt\n",
      "2012.txt\n",
      "1747.txt\n",
      "2155.txt\n",
      "1297.txt\n",
      "1483.txt\n",
      "80.txt\n",
      "2141.txt\n",
      "270.txt\n",
      "1176.txt\n",
      "1248.txt\n",
      "376.txt\n",
      "1540.txt\n",
      "1888.txt\n",
      "142.txt\n",
      "2.txt\n",
      "747.txt\n",
      "2389.txt\n",
      "685.txt\n",
      "44.txt\n",
      "2347.txt\n",
      "1531.txt\n",
      "1578.txt\n",
      "2357.txt\n",
      "234.txt\n",
      "502.txt\n",
      "2006.txt\n",
      "2057.txt\n",
      "1286.txt\n",
      "1438.txt\n",
      "448.txt\n",
      "1372.txt\n",
      "422.txt\n",
      "966.txt\n",
      "2508.txt\n",
      "1084.txt\n",
      "127.txt\n",
      "86.txt\n",
      "796.txt\n",
      "1002.txt\n",
      "2145.txt\n",
      "2118.txt\n",
      "1721.txt\n",
      "1686.txt\n",
      "592.txt\n",
      "2161.txt\n",
      "1984.txt\n",
      "1211.txt\n",
      "967.txt\n",
      "338.txt\n",
      "358.txt\n",
      "697.txt\n",
      "2049.txt\n",
      "594.txt\n",
      "779.txt\n",
      "2027.txt\n",
      "2448.txt\n",
      "1155.txt\n",
      "1257.txt\n",
      "707.txt\n",
      "184.txt\n",
      "1205.txt\n",
      "2537.txt\n",
      "2200.txt\n",
      "155.txt\n",
      "2345.txt\n",
      "2073.txt\n",
      "725.txt\n",
      "343.txt\n",
      "1381.txt\n",
      "1746.txt\n",
      "575.txt\n",
      "642.txt\n",
      "1001.txt\n",
      "710.txt\n",
      "2204.txt\n",
      "688.txt\n",
      "1859.txt\n",
      "2043.txt\n",
      "1793.txt\n",
      "1663.txt\n",
      "961.txt\n",
      "1943.txt\n",
      "319.txt\n",
      "2321.txt\n",
      "1950.txt\n",
      "2040.txt\n",
      "2399.txt\n",
      "1480.txt\n",
      "2566.txt\n",
      "2438.txt\n",
      "1683.txt\n",
      "875.txt\n",
      "2108.txt\n",
      "2018.txt\n",
      "711.txt\n",
      "1096.txt\n",
      "2078.txt\n",
      "829.txt\n",
      "341.txt\n",
      "2142.txt\n",
      "595.txt\n",
      "1677.txt\n",
      "2518.txt\n",
      "704.txt\n",
      "2545.txt\n",
      "2159.txt\n",
      "1605.txt\n",
      "1514.txt\n",
      "907.txt\n",
      "2088.txt\n",
      "366.txt\n",
      "1149.txt\n",
      "372.txt\n",
      "1057.txt\n",
      "2136.txt\n",
      "1673.txt\n",
      "767.txt\n",
      "2383.txt\n",
      "414.txt\n",
      "204.txt\n",
      "1669.txt\n",
      "1884.txt\n",
      "1832.txt\n",
      "2275.txt\n",
      "1795.txt\n",
      "1960.txt\n",
      "1316.txt\n",
      "1080.txt\n",
      "2266.txt\n",
      "2329.txt\n",
      "758.txt\n",
      "946.txt\n",
      "1511.txt\n",
      "492.txt\n",
      "1687.txt\n",
      "1538.txt\n",
      "486.txt\n",
      "652.txt\n",
      "373.txt\n",
      "1085.txt\n",
      "119.txt\n",
      "2052.txt\n",
      "283.txt\n",
      "258.txt\n",
      "547.txt\n",
      "786.txt\n",
      "2115.txt\n",
      "578.txt\n",
      "1251.txt\n",
      "38.txt\n",
      "1656.txt\n",
      "2028.txt\n",
      "2109.txt\n",
      "2497.txt\n",
      "994.txt\n",
      "2102.txt\n",
      "1038.txt\n",
      "2482.txt\n",
      "525.txt\n",
      "2271.txt\n",
      "835.txt\n",
      "2259.txt\n",
      "1246.txt\n",
      "979.txt\n",
      "1200.txt\n",
      "1568.txt\n",
      "2083.txt\n",
      "954.txt\n",
      "2173.txt\n",
      "1230.txt\n",
      "1166.txt\n",
      "1846.txt\n",
      "1954.txt\n",
      "1770.txt\n",
      "2551.txt\n",
      "198.txt\n",
      "1957.txt\n",
      "131.txt\n",
      "1249.txt\n",
      "2506.txt\n",
      "2488.txt\n",
      "2425.txt\n",
      "213.txt\n",
      "1529.txt\n",
      "1009.txt\n",
      "22.txt\n",
      "883.txt\n",
      "931.txt\n",
      "2534.txt\n",
      "1146.txt\n",
      "2231.txt\n",
      "942.txt\n",
      "2525.txt\n",
      "548.txt\n",
      "2442.txt\n",
      "7.txt\n",
      "513.txt\n",
      "1472.txt\n",
      "214.txt\n",
      "410.txt\n",
      "1464.txt\n",
      "1608.txt\n",
      "1242.txt\n",
      "521.txt\n",
      "2239.txt\n",
      "2582.txt\n",
      "2082.txt\n",
      "2216.txt\n",
      "1041.txt\n",
      "776.txt\n",
      "264.txt\n",
      "1519.txt\n",
      "242.txt\n",
      "1219.txt\n",
      "569.txt\n",
      "1719.txt\n",
      "2372.txt\n",
      "777.txt\n",
      "2514.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895.txt\n",
      "2281.txt\n",
      "364.txt\n",
      "814.txt\n",
      "463.txt\n",
      "728.txt\n",
      "1466.txt\n",
      "2530.txt\n",
      "388.txt\n",
      "2104.txt\n",
      "2132.txt\n",
      "1761.txt\n",
      "1937.txt\n",
      "938.txt\n",
      "357.txt\n",
      "201.txt\n",
      "35.txt\n",
      "1621.txt\n",
      "1728.txt\n",
      "842.txt\n",
      "692.txt\n",
      "299.txt\n",
      "639.txt\n",
      "818.txt\n",
      "1030.txt\n",
      "1378.txt\n",
      "512.txt\n",
      "224.txt\n",
      "197.txt\n",
      "1896.txt\n",
      "325.txt\n",
      "2336.txt\n",
      "1223.txt\n",
      "1063.txt\n",
      "82.txt\n",
      "1065.txt\n",
      "1045.txt\n",
      "1588.txt\n",
      "1921.txt\n",
      "302.txt\n",
      "1823.txt\n",
      "95.txt\n",
      "1337.txt\n",
      "2456.txt\n",
      "363.txt\n",
      "1303.txt\n",
      "524.txt\n",
      "2473.txt\n",
      "1012.txt\n",
      "978.txt\n",
      "158.txt\n",
      "683.txt\n",
      "686.txt\n",
      "1054.txt\n",
      "820.txt\n",
      "1987.txt\n",
      "2222.txt\n",
      "1979.txt\n",
      "1763.txt\n",
      "92.txt\n",
      "2351.txt\n",
      "900.txt\n",
      "2280.txt\n",
      "280.txt\n",
      "1786.txt\n",
      "1812.txt\n",
      "554.txt\n",
      "1413.txt\n",
      "1753.txt\n",
      "679.txt\n",
      "2124.txt\n",
      "2577.txt\n",
      "1006.txt\n",
      "1810.txt\n",
      "996.txt\n",
      "899.txt\n",
      "761.txt\n",
      "2363.txt\n",
      "1963.txt\n",
      "2017.txt\n",
      "1192.txt\n",
      "1445.txt\n",
      "1946.txt\n",
      "849.txt\n",
      "1903.txt\n",
      "2335.txt\n",
      "2396.txt\n",
      "279.txt\n",
      "225.txt\n",
      "2557.txt\n",
      "387.txt\n",
      "2085.txt\n",
      "243.txt\n",
      "1417.txt\n",
      "1974.txt\n",
      "2505.txt\n",
      "1250.txt\n",
      "1585.txt\n",
      "999.txt\n",
      "1289.txt\n",
      "1088.txt\n",
      "2021.txt\n",
      "2217.txt\n",
      "1021.txt\n",
      "2258.txt\n",
      "203.txt\n",
      "868.txt\n",
      "46.txt\n",
      "659.txt\n",
      "1947.txt\n",
      "2068.txt\n",
      "21.txt\n",
      "478.txt\n",
      "402.txt\n",
      "1993.txt\n",
      "311.txt\n",
      "1183.txt\n",
      "1582.txt\n",
      "2176.txt\n",
      "1169.txt\n",
      "2542.txt\n",
      "2412.txt\n",
      "1814.txt\n",
      "1256.txt\n",
      "1504.txt\n",
      "2540.txt\n",
      "1901.txt\n",
      "1543.txt\n",
      "2574.txt\n",
      "2250.txt\n",
      "1526.txt\n",
      "737.txt\n",
      "1409.txt\n",
      "1327.txt\n",
      "585.txt\n",
      "522.txt\n",
      "1025.txt\n",
      "1819.txt\n",
      "160.txt\n",
      "1609.txt\n",
      "144.txt\n",
      "1755.txt\n",
      "1191.txt\n",
      "957.txt\n",
      "2042.txt\n",
      "1834.txt\n",
      "1592.txt\n",
      "1601.txt\n",
      "59.txt\n",
      "1773.txt\n",
      "2326.txt\n",
      "2579.txt\n",
      "1698.txt\n",
      "1938.txt\n",
      "1737.txt\n",
      "1423.txt\n",
      "1346.txt\n",
      "310.txt\n",
      "2177.txt\n",
      "560.txt\n",
      "1172.txt\n",
      "26.txt\n",
      "936.txt\n",
      "1918.txt\n",
      "288.txt\n",
      "2405.txt\n",
      "674.txt\n",
      "1195.txt\n",
      "566.txt\n",
      "2051.txt\n",
      "1364.txt\n",
      "2502.txt\n",
      "1253.txt\n",
      "2048.txt\n",
      "1907.txt\n",
      "370.txt\n",
      "2236.txt\n",
      "519.txt\n",
      "1329.txt\n",
      "634.txt\n",
      "457.txt\n",
      "817.txt\n",
      "275.txt\n",
      "2277.txt\n",
      "1120.txt\n",
      "2458.txt\n",
      "199.txt\n",
      "773.txt\n",
      "268.txt\n",
      "993.txt\n",
      "1726.txt\n",
      "854.txt\n",
      "247.txt\n",
      "1406.txt\n",
      "1486.txt\n",
      "2245.txt\n",
      "1899.txt\n",
      "631.txt\n",
      "2254.txt\n",
      "903.txt\n",
      "1019.txt\n",
      "764.txt\n",
      "892.txt\n",
      "1693.txt\n",
      "1449.txt\n",
      "1730.txt\n",
      "2062.txt\n",
      "217.txt\n",
      "2235.txt\n",
      "1675.txt\n",
      "640.txt\n",
      "793.txt\n",
      "852.txt\n",
      "1616.txt\n",
      "1261.txt\n",
      "1203.txt\n",
      "322.txt\n",
      "1927.txt\n",
      "496.txt\n",
      "828.txt\n",
      "250.txt\n",
      "49.txt\n",
      "1214.txt\n",
      "2261.txt\n",
      "1765.txt\n",
      "612.txt\n",
      "605.txt\n",
      "1711.txt\n",
      "1123.txt\n",
      "812.txt\n",
      "1013.txt\n",
      "329.txt\n",
      "2291.txt\n",
      "1125.txt\n",
      "1103.txt\n",
      "1494.txt\n",
      "1272.txt\n",
      "417.txt\n",
      "884.txt\n",
      "623.txt\n",
      "1966.txt\n",
      "428.txt\n",
      "2133.txt\n",
      "1561.txt\n",
      "1330.txt\n",
      "2439.txt\n",
      "1140.txt\n",
      "1429.txt\n",
      "1611.txt\n",
      "1671.txt\n",
      "2195.txt\n",
      "408.txt\n",
      "1627.txt\n",
      "2306.txt\n",
      "432.txt\n",
      "1071.txt\n",
      "754.txt\n",
      "1678.txt\n",
      "2376.txt\n",
      "731.txt\n",
      "13.txt\n",
      "2297.txt\n",
      "117.txt\n",
      "2106.txt\n",
      "831.txt\n",
      "1898.txt\n",
      "2198.txt\n",
      "2096.txt\n",
      "539.txt\n",
      "1496.txt\n",
      "101.txt\n",
      "2206.txt\n",
      "1047.txt\n",
      "2449.txt\n",
      "2129.txt\n",
      "2496.txt\n",
      "808.txt\n",
      "805.txt\n",
      "501.txt\n",
      "1463.txt\n",
      "1499.txt\n",
      "1544.txt\n",
      "1379.txt\n",
      "1541.txt\n",
      "1087.txt\n",
      "1419.txt\n",
      "1743.txt\n",
      "2014.txt\n",
      "2373.txt\n",
      "1090.txt\n",
      "8.txt\n",
      "1264.txt\n",
      "963.txt\n",
      "293.txt\n",
      "2452.txt\n",
      "1062.txt\n",
      "2146.txt\n",
      "1004.txt\n",
      "607.txt\n",
      "2099.txt\n",
      "1546.txt\n",
      "2077.txt\n",
      "74.txt\n",
      "1150.txt\n",
      "745.txt\n",
      "616.txt\n",
      "2477.txt\n",
      "353.txt\n",
      "1933.txt\n",
      "663.txt\n",
      "248.txt\n",
      "1281.txt\n",
      "2533.txt\n",
      "1840.txt\n",
      "499.txt\n",
      "1134.txt\n",
      "60.txt\n",
      "1477.txt\n",
      "2443.txt\n",
      "1048.txt\n",
      "2070.txt\n",
      "1227.txt\n",
      "1448.txt\n",
      "1349.txt\n",
      "1148.txt\n",
      "1022.txt\n",
      "696.txt\n",
      "1215.txt\n",
      "1291.txt\n",
      "360.txt\n",
      "1866.txt\n",
      "453.txt\n",
      "1729.txt\n",
      "2388.txt\n",
      "2182.txt\n",
      "1625.txt\n",
      "140.txt\n",
      "2400.txt\n",
      "1194.txt\n",
      "477.txt\n",
      "689.txt\n",
      "791.txt\n",
      "1356.txt\n",
      "1559.txt\n",
      "1108.txt\n",
      "2269.txt\n",
      "748.txt\n",
      "188.txt\n",
      "67.txt\n",
      "2548.txt\n",
      "2428.txt\n",
      "552.txt\n",
      "1199.txt\n",
      "2330.txt\n",
      "1395.txt\n",
      "89.txt\n",
      "1107.txt\n",
      "677.txt\n",
      "1331.txt\n",
      "1892.txt\n",
      "52.txt\n",
      "533.txt\n",
      "120.txt\n",
      "1722.txt\n",
      "1288.txt\n",
      "928.txt\n",
      "1539.txt\n",
      "1807.txt\n",
      "1916.txt\n",
      "1101.txt\n",
      "450.txt\n",
      "2110.txt\n",
      "topic:  set12_train/215\n",
      "37.txt\n",
      "168.txt\n",
      "88.txt\n",
      "40.txt\n",
      "90.txt\n",
      "12.txt\n",
      "43.txt\n",
      "151.txt\n",
      "97.txt\n",
      "66.txt\n",
      "5.txt\n",
      "105.txt\n",
      "129.txt\n",
      "152.txt\n",
      "172.txt\n",
      "179.txt\n",
      "114.txt\n",
      "148.txt\n",
      "73.txt\n",
      "16.txt\n",
      "81.txt\n",
      "162.txt\n",
      "186.txt\n",
      "32.txt\n",
      "111.txt\n",
      "47.txt\n",
      "3.txt\n",
      "85.txt\n",
      "154.txt\n",
      "123.txt\n",
      "174.txt\n",
      "149.txt\n",
      "109.txt\n",
      "191.txt\n",
      "106.txt\n",
      "20.txt\n",
      "202.txt\n",
      "208.txt\n",
      "175.txt\n",
      "135.txt\n",
      "28.txt\n",
      "23.txt\n",
      "27.txt\n",
      "51.txt\n",
      "187.txt\n",
      "181.txt\n",
      "130.txt\n",
      "84.txt\n",
      "167.txt\n",
      "70.txt\n",
      "80.txt\n",
      "142.txt\n",
      "2.txt\n",
      "9.txt\n",
      "44.txt\n",
      "206.txt\n",
      "184.txt\n",
      "155.txt\n",
      "69.txt\n",
      "58.txt\n",
      "180.txt\n",
      "77.txt\n",
      "204.txt\n",
      "126.txt\n",
      "38.txt\n",
      "198.txt\n",
      "131.txt\n",
      "157.txt\n",
      "22.txt\n",
      "189.txt\n",
      "87.txt\n",
      "139.txt\n",
      "108.txt\n",
      "201.txt\n",
      "125.txt\n",
      "197.txt\n",
      "158.txt\n",
      "92.txt\n",
      "56.txt\n",
      "98.txt\n",
      "102.txt\n",
      "46.txt\n",
      "115.txt\n",
      "30.txt\n",
      "183.txt\n",
      "19.txt\n",
      "160.txt\n",
      "144.txt\n",
      "128.txt\n",
      "59.txt\n",
      "118.txt\n",
      "165.txt\n",
      "171.txt\n",
      "136.txt\n",
      "55.txt\n",
      "93.txt\n",
      "49.txt\n",
      "78.txt\n",
      "195.txt\n",
      "53.txt\n",
      "117.txt\n",
      "177.txt\n",
      "74.txt\n",
      "60.txt\n",
      "140.txt\n",
      "67.txt\n",
      "topic:  set12_train/221\n",
      "2289.txt\n",
      "50.txt\n",
      "941.txt\n",
      "451.txt\n",
      "330.txt\n",
      "221.txt\n",
      "2379.txt\n",
      "1962.txt\n",
      "953.txt\n",
      "2137.txt\n",
      "113.txt\n",
      "2015.txt\n",
      "571.txt\n",
      "2568.txt\n",
      "2435.txt\n",
      "1744.txt\n",
      "2324.txt\n",
      "2478.txt\n",
      "1479.txt\n",
      "1856.txt\n",
      "2654.txt\n",
      "2301.txt\n",
      "168.txt\n",
      "693.txt\n",
      "11.txt\n",
      "2588.txt\n",
      "1618.txt\n",
      "1465.txt\n",
      "727.txt\n",
      "1909.txt\n",
      "1822.txt\n",
      "1843.txt\n",
      "2333.txt\n",
      "210.txt\n",
      "1509.txt\n",
      "2940.txt\n",
      "1932.txt\n",
      "532.txt\n",
      "843.txt\n",
      "2352.txt\n",
      "1124.txt\n",
      "1241.txt\n",
      "1934.txt\n",
      "1776.txt\n",
      "2503.txt\n",
      "765.txt\n",
      "2069.txt\n",
      "751.txt\n",
      "386.txt\n",
      "88.txt\n",
      "445.txt\n",
      "1727.txt\n",
      "1638.txt\n",
      "431.txt\n",
      "712.txt\n",
      "1624.txt\n",
      "1488.txt\n",
      "2905.txt\n",
      "915.txt\n",
      "235.txt\n",
      "40.txt\n",
      "1471.txt\n",
      "2092.txt\n",
      "1923.txt\n",
      "475.txt\n",
      "1935.txt\n",
      "1908.txt\n",
      "2227.txt\n",
      "2358.txt\n",
      "1244.txt\n",
      "1733.txt\n",
      "1767.txt\n",
      "1740.txt\n",
      "2459.txt\n",
      "1157.txt\n",
      "1956.txt\n",
      "1028.txt\n",
      "882.txt\n",
      "1207.txt\n",
      "2599.txt\n",
      "121.txt\n",
      "2097.txt\n",
      "2474.txt\n",
      "1210.txt\n",
      "897.txt\n",
      "1595.txt\n",
      "1490.txt\n",
      "910.txt\n",
      "2631.txt\n",
      "2649.txt\n",
      "2416.txt\n",
      "2005.txt\n",
      "2521.txt\n",
      "1258.txt\n",
      "1659.txt\n",
      "736.txt\n",
      "1769.txt\n",
      "1391.txt\n",
      "734.txt\n",
      "1689.txt\n",
      "12.txt\n",
      "1897.txt\n",
      "2861.txt\n",
      "2790.txt\n",
      "874.txt\n",
      "2122.txt\n",
      "2683.txt\n",
      "1759.txt\n",
      "550.txt\n",
      "483.txt\n",
      "1862.txt\n",
      "2591.txt\n",
      "2170.txt\n",
      "1522.txt\n",
      "2879.txt\n",
      "649.txt\n",
      "658.txt\n",
      "2951.txt\n",
      "1664.txt\n",
      "2898.txt\n",
      "429.txt\n",
      "2119.txt\n",
      "636.txt\n",
      "542.txt\n",
      "2151.txt\n",
      "1973.txt\n",
      "1854.txt\n",
      "856.txt\n",
      "14.txt\n",
      "783.txt\n",
      "262.txt\n",
      "2286.txt\n",
      "723.txt\n",
      "2223.txt\n",
      "307.txt\n",
      "1628.txt\n",
      "1999.txt\n",
      "1760.txt\n",
      "151.txt\n",
      "170.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-58f47d8307a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topic: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m               \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTexto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m               \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d56ec16aa3d7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, idx, path, spacy_doc)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# str.lower() added by Ceci\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# all the tokens in the doc, no filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTexto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m#self.tokens = Texto.nlp(self.raw_text, disable=['textcat','ner','tagger','parser'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/pipeline/tok2vec.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mtokvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     30\u001b[0m ) -> Tuple[SeqT, Callable]:\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSeqT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ragged_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPadded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSeqT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_padded_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36m_ragged_forward\u001b[0;34m(model, Xr, is_train)\u001b[0m\n\u001b[1;32m     85\u001b[0m ) -> Tuple[Ragged, Callable]:\n\u001b[1;32m     86\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mArrayXd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArrayXd\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataXd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdYr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRagged\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRagged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/layers/maxout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape2f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape1f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "#!mkdir -p db/cache\n",
    "if not (os.path.isfile('pickle_texts/dmoz_train_25.p')):\n",
    "    \n",
    "    !mkdir -p 'pickle_texts'\n",
    "    print('[ INFO  ] Reading raw text and transforming it into object Texto.')\n",
    "\n",
    "    #path_train = 'set12_train_3_topics/'\n",
    "    path_train = 'set12_train'\n",
    "\n",
    "\n",
    "    idx = 0\n",
    "    train = []\n",
    "    for dirpath,_,filenames in os.walk(path_train):\n",
    "        print(\"topic: \", dirpath)\n",
    "        for filename in filenames:\n",
    "              train.append(Texto(idx, os.path.join(dirpath, filename)))\n",
    "              idx += 1\n",
    "              print(filename)\n",
    "    pickle.dump(train, open('pickle_texts/dmoz_train_25.p','wb'))\n",
    "    print('[ INFO  ] Save train set into pickles')\n",
    "    print('train set size: {}'.format(len(train)))\n",
    "    \n",
    "else:\n",
    "    print('[ INFO  ] Retrieve train set from pickles')    \n",
    "    train = pickle.load(open('pickle_texts/dmoz_train_25.p', 'rb'))    \n",
    "    \n",
    "\n",
    "\n",
    "if not (os.path.isfile('pickle_texts/dmoz_test_25.p')):    \n",
    "    idx = 0\n",
    "    test = []\n",
    "    #path_test = 'set3_test_3_topics/'\n",
    "    path_test = 'set3_test'\n",
    "    for dirpath,_,filenames in os.walk(path_test):\n",
    "        print(\"topic: \", dirpath)\n",
    "        for filename in filenames:\n",
    "            test.append(Texto(idx, os.path.join(dirpath, filename)))\n",
    "            idx += 1\n",
    "    pickle.dump(test, open('pickle_texts/dmoz_test_25.p','wb'))\n",
    "    print('[ INFO  ] Save test set into pickles')\n",
    "    print('test set size:  {}'.format(len(test)))\n",
    "    \n",
    "else:\n",
    "    print('[ INFO  ] Retrieve test set from pickles')    \n",
    "    test = pickle.load(open('pickle_texts/dmoz_test_25.p', 'rb'))\n",
    "    \n",
    "print('[  OK   ] train/test sets loaded.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FrFhg9x9f-q"
   },
   "source": [
    "# Matrix and vocabulary building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKeQcspf9z3I"
   },
   "source": [
    "**Trigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44113,
     "status": "ok",
     "timestamp": 1628721772578,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "Js4tZptz8zw2",
    "outputId": "c2302e80-4c39-474b-a4ff-25e6d7bd5fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  OK   ] Trigrams computed: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def valid_token(token):\n",
    "    return not token is None and not token.is_stop and token.is_alpha and token.lemma_.isalnum() and not Texto.nlp.vocab[token.lemma_].is_stop\n",
    "\n",
    "def build_trigrams(textos, umbral=15):\n",
    "    freq = defaultdict(int)\n",
    "    for texto in textos:\n",
    "        \n",
    "        visited = set()\n",
    "        for idx in range(len((texto.tokens[:-2]))):\n",
    "            t1 = texto.tokens[idx]\n",
    "            t2 = texto.tokens[idx+1]\n",
    "            t3 = texto.tokens[idx+2]\n",
    "            \n",
    "            if valid_token(t1) and valid_token(t2) and valid_token(t3) and not (t1.lemma_,t2.lemma_,t3.lemma_) in visited:\n",
    "                freq[(t1.lemma_,t2.lemma_,t3.lemma_)] +=1 \n",
    "                visited.add((t1.lemma_,t2.lemma_,t3.lemma_))\n",
    "    return [word for word in freq if freq[word]>umbral]\n",
    "            \n",
    "####### CON TRAIN Y TEST PARA ARMAR LA MATRIZ DE VOCABULARIO (INDICE)\n",
    "trigrams = build_trigrams(train+test, umbral =35)                            # TEST: umbral=1, aparece al menos 2 veces)\n",
    "print('[  OK   ] Trigrams computed: {}'.format(len(trigrams)))\n",
    "#print(\"some trigrams: \", trigrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpZzAygN-DuM"
   },
   "source": [
    "**Bigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45445,
     "status": "ok",
     "timestamp": 1628721818007,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "WWnXJ8M8-Br5",
    "outputId": "81e5fa42-5398-431a-fd1a-2892d5e60b91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  OK   ] Bigrams computed: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_bigrams(textos,vocab, umbral=15):\n",
    "    freq = defaultdict(int)\n",
    "    for texto in textos:\n",
    "        visited = set()\n",
    "        for idx in range(len((texto.tokens[:-1]))):\n",
    "            t1 = texto.tokens[idx] \n",
    "            t2 = texto.tokens[idx+1]\n",
    "            t3 = texto.tokens[idx+2] if idx+2<len(texto.tokens) else None\n",
    "            \n",
    "                \n",
    "            \n",
    "            if valid_token(t1) and valid_token(t2) and not (not t3 is None and (t1.lemma_,t2.lemma_,t3.lemma_) in vocab) and not (t1.lemma_,t2.lemma_) in visited:\n",
    "                freq[(t1.lemma_,t2.lemma_)] +=1 \n",
    "                visited.add((t1.lemma_,t2.lemma_))\n",
    "\n",
    "    return [word for word in freq if freq[word]>umbral]\n",
    "            \n",
    "vocab = set(trigrams)\n",
    "####### CON TRAIN Y TEST PARA ARMAR LA MATRIZ DE VOCABULARIO (INDICE)\n",
    "bigrams = build_bigrams(train+test,vocab, umbral =35)                            # TEST: umbral=1, aparece al menos 2 veces)\n",
    "print('[  OK   ] Bigrams computed: {}'.format(len(bigrams)))\n",
    "#print(\"some bigrams: \", bigrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycKq9dEC-NTG"
   },
   "source": [
    "**Unigrams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69640,
     "status": "ok",
     "timestamp": 1628721887639,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "SoPTQXGV-QIu",
    "outputId": "513df88e-97ee-41d1-f57c-3551be569987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  OK   ] Unigrams computed: 0\n"
     ]
    }
   ],
   "source": [
    "def build_unigrams(textos,vocab, umbral=15):\n",
    "    freq = defaultdict(int)\n",
    "    for texto in textos:\n",
    "        visited = set()\n",
    "        for idx in range(len((texto.tokens))):\n",
    "            t1 = texto.tokens[idx]             \n",
    "            t2 = texto.tokens[idx+1] if idx+1<len(texto.tokens) else None            \n",
    "            t3 = texto.tokens[idx+2] if idx+2<len(texto.tokens) else None            \n",
    "            \n",
    "            if not t2 is None and (t1.lemma_,t2.lemma_) in vocab:\n",
    "                valid_bigram = True\n",
    "            else:\n",
    "                valid_bigram = False\n",
    "            \n",
    "            if not t3 is None and (t1.lemma_,t2.lemma_,t3.lemma_) in vocab:\n",
    "                valid_trigram=True\n",
    "            else:\n",
    "                valid_trigram=False                \n",
    "            \n",
    "            if valid_token(t1) and valid_token(t2) and not valid_bigram and not valid_trigram and not (t1.lemma_,) in visited:\n",
    "                freq[(t1.lemma_,)] +=1 \n",
    "                visited.add((t1.lemma_,))\n",
    "\n",
    "    return [word for word in freq if freq[word]>umbral]\n",
    "vocab = set(trigrams+bigrams)\n",
    "\n",
    "####### CON TRAIN Y TEST PARA ARMAR LA MATRIZ DE VOCABULARIO (INDICE)\n",
    "unigrams = build_unigrams(train+test, vocab,  umbral =35)        # TEST: umbral=1, aparece al menos 1 vez\n",
    "\n",
    "#unigrams = unigrams[0:20]\n",
    "print('[  OK   ] Unigrams computed: {}'.format(len(unigrams)))\n",
    "#print(\"some unigrams:\", unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5AqI1E6_V-g"
   },
   "source": [
    "**vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1628721887639,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "NL2inBuC_VPc",
    "outputId": "9f68597d-a097-4096-e981-8403db2255a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  OK   ] Vocab computed: 0\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(unigrams+bigrams+trigrams))\n",
    "print('[  OK   ] Vocab computed: {}'.format(len(vocab)))\n",
    "\n",
    "# if debug:  \n",
    "#   print(\"\\n\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75bs9Jr6_sjw"
   },
   "source": [
    "**Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48543,
     "status": "ok",
     "timestamp": 1628721936173,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "HFNGF69d_u-4",
    "outputId": "81fd9b85-7539-4937-caab-ca82853346c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training matrix: (0, 0)\n",
      "testing matrix: (0, 0)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0ee48be86c22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# f1.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_tfIdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"matriz con columna 0 ordenada de mayor a menor:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def build_matrix(textos, vocab):\n",
    "    m = np.zeros(shape=(len(textos),len(vocab)))\n",
    "    word2index = dict([(word,index) for index,word in enumerate(vocab)])    \n",
    "    # dict_df[termino]= nro de ocurrencias en docs del corpus. \n",
    "    dict_df = dict([(word,0) for word in vocab])\n",
    "        \n",
    "    for idx, text in enumerate(textos):\n",
    "        \n",
    "        set_tokens=set([]) # elimina repetidos - para saber si el token esta o no en el doc - calcular DF\n",
    "        \n",
    "        for tkn_idx in range(len(text.tokens)):\n",
    "            t1 = text.tokens[tkn_idx].lemma_\n",
    "            t2 = text.tokens[tkn_idx + 1].lemma_ if tkn_idx+1<len(text.tokens) else None\n",
    "            t3 = text.tokens[tkn_idx + 2].lemma_ if tkn_idx+2<len(text.tokens) else None\n",
    "            \n",
    "            unigram = (t1,)\n",
    "            bigram = (t1,t2) if not t2 is None else None\n",
    "            trigram = (t1,t2,t3) if not t3 is None and not t2 is None else None\n",
    "            \n",
    "            if not trigram is None and trigram in word2index:\n",
    "                m[idx,word2index[trigram]] += 1\n",
    "                set_tokens.add(trigram)\n",
    "            elif not bigram is None and bigram in word2index:                \n",
    "                m[idx, word2index[bigram]]+=1\n",
    "                set_tokens.add(bigram)\n",
    "            elif unigram in word2index:\n",
    "                m[idx,word2index[unigram]]+=1\n",
    "                set_tokens.add(unigram)\n",
    "        \n",
    "        for t in set_tokens:\n",
    "            dict_df[t]+=1\n",
    "    \n",
    "    ################################\n",
    "    ########## calculo matriz TF-IDF\n",
    "    ################################\n",
    "\n",
    "    # print(dict_df)\n",
    "    #print (\"dict_df[\",vocab[0],\"]=\",dict_df[vocab[0]], \"dict_df[\",vocab[1],\"]=\",dict_df[vocab[1]],\"vocab[2]=\",dict_df[vocab[2]])\n",
    "    \n",
    "    m_tfIdf = copy.deepcopy(m)\n",
    "    \n",
    "    corpus_size= len(textos)\n",
    "    \n",
    "    # multiplica cada columna (que tiene el TF) por el IDF del termino. \n",
    "    # El termino es la columna de la matriz\n",
    "    for indice in range(0, len(vocab)):\n",
    "        m_tfIdf[:, indice] = m_tfIdf[:,indice] * np.log(corpus_size / (dict_df[vocab[indice]] + 1 ) )\n",
    "        #m_tfIdf[:, indice] = m_tfIdf[:,indice] * 2  ## test              \n",
    "    \n",
    "    \n",
    "    return m, m_tfIdf\n",
    "\n",
    "m_test, m_tfIdf_test = build_matrix(test, vocab)\n",
    "m_train, m_tfIdf_train = build_matrix(train, vocab)\n",
    "t_testing = Texto.target(test)\n",
    "t_training = Texto.target(train)\n",
    "print('training matrix: {}'.format(m_train.shape))\n",
    "print('testing matrix: {}'.format(m_test.shape))\n",
    "\n",
    "# print('training matrix columna 0: {} equivale al primer termino de la matriz'.format(m_train[:, 0].shape))\n",
    "# f = open(\"original.txt\", \"w+\")\n",
    "# print(\"original=\")\n",
    "# print(m_train[0,:])\n",
    "\n",
    "# for a in range(0,len(m_train[0,:])):    \n",
    "#     f.write(str(m_train[0,a]))\n",
    "#     f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# f.close()\n",
    "\n",
    "# print(\"tfidf=\")\n",
    "\n",
    "# f1 = open(\"tfidf.txt\", \"w+\")\n",
    "# for a in range(0,len(m_tfIdf_train[0,:])):    \n",
    "#     f1.write(str(m_tfIdf_train[0,a]))\n",
    "#     f1.write(\"\\n\")\n",
    "    \n",
    "# f1.close()\n",
    "print(m_tfIdf_train[0,:])\n",
    "print(\"matriz con columna 0 ordenada de mayor a menor:\")\n",
    "\n",
    "## ordenamiento de columna (devuelve indices ordenados):\n",
    "indicesDeDocus = np.argsort(m_tfIdf_train[:,0])\n",
    "n=10\n",
    "#los primeros 10 de mayor a menor\n",
    "indicesDeDocus = indicesDeDocus[::-1][:n]\n",
    "\n",
    "print (\"TERMINO \", vocab[0])\n",
    "for i in range(0,9):\n",
    "    print(\"indice del documento recuperado \", indicesDeDocus[i])      \n",
    "    print(\"valor tf-idf \", m_tfIdf_train[indicesDeDocus[i], 0])\n",
    "    print(\"topico al que pertenece del train \", t_training[indicesDeDocus[i]])\n",
    "    print(\" ----------- \")\n",
    "\n",
    "print()\n",
    "print(\"vocab[0]=\", vocab[0])\n",
    "\n",
    "print('training target: {}'.format(t_training.shape))\n",
    "print('testing target: {}'.format(t_testing.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # el topico 529 pareciera ser el ultimo de la matriz\n",
    "# print(t_training==529)\n",
    "# best_100_words = get_best_word(m_train,t_training, 529, vocab,beta=beta)\n",
    "# # arma bien las consultas del sistema solar\n",
    "# print (best_100_words)\n",
    "\n",
    "# # topico 363 - air quality \n",
    "# print(t_training==363)\n",
    "# best_100_words = get_best_word(m_train,t_training, 363, vocab,beta=beta)\n",
    "# # arma bien las consultas del sistema solar\n",
    "# print (best_100_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1628721937403,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "oXxpwGXGDGzE",
    "outputId": "f8ab94cc-7ee3-4f89-e2e7-7076724873ae"
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "  print(\"vector de topicos de cada texto/Documento en la matriz TRAIN\")\n",
    "  #print(t_training)\n",
    "  print(\"\\nmatriz de TRAIN\")\n",
    "  #print(m_train)\n",
    "  print(\"\\nterminos en archivos de TRAIN a traves de la matriz\")\n",
    "  non_zero = np.argwhere(m_train != 0)\n",
    "  print(non_zero)\n",
    "  print(\"\\nterminos en archivos de TEST a traves de la matriz\")\n",
    "  non_zero = np.argwhere(m_test != 0)\n",
    "\n",
    "  print(non_zero)\n",
    "\n",
    "    \n",
    "\n",
    "  print(vocab[0])\n",
    "  print(vocab[1])\n",
    "  print(vocab[2])\n",
    "  print(vocab[3])\n",
    "  print(vocab[4])\n",
    "  print(vocab[5])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl-7xQ1IMdPi"
   },
   "source": [
    "# Term Weighting schemes\n",
    "\n",
    "• A denotes the number of documents that belong to class c_k and **contain** term t_i\n",
    "\n",
    "• B denotes the number of documents that belong to class c_k **but do not contain** the term t_i\n",
    "\n",
    "• C denotes the number of documents that **do not belong** to class c_k but **contain** the term t_i\n",
    "\n",
    "• D denotes the number of documents that **do not belong** to class c_k and **do not contain** the term t_i\n",
    "\n",
    "• N denotes the total number of documents in the collection (i.e., N = A + B + C + D).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1628721937404,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "yWXpjT2lMc3z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "eps = np.finfo(np.float64).eps\n",
    "eps\n",
    "\n",
    "# param matrix, matriz de Train o Test\n",
    "# param target: arreglo booleano. cant elementos como textos en matriz\n",
    "# TRUE, si el indice de la matrix corresponde a un docum del topico en cuestion, \n",
    "# FALSE en caso contrario\n",
    "def get_abcd(matrix, target):\n",
    "    m_bool = matrix.copy() > 0\n",
    "    \n",
    "    a = np.sum( m_bool &  target, axis=0)\n",
    "    b = np.sum(~m_bool &  target, axis=0)\n",
    "    c = np.sum( m_bool & ~target, axis=0)\n",
    "    d = np.sum(~m_bool & ~target, axis=0)\n",
    "    return a.reshape(1,len(a)),b.reshape(1,len(b)),c.reshape(1,len(c)),d.reshape(1,len(d))\n",
    "\n",
    "\n",
    "def get_best_word_method(matrix,target, category_idx, vocab, method, beta=0.477, testing=False):\n",
    "    # a y c son arreglos de longitud cantidad de terminos (vocab)-y cada elemento del arreglo representa el nro de documentos del tema y que tienen el termino (en A) \n",
    "    # C docus de otros topicos pero que si tienen el termibno\n",
    "    a,b,c,d=get_abcd(matrix,target==category_idx)\n",
    "    \n",
    "    score = []\n",
    "    if method.__name__ == 'MOGP':\n",
    "        print(\"no calcular nada, ya esta almacenado! -- ver que hacer\")\n",
    "    elif method.__name__ == 'fdd_05':\n",
    "        score = fdd(a,b,c,d,beta=0.5)\n",
    "    elif method.__name__ == 'fdd_1':\n",
    "        score = fdd(a,b,c,d,beta=1.0)\n",
    "    elif method.__name__ == 'fdd_10':\n",
    "        score = fdd(a,b,c,d,beta=10)        \n",
    "    elif \"igm\" in method.__name__:        \n",
    "        if testing:\n",
    "            t_testing = Texto.target(test) #al no pasar 2do argumento a target, devuelve los nros de topicos de cada doc\n",
    "            score = method(matrix,t_testing, category_idx )  # checked, usa category que es topic_number\n",
    "        else:                    \n",
    "            t_training = Texto.target(train)\n",
    "            score = method(matrix,t_training, category_idx )  # checked, usa category que es topic_number\n",
    "    else:\n",
    "        score = method(a,b,c,d)  \n",
    "    \n",
    "    words_100 = []\n",
    "    \n",
    "    print(\"score: \", score)\n",
    "    \n",
    "    \n",
    "    # score son punteros a los indices del arreglo de palabras\n",
    "    cant = 100\n",
    "    #cant=5\n",
    "    \n",
    "    score = np.argsort(score)[0]\n",
    "    print(\"np.argsort(score)[0,:]: \", score)\n",
    "    \n",
    "    for s in range(1, cant+1):\n",
    "        # score es de la forma [[ 1,2,3]] por eso el primer indice es 0\n",
    "        words_100.append(vocab[score[-s]])\n",
    "\n",
    "        #print(\"palabra \",-s, \" \",  vocab[np.argsort(score)[0,-s]], \" score \", np.argsort(score)[0,-s])\n",
    "    \n",
    "    \n",
    "    # las palabras ya las tengo en el orden correcto. El score esta desordenado\n",
    "   \n",
    "    return words_100, score\n",
    "\n",
    "\n",
    "# SOLO SIRVE PARA FDD\n",
    "def get_best_word(matrix,target, category_idx, vocab,beta=0.477):\n",
    "\n",
    "    a,b,c,d = get_abcd(matrix,target==category_idx)\n",
    "    score = fdd(a,b,c,d,beta=beta)\n",
    "    \n",
    "    # score es un arreglo con tantas filas como documentos del topico C_k que contienen al termino t_i\n",
    "    # print(\"lo que tiene score = \", score)\n",
    "\n",
    "    words_100 = []\n",
    "    \n",
    "    print(\"descomentar cant\")\n",
    "    cant = 100\n",
    "    #cant=5\n",
    "    for s in range(1, cant+1):\n",
    "        words_100.append(vocab[np.argsort(score)[0,-s]])\n",
    "        #print(\"palabra \",-s, \" \",  vocab[np.argsort(score)[0,-s]], \" score \", np.argsort(score)[0,-s])\n",
    "    \n",
    "    #return vocab[np.argsort(score)[0,-1]]\n",
    "    return words_100\n",
    "\n",
    "#en este caso, target es un arreglo de nro de topicos, corresponde al topic_id de cada doc de la matriz\n",
    "def get_ir_metrics(word, matrix, m_tfIdf, target, category_idx, vocab):\n",
    "    word_idx = vocab.index(word)\n",
    "    \n",
    "    vec = matrix[:,word_idx]\n",
    "    \n",
    "    #     for indi in range(0, len(vec)):\n",
    "    #         if vec[indi]>1:\n",
    "    #             print (vec[indi])\n",
    "    \n",
    "    cant_recuperados = np.sum(vec>0)\n",
    "    cant_relevantes = np.sum(target==category_idx)\n",
    "    \n",
    "\n",
    "    cant_relevantes_recuperados = np.sum((vec>0) & (target[:,0]==category_idx))\n",
    "    relevantes_recuperados = (vec>0) & (target[:,0]==category_idx)\n",
    "    \n",
    "    # para chequear si se estaban calculando bien los relevantes recuperados\n",
    "    #     relev_recuperados_a_mano=0\n",
    "    #     for t in range(0, vec.shape[0]):\n",
    "    #         if(vec[t]) and ((target[t,0]==category_idx)):\n",
    "    #             relev_recuperados_a_mano+=1\n",
    "            \n",
    "    \n",
    "    # ordenar de mayor a menor los indices de matriz tfIdf correspondientes a 'word' y tomar los 10 primeros   \n",
    "    #print(\"matriz con columna \",word , \" indice \",  word_idx,\" ordenada de mayor a menor:\")\n",
    "\n",
    "    ## ordenamiento de columna (devuelve indices ordenados):\n",
    "    indicesDeDocusA10 = np.argsort(m_tfIdf[:,word_idx])\n",
    "    n=10\n",
    "    #los primeros 10 de mayor a menor\n",
    "    indicesDeDocusA10 = indicesDeDocusA10[::-1][:n]\n",
    "\n",
    "    # print (\"TERMINO \", word)\n",
    "    \n",
    "#     for i in range(0,n-1):\n",
    "#         print(\"indice del documento recuperado \", indicesDeDocusA10[i])      \n",
    "#         print(\"valor tf-idf \", m_tfIdf[indicesDeDocusA10[i], word_idx])\n",
    "#         print(\"topico al que pertenece \", target[indicesDeDocusA10[i]])\n",
    "#         print(\" ----------- \")    \n",
    "    \n",
    "    cant_relevantes_recuperados_at_10 = 0\n",
    "    cant_recuperados_at_10 = 0\n",
    "    for i in range(0,n):\n",
    "        if (m_tfIdf[indicesDeDocusA10[i], word_idx]>0):\n",
    "            cant_recuperados_at_10 += 1\n",
    "            # si el TF-IDF no es cero entonces me fijo si es del topico o no...\n",
    "            if (target[indicesDeDocusA10[i]]==category_idx):\n",
    "                cant_relevantes_recuperados_at_10 += 1\n",
    "\n",
    "    #print(\"cant recuperados a 10 =\", cant_recuperados_at_10)\n",
    "    #print(\"cant relevantes recuperados a 10 =\", cant_relevantes_recuperados_at_10)\n",
    "    ##print(\"cant relevantes recuperados A MANO =\", relev_recuperados_a_mano)\n",
    "    #print(\"cant relevantes recuperados NORMAL =\", cant_relevantes_recuperados)\n",
    "    \n",
    "       \n",
    "    # precision clasica\n",
    "    #precision = cant_relevantes_recuperados/max(cant_recuperados,1.0)\n",
    "    \n",
    "    #precision a 10\n",
    "    precision = cant_relevantes_recuperados_at_10/max(cant_recuperados_at_10, 1.0)\n",
    "    \n",
    "    recall = cant_relevantes_recuperados/cant_relevantes\n",
    "    f1 = 2.0*((precision*recall)/(precision+recall+eps))\n",
    "\n",
    "    #     print(\"Precision = \", precision)\n",
    "    #     print(\"Recall = \", recall)\n",
    "    \n",
    "    # devuelvo cuales son los relevantes_recuperados para calcular GLOBAL RECALL\n",
    "    return precision, recall, f1, relevantes_recuperados\n",
    "\n",
    "# los relevantes_recuperados son de todas las 100 queries\n",
    "def globalRecall(relevantes_recuperados, target, category, vocab):    \n",
    "    cant_relevantes = np.sum(target==category)    \n",
    "    if cant_relevantes!=0:\n",
    "        global_recall = np.sum(relevantes_recuperados) / cant_relevantes\n",
    "    else:\n",
    "        global_recall = 0.0\n",
    "    return global_recall\n",
    "\n",
    "\n",
    "# best_100_words: las mejores 100 palabras (--> QUERIES) elegidas segun el metodo a probar \n",
    "# matrix: o bien es la matriz de train o la de test. cada celda i,j tiene la cant de veces que el termino j aparece en el doc i\n",
    "# relevantes recuperados: vector booleano-indica cuales son los docs relev recuperados\n",
    "# target (train o test): arreglo de numeros de topicos, para saber a que topico pertenece cada True/false del arreglo anterior\n",
    "# category: topic_number\n",
    "# vocab: todos los posibles terminos del dataset\n",
    "def get_ir_poblational_metrics(best_100_words, matrix, relevantes_recuperados, target, category, vocab):\n",
    "     \n",
    "    #print(\"calculando metrica poblacional GLOBAL RECALL\")\n",
    "    global_recall = globalRecall(relevantes_recuperados, target, category, vocab)\n",
    "    #print(\"calculando metrica poblacional JACCARD\")\n",
    "    jaccard_similarity = JaccardSimilarity(best_100_words, matrix, target, category, vocab)\n",
    "    \n",
    "    return global_recall, jaccard_similarity\n",
    "    \n",
    "# calcula average final\n",
    "def JaccardSimilarity(queries, matrix, target, category_idx, vocab):\n",
    "    \n",
    "    jaccard_sim = 0.0\n",
    "    \n",
    "    num=0\n",
    "    \n",
    "    # calcular indiceSimJaccard entre cada par de queries\n",
    "    for indx_i, q_i in enumerate(queries):\n",
    "        for indx_j, q_j in enumerate(queries):\n",
    "            num+=1\n",
    "            #print(\"par de queries: \", num)\n",
    "            #print(\"query i: \", q_i, \"\\nquery j: \", q_j )\n",
    "            \n",
    "            jaccard_sim += JaccardSimilarityIndex(q_i, q_j, matrix, target, category_idx, vocab)                \n",
    "            \n",
    "            #print(\"indice sim Jaccard ACUMULADO: \", jaccard_sim)         \n",
    "    #return jaccard_sim/(len(queries)*(len(queries)-1))            # no estoy omitiendo las queries cuando i=j\n",
    "    return jaccard_sim/(len(queries)*len(queries))\n",
    "\n",
    "def JaccardSimilarityIndex(q_i, q_j, matrix, target,category_idx, vocab):\n",
    "    # genero vector de documentos recuperados por cada query\n",
    "    word_idx_q_i = vocab.index(q_i)    \n",
    "    vec_q_i = matrix[:,word_idx_q_i]\n",
    "    \n",
    "    word_idx_q_j = vocab.index(q_j)    \n",
    "    vec_q_j = matrix[:,word_idx_q_j]\n",
    "    \n",
    "    RR_q_i = (vec_q_i>0) & (target[:,0]==category_idx)\n",
    "    relevantes_recuperados_q_i = np.array(RR_q_i)      #conversion a arreglo Numpy\n",
    "    \n",
    "    \n",
    "    # esto genera muchisimo output\n",
    "#     for index, i in enumerate(relevantes_recuperados_q_i):\n",
    "#         if i:\n",
    "#             if usingTrainMatrix:                \n",
    "#                 print(\"recuperado docu relevante de q_i \", train[index].path)\n",
    "#                 #print(\"relev recup de q_i \", relevantes_recuperados_q_i )            \n",
    "#             else:\n",
    "#                 print(\"recuperado docu relevante de q_i \", test[index].path)\n",
    "                \n",
    "        \n",
    "    RR_q_j = (vec_q_j>0) & (target[:,0]==category_idx)\n",
    "    relevantes_recuperados_q_j = np.array(RR_q_j)\n",
    "    \n",
    "#     # esto genera muchisimo output\n",
    "#     for index, i in enumerate(relevantes_recuperados_q_j):\n",
    "#         if i:\n",
    "#             if usingTrainMatrix:\n",
    "#                 print(\"recuperado docu relevante de q_j \", train[index].path)\n",
    "#                 #print(\"relev recup de q_j \", relevantes_recuperados_q_j)\n",
    "#             else:\n",
    "#                 print(\"recuperado docu relevante de q_j \", test[index].path)\n",
    "                               \n",
    "    \n",
    "    # FALTARIA quedarme con el TOPIC ID tomandolo desde train[index].path y ver la interseccion de los topic_ids\n",
    "    # pero esto se puede hacer como el AND entre los vectores booleanos!\n",
    "    \n",
    "    #print(\"arreglo relevantes recuperados q_i y q_j: \")\n",
    "    #print (relevantes_recuperados_q_i & relevantes_recuperados_q_j)\n",
    "    interseccion_relevantes_recuperados = np.sum(relevantes_recuperados_q_i & relevantes_recuperados_q_j)\n",
    "    union_relevantes_recuperados = np.sum(relevantes_recuperados_q_i | relevantes_recuperados_q_j)\n",
    "    #print (\"interseccion = \", interseccion_relevantes_recuperados)\n",
    "    #print (\"union = \", union_relevantes_recuperados)\n",
    "    \n",
    "    \n",
    "    if (interseccion_relevantes_recuperados == 0 | union_relevantes_recuperados==0):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(interseccion_relevantes_recuperados/union_relevantes_recuperados)\n",
    "\n",
    "    \n",
    "def fdd(a,b,c,d,beta=0.477):\n",
    "    toReturn = np.zeros(shape=(1,a.shape[1]))\n",
    "    p = discr(a,b,c,d)\n",
    "    r = descr(a,b,c,d)\n",
    "    num = (1+beta**2)*p*r\n",
    "    den = (beta**2)*p+r\n",
    "    mask = (p!=0) & (r!=0)\n",
    "    toReturn[mask] = num[mask]/den[mask]\n",
    "    return toReturn#/np.max(toReturn)\n",
    "def discr(a,b,c,d):\n",
    "    solucion = np.zeros(shape=a.shape)\n",
    "    mask = (a+c)!=0\n",
    "    solucion[mask] = a[mask]/(a[mask]+c[mask])\n",
    "    return solucion\n",
    "    #return a/(a+c)\n",
    "def descr(a,b,c,d):\n",
    "    solucion = np.zeros(shape=a.shape)\n",
    "    mask = (a+b)!=0\n",
    "    solucion[(a+b)!=0] = a[mask]/(a[mask]+b[mask])\n",
    "    return solucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1628721937405,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "PH3u7FrxOSxA"
   },
   "outputs": [],
   "source": [
    "# TEST para el topico 119\n",
    "\n",
    "# category = 119\n",
    "# #print(type(category))\n",
    "\n",
    "# t_temp_training = Texto.target(train, category_idx=category)   # el topic_id y category DEBEN SER ENTEROS!!!!!!!!! CHECK\n",
    "# print(t_temp_training)\n",
    "# a,b,c,d = get_abcd(m_train, t_temp_training)\n",
    "# print(\"\\na:\", a, \"\\nb:\", b, \"\\nc:\", c, \"\\nd:\", d)\n",
    "# print(\"N=a+b+c+d=\",a+b+c+d, \" <-- total de documentos segun cada termino (el indice del arreglo es el termino)\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZDWRtPvvxKT"
   },
   "source": [
    "# FDD definition and auxiliary function - Pre-compute data and store in FDD-CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9731,
     "status": "ok",
     "timestamp": 1628721947125,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "NMBamBKNSyoF",
    "outputId": "37e76d61-f193-4b30-a96e-622e3cf15186"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "!mkdir 'FDD-CSVs'\n",
    "\n",
    "all_topics = dict_topic_numb_name.keys()\n",
    "print(all_topics)\n",
    "some_topics = [215, 1, 529, 528]                           #### NOT ALL TOPICS\n",
    "\n",
    "topics = all_topics\n",
    "\n",
    "for category in topics:\n",
    "    topic_name = dict_topic_numb_name[category]['topic_name']\n",
    "    topic_name = topic_name.replace(\"/\", \"-\")  \n",
    "    #topic_name = Texto.categorias[category].replace(\"/\", \"-\")   \n",
    "    print(\"Topic name \", topic_name, \"\\n\")\n",
    "\n",
    "    writer = open('FDD-CSVs/{}_{}.csv'.format(topic_name, category), \"w\")\n",
    "    #writer = open('FDD-CSVs/{}_{}.csv'.format(Texto.categorias[category],category), 'w')\n",
    "    writer.write('{},{},{},{},{},{},{},{}\\n'.format(\n",
    "        'beta',\n",
    "        'termino',\n",
    "        'precision_train',\n",
    "        'recall_train',\n",
    "        'f1_train',\n",
    "        'precision_test',\n",
    "        'recall_test',\n",
    "        'f1_test'\n",
    "    ))\n",
    "    \n",
    "    writer_poblational = open('FDD-CSVs/{}_{}_poblational.csv'.format(topic_name, category), \"w\")\n",
    "    writer_poblational.write('{},{},{},{},{},{}\\n'.format(\n",
    "        'topic_name',\n",
    "        'topic_number',\n",
    "        'global_recall_train',\n",
    "        'global_recall_test',\n",
    "        'jaccard_sim_train',\n",
    "        'jaccard_sim_test'\n",
    "    ))\n",
    "\n",
    "    beta = 0.477\n",
    "    best_100_words = get_best_word(m_train,t_training, category, vocab,beta=beta)\n",
    "    \n",
    "    print(\"calculo de las primeras 100 queries\")\n",
    "    \n",
    "    total_RR_train = []\n",
    "    total_RR_test = []\n",
    "    pr1 = 0.0\n",
    "    re1 = 0.0\n",
    "    fs1 = 0.0\n",
    "    pr2 = 0.0\n",
    "    re2 = 0.0\n",
    "    fs2 = 0.0\n",
    "    RR1 = []\n",
    "    RR2 = []\n",
    "    \n",
    "    for word in best_100_words:                \n",
    "        #RR: relevantes_recuperados por cada termino-vectores de Bool - tantas filas como docus en train o test\n",
    "        pr1,re1,fs1, RR1 = get_ir_metrics(word, m_train, m_tfIdf_train, t_training, category, vocab)\n",
    "        \n",
    "        if len(total_RR_train)==0:\n",
    "            total_RR_train = RR1\n",
    "        else:            \n",
    "            total_RR_train = (total_RR_train | RR1)            \n",
    "        #print(\"termino= \", word, \" total_RR_train \\t\" ,np.sum(total_RR_train) )\n",
    "        \n",
    "\n",
    "        # Para el TEST son las mismas queries\n",
    "        pr2,re2,fs2, RR2 = get_ir_metrics(word, m_test, m_tfIdf_test, t_testing, category, vocab)\n",
    "        \n",
    "        if len(total_RR_test) == 0:\n",
    "            total_RR_test = RR2\n",
    "        else:            \n",
    "            total_RR_test = (total_RR_test | RR2)\n",
    "        #print(\"termino= \", word,\" total_RR_test \\t\", np.sum(total_RR_test)) \n",
    "        \n",
    "        writer.write('{},{},{},{},{},{},{},{}\\n'.format(\n",
    "            beta,\n",
    "            ' '.join([w for w in word]),\n",
    "            pr1,\n",
    "            re1,\n",
    "            fs1,\n",
    "            pr2,\n",
    "            re2,\n",
    "            fs2\n",
    "        ))\n",
    "    \n",
    "    # global recall & jaccard similarity(ploblational)\n",
    "    print(\"llamando a POBLACIONAL CON TRAIN\")\n",
    "    gr1, js1 = get_ir_poblational_metrics(best_100_words, m_train, total_RR_train, t_training, category, vocab)    \n",
    "    print(\"llamando a POBLACIONAL CON TEST\")\n",
    "    gr2, js2 = get_ir_poblational_metrics(best_100_words, m_test, total_RR_test, t_testing, category, vocab)\n",
    "        \n",
    "    writer_poblational.write('{},{},{},{},{},{}\\n'.format(\n",
    "        topic_name,\n",
    "        category,        \n",
    "        gr1,\n",
    "        gr2,\n",
    "        js1,\n",
    "        js2\n",
    "\n",
    "    ))        \n",
    "\n",
    "    writer.close()\n",
    "    writer_poblational.close()\n",
    "    gr1 = 0.0\n",
    "    gr2 = 0.0\n",
    "    js1 = 0.0\n",
    "    js2 = 0.0\n",
    "    print(\"end topic \", category, \" --------------------------------------- \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salida = False\n",
    "if salida:\n",
    "    a = np.array([True, False, True])\n",
    "    print(np.sum(a))\n",
    "\n",
    "    b=  np.array([True, True, True])\n",
    "\n",
    "    print(float(np.sum(a)/np.sum(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 476,
     "status": "ok",
     "timestamp": 1628722279928,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "YQM5RnBHxvkW",
    "outputId": "5d546191-90fe-4fea-b178-7e208868c858"
   },
   "outputs": [],
   "source": [
    "tupla = ('number', 'theory')\n",
    "print( vocab.index(tupla)) # indice de la tupla \"number theory\"\n",
    "\n",
    "indice = vocab.index(tupla)\n",
    "vector = m_test[:,indice]\n",
    "\n",
    "# for i in range(1, len(vector)):\n",
    "#     if vector[i] !=0.0:\n",
    "#         print(i, vector[i])\n",
    "\n",
    "cant_recu = np.sum(vector>0)\n",
    "cant_relev = np.sum(t_testing==1)\n",
    "cant_relev_recu = np.sum((vector>0) & (t_testing[:,0]== 1))\n",
    "print(\"cant_recu > 0) de la tupla 'Number theory' en TEST\" , cant_recu)\n",
    "print(\"cant_relev de la tupla 'Number theory' en TEST\" , cant_relev)\n",
    "print(\"cant_relev_recu de la tupla 'Number theory' en TEST\" , cant_relev_recu)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(dict_topic_numb_name.keys() )\n",
    "# print(dict_topic_numb_name[586])\n",
    "# print(dict_topic_numb_name[561])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oCR9dsqegEY"
   },
   "source": [
    "Build tuplas dictionary from CSVs (more preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1628722419266,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "-HPDddFyfOUj",
    "outputId": "5ddb695e-469b-4a78-a438-8c039cd68228"
   },
   "outputs": [],
   "source": [
    "\n",
    "# tuplas = {}\n",
    "\n",
    "# for category in topics:\n",
    "#     # topic_name = Texto.categorias[category].replace(\"/\", \"-\")\n",
    "#     topic_name = dict_topic_numb_name[category]['topic_name']\n",
    "#     topic_name = topic_name.replace(\"/\", \"-\")  \n",
    "\n",
    "#     tuplas[topic_name] = []    \n",
    "#     lineas = open('FDD-CSVs/{}_{}.csv'.format(topic_name,category), 'r').read().splitlines()\n",
    "#     lineas = lineas[1:]\n",
    "#     for li in lineas:\n",
    "#         print(li)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1628722453905,
     "user": {
      "displayName": "Cecilia Baggio",
      "photoUrl": "",
      "userId": "12351718925280275845"
     },
     "user_tz": 180
    },
    "id": "dFXIil0AeiYS",
    "outputId": "5666a23a-2e94-4106-cd06-555639d47cfa"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ejemplo de tuplas\n",
    "# {\n",
    "#   'Top-Business-Food_and_Related_Products-Sweeteners': [{'word': 'Home', \n",
    "#                                                         'Beta_range': [0.0, 9.9], \n",
    "#                                                         'prec_train': 1.0, \n",
    "#                                                         'recall_train': 1.0, \n",
    "#                                                         'f1_train': 1.0, \n",
    "#                                                         'prec_test': 0.0, \n",
    "#                                                         'recall_test': 0.0, \n",
    "#                                                         'f1_test': 0.0}], \n",
    "#  'Top-Society-Subcultures-Gothic': [{'word': 'Computational', \n",
    "#                                      'Beta_range': [0.0, 9.9], \n",
    "#                                      'prec_train': 0.0, \n",
    "#                                      'recall_train': 0.0, \n",
    "#                                      'f1_train': 0.0, \n",
    "#                                      'prec_test': 1.0, \n",
    "#                                      'recall_test': 1.0, \n",
    "#                                      'f1_test': 1.0}]\n",
    "#  }\n",
    "\n",
    "\n",
    "tuplas = {}\n",
    "for category in topics:\n",
    "    #topic_name = Texto.categorias[category].replace(\"/\", \"-\")\n",
    "    topic_name = dict_topic_numb_name[category]['topic_name']\n",
    "    topic_name = topic_name.replace(\"/\", \"-\")  \n",
    "    tuplas[topic_name] = []\n",
    "    \n",
    "    lineas = open('FDD-CSVs/{}_{}.csv'.format(topic_name,category), 'r').read().splitlines()\n",
    "    lineas = lineas[1:]\n",
    "    xs = [ float(linea.split(',')[0]) for linea in lineas]\n",
    "\n",
    "    \n",
    "    precs_train =[ float(linea.split(',')[2]) for linea in lineas]\n",
    "    recalls_train = [ float(linea.split(',')[3]) for linea in lineas]\n",
    "    f1s_train = [ float(linea.split(',')[4]) for linea in lineas]\n",
    "\n",
    "    words = [ (linea.split(',')[1]) for linea in lineas]\n",
    "    precs_test =[ float(linea.split(',')[5]) for linea in lineas]\n",
    "    recalls_test = [ float(linea.split(',')[6]) for linea in lineas]\n",
    "    f1s_test = [ float(linea.split(',')[7]) for linea in lineas]\n",
    "    \n",
    "    inicio = xs[0]\n",
    "    fin = None\n",
    "    prec_train = precs_train[0]\n",
    "    recall_train = recalls_train[0]\n",
    "    f1_train = f1s_train[0]\n",
    "    prec_test = precs_test[0]\n",
    "    recall_test = recalls_test[0]\n",
    "    f1_test = f1s_test[0]\n",
    "    ultima = words[0]\n",
    "\n",
    "    for idx,x in enumerate(xs):\n",
    "        word = words[idx]\n",
    "        if (word!=ultima):\n",
    "            fin = xs[idx-1]\n",
    "            t = {'word':ultima,'Beta_range':[inicio,fin],'prec_train':prec_train,\n",
    "                 'recall_train':recall_train,\n",
    "                 'f1_train':f1_train,\n",
    "                 'prec_test':prec_test,\n",
    "                 'recall_test':recall_test,\n",
    "                 'f1_test':f1_test}\n",
    "            topic_name = dict_topic_numb_name[category]['topic_name']\n",
    "            topic_name = topic_name.replace(\"/\", \"-\")  \n",
    "            #topic_name = Texto.categorias[category].replace(\"/\", \"-\")     \n",
    "            tuplas[topic_name].append(t)\n",
    "            inicio = xs[idx]\n",
    "            ultima = words[idx]    \n",
    "            prec_train = precs_train[idx]\n",
    "            recall_train = recalls_train[idx]\n",
    "            f1_train = f1s_train[idx]\n",
    "            prec_test = precs_test[idx]\n",
    "            recall_test = recalls_test[idx]\n",
    "            f1_test = f1s_test[idx]\n",
    "\n",
    "    fin = xs[idx]\n",
    "    t = {'word':ultima,'Beta_range':[inicio,fin],'prec_train':prec_train,\n",
    "         'recall_train':recall_train,\n",
    "         'f1_train':f1_train,\n",
    "         'prec_test':prec_test,\n",
    "         'recall_test':recall_test,\n",
    "         'f1_test':f1_test}\n",
    "    tuplas[topic_name].append(t)\n",
    "    \n",
    "# if debug:\n",
    "#     for key, value in tuplas.items():\n",
    "#         print(key, ' : ', value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL9PA2fvmwnZ"
   },
   "source": [
    "## Plots TRAINING\n",
    "# backup - no almacena csv de las queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wut4TkIvm2O6"
   },
   "source": [
    "**Histogram plot per method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT TRAINING & SAVE QUERIES FOR TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "\n",
    "!mkdir 'queries-CSVs'\n",
    "    \n",
    "# names ceci\n",
    "names = ['MOGP','TGF', 'IDF', 'TGF*',   'TGF*-IDFEC', '$\\chi^2$', 'OR','IG', 'GR',  r'FDD$_{0.5}$',r'FDD$_{10}$', r'FDD$_{1.0}$']\n",
    "#names = ['TGF', 'IDF', 'TGF*', 'MOGP']\n",
    "\n",
    "# comentado por Ceci\n",
    "# names = ['TGF', 'IDF', 'TGF*',   'TGF*-IDFEC',                 '$\\chi^2$', 'OR','TGF*-IGM',        'IG',                'GR', 'TGF*-IGM$_{imp}$',\n",
    "#          'GSS', 'Prob', 'SQRT-TGF*-IGM$_{imp}$',      'RF', 'IDFEC', 'TGF-IDFEC',  'MI',     'IDFEC_B',r'FDD$_{0.5}$',r'FDD$_{10}$']\n",
    "# comentado por maiso\n",
    "# names = ['TGF', 'IDF', 'TGF*',   'TGF*-IDFEC',                 '$\\chi^2$', 'OR','TGF*-IGM',        'IG',                'GR', 'TGF*-IGM$_{imp}$',\n",
    "#           'GSS', 'Prob',  'RF', 'IDFEC', 'TGF-IDFEC',  'MI',     'IDFEC_B',r'FDD$_{0.5}$',r'FDD$_{10}$']\n",
    "\n",
    "def tf(a,b,c,d):    \n",
    "    return a + c\n",
    "def tf_idf_ast(a,b,c,d):\n",
    "\treturn tf(a,b,c,d)*idfec(a,b,c,d)\n",
    "def tf_ast(a,b,c,d):\n",
    "\treturn a\n",
    "\n",
    "def tf_ast_idf_ast(a,b,c,d):\n",
    "\treturn tf_ast(a,b,c,d) * idfec(a,b,c,d)#np.log10((c+d)/(c+eps))\n",
    "def discr(a,b,c,d):\n",
    "\tsolucion = np.zeros(shape=a.shape)\n",
    "\tmask = (a+c)!=0\n",
    "\tsolucion[mask] = a[mask]/(a[mask]+c[mask])\n",
    "\treturn solucion\n",
    "\t#return a/(a+c)\n",
    "def descr(a,b,c,d):\n",
    "\tsolucion = np.zeros(shape=a.shape)\n",
    "\tmask = (a+b)!=0\n",
    "\tsolucion[(a+b)!=0] = a[mask]/(a[mask]+b[mask])\n",
    "\treturn solucion\n",
    "\n",
    "def fscore(a,b,c,d, alpha=0.82):\n",
    "\tdi = discr(a,b,c,d)\n",
    "\tde = descr(a,b,c,d)\n",
    "\tbeta = 1 - alpha\n",
    "\treturn 2*((alpha*de*beta*di)/(eps+alpha*de+beta*di))\n",
    "\n",
    "\n",
    "def delete_zeros(c):\n",
    "\tj = c.copy()\n",
    "\tj[j==0]=1\n",
    "\treturn j\n",
    "\n",
    "def prob_based(a,b,c,d):\n",
    "\treturn np.log10(1+(a/b)*(a/(c+eps))) # fix delete_zeros\n",
    "\n",
    "def gss(a,b,c,d):\n",
    "    #checkeado Domeneconi\n",
    "\tN = a + b + c + d\n",
    "\treturn (a*d - b*c)/(N**2)\n",
    "\n",
    "def x_sqr(a,b,c,d):\n",
    "\tN = a + b + c + d\n",
    "\tnum = (a*d - b*c) ** 2\n",
    "\tden = (a + c)* (b + d) * (a + b) * (c + d)\n",
    "\treturn N / ((num/(den+eps))+eps)\n",
    "\n",
    "def odds_ratio(a,b,c,d):\n",
    "\treturn np.log10((delete_zeros(a)*d)/(b*c+eps))\n",
    "\n",
    "def information_gain(a,b,c,d):\n",
    "\tN = a + b + c + d\n",
    "\ttermino1 = -((a+b)/N)  * np.log10((a+b)/N)\n",
    "\ttermino2 = (a / N) * np.log10((a+eps)/(a+c+eps))\n",
    "\ttermino3 = (b/N) * np.log10((b+eps)/(b+d+eps))\n",
    "\treturn (termino1 + termino2 + termino3)\n",
    "\n",
    "def gain_ratio(a,b,c,d):\n",
    "\tN = a + b + c + d\n",
    "\tig = information_gain(a,b,c,d)\n",
    "\tterm1 = -((a+b)/N)*np.log10((a+b+eps)/N)\n",
    "\tterm2 = -((c+d)/N)*np.log10((c+d+eps)/N)\n",
    "\treturn ig/(term1+term2+eps)\n",
    "\n",
    "def mutual_information(a,b,c,d):\n",
    "\tN = a + b + c + d\n",
    "\treturn np.log10(  ((eps+a)*N)  /   (eps +(a+b) * (a+c))   )\n",
    "def idf(a,b,c,d):\n",
    "\tN = a + b + c + d\n",
    "\treturn np.log10(N/(a+c+eps))\n",
    "def idf_prob(a,b,c,d):\n",
    "\treturn np.log10((b+d+eps)/(a+c+eps))\n",
    "def rf(a,b,c,d):\n",
    "\treturn np.log10(2+(a/(c+eps) ) )\n",
    "\n",
    "def idfec(a,b,c,d):\n",
    "\treturn np.log10(  (eps+c+d)/  ((eps+c))  )\n",
    "\n",
    "def idfec_b(a,b,c,d):\n",
    "\treturn np.log10(2 + (  (a+c+d)   /   ((c+eps))  ))\n",
    " \n",
    "def fdd_05(a,b,c,d):\n",
    "    return  fdd(a,b,c,d,beta=0.5)\n",
    "def fdd_10(a,b,c,d):\n",
    "    return fdd(a,b,c,d,beta=10)\n",
    "\n",
    "#ceci\n",
    "def fdd_1(a,b,c,d):\n",
    "    return fdd(a,b,c,d,beta=1.0)\n",
    "\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# # # # # # # # # #       IGM       # # # # # # # # # # # # # # # # \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# ESWA 2016\n",
    "# Generally, the\n",
    "# coefficient is set empirically to be 5.0 ∼ 9.0. The default value of λ\n",
    "# is set to be 7.0\n",
    "def igm(matriz, target, lambda_=7):\n",
    "  cant_docs = matriz.shape[0]\n",
    "  cant_terms = matriz.shape[1]\n",
    "  assert target.shape == (cant_docs,1)\n",
    "\n",
    "  igm_ = np.zeros(shape=(1,cant_terms))\n",
    "  \n",
    "  for term_idx in range(cant_terms):\n",
    "    cant_categorias = len(set(target[:,0]))\n",
    "    \n",
    "#     print(\"cant_categorias\", cant_categorias)\n",
    "#     print(\"np.sum(target > cant_categorias) \", np.sum(target>cant_categorias))\n",
    "#     print(\"target\", target)\n",
    "\n",
    "#    assert np.sum(target>cant_categorias)==0, 'Los indices de la categorias van\\ desde cero hasta cant_categorias-1'\n",
    "    f_kr = np.zeros(shape=(cant_categorias))\n",
    "    for categoria in range(cant_categorias):\n",
    "      freq = matriz[target[:,0]==categoria,term_idx]\n",
    "      f_kr[categoria] = np.sum(freq)\n",
    "\n",
    "\n",
    "    f_kr = np.sort(f_kr)[::-1]\n",
    "\n",
    "    igm_[0,term_idx] = f_kr[0]/(np.sum(f_kr*np.arange(1,1+len(f_kr))))\n",
    "      \n",
    "  return 1 + lambda_ *  igm_\n",
    "\n",
    "\n",
    "def tf_ast_igm(matrix,target, category):\n",
    "  a, b, c, d = get_abcd(matrix,target==category)\n",
    "\n",
    "#   print( \"\\n\\nen tf_ast_igm llamare a tf_ast(a,b,c,d)\")\n",
    "\n",
    "#   tf_ast_ = tf_ast(a,b,c,d)\n",
    "#   print(tf_ast_)\n",
    "#   print(\"\\n\\nllamare a igm(matrix, target) con target =\")\n",
    "#   print(target, \"\\n\\n\")\n",
    "    \n",
    "  igm_ = igm(matrix, target)\n",
    "  return tf_ast_*igm_\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# # # # # # # # # #    END IGM        # # # # # # # # # # # # # # # \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# # # # # # # # #       IGM_imp       # # # # # # # # # # # # # # # \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "\n",
    "def igm_imp(matriz, target, lambda_=7):\n",
    "  cant_docs = matriz.shape[0]\n",
    "  cant_terms = matriz.shape[1]\n",
    "  assert target.shape == (cant_docs,1)\n",
    "\n",
    "  igm_imp_ = np.zeros(shape=(1,cant_terms))\n",
    "  \n",
    "  for term_idx in range(cant_terms):\n",
    "    cant_categorias = len(set(target[:,0]))\n",
    "#    assert np.sum(target>cant_categorias)==0, 'Los indices de la categorias van\\ desde cero hasta cant_categorias-1'\n",
    "    f_kr = np.zeros(shape=(cant_categorias))\n",
    "    for categoria in range(cant_categorias):\n",
    "      freq = matriz[target[:,0]==categoria,term_idx]\n",
    "      f_kr[categoria] = np.sum(freq)\n",
    "\n",
    "\n",
    "    doc_per_cat = np.array([np.sum(target[:,0]==cat) for cat in range(cant_categorias)])\n",
    "    Dtotal = doc_per_cat[np.argsort(f_kr)[::-1][0]]\n",
    "\n",
    "\n",
    "\n",
    "    f_kr = np.sort(f_kr)[::-1]\n",
    "\n",
    "    f_k1 = f_kr[0]\n",
    "\n",
    "\n",
    "\n",
    "    igm_imp_[0,term_idx] = f_kr[0]/(np.sum(f_kr*np.arange(1,1+len(f_kr))) + np.log10(Dtotal/f_k1))\n",
    "      \n",
    "  return 1 + lambda_ * igm_imp_\n",
    "\n",
    "def tf_ast_igm_imp(matrix,target, category):\n",
    "  a, b, c, d = get_abcd(matrix,target==category)\n",
    "  tf_ast_ = tf_ast(a,b,c,d)\n",
    "  igm_imp_ = igm_imp(matrix, target)\n",
    "  return tf_ast_*igm_imp_\n",
    "\n",
    "def sqrt_tf_ast_igm_imp(matrix,target, category):\n",
    "  a, b, c, d = get_abcd(matrix,target==category)\n",
    "  tf_ast_ = tf_ast(a,b,c,d)\n",
    "  igm_imp_ = igm_imp(matrix, target)\n",
    "  return np.sqrt(tf_ast_)*igm_imp_\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
    "# # # # # # # #       end IGM_imp       # # # # # # # # # # # # # # \n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "\n",
    "def MOGP(a,b,c,d):\n",
    "    \n",
    "    # a denotes the number of documents that belong to class c_k and contain term t_i\n",
    "    solucion = np.zeros(shape=a.shape)    \n",
    "    return solucion\n",
    "    \n",
    "\n",
    "def get_GR_JIS_MOGP(topic_id):\n",
    "    \n",
    "    ## abrir CSV y devolver para GR la fila de promedios para todas las 5 corridas del topicId - es la ultima columna\n",
    "    ## idem JIS\n",
    "\n",
    "    # CSV:\n",
    "    # Topic_number  Run_1  Run_2  Run_3  Run_4  Run_5\n",
    "    #          1     0.96     1.00     1.00     0.94 \n",
    "    #         134    1.00     0.86     0.96     0.94 \n",
    "    #         ... \n",
    "\n",
    "    root = 'MOGP/'\n",
    "    file = 'Co3_globalRecall_allRun_lastGen_per_topic_TRAINING.csv'\n",
    "    \n",
    "    # GR LAST GEN - MEAN ALL RUN\n",
    "    df_GR = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "    gr_topicId = df_GR.loc[df_GR['Topic_number'] == topic_id]    \n",
    "    GR = gr_topicId[\"mean\"]\n",
    "    \n",
    "    \n",
    "    file = 'Co3_JaccardIndex_allRun_lastGen_per_topic_TRAINING.csv'\n",
    "    \n",
    "    # GR LAST GEN - MEAN ALL RUN\n",
    "    df_JIS = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "    jis_topicId = df_JIS.loc[df_JIS['Topic_number'] == topic_id]\n",
    "    JIS = jis_topicId[\"mean\"]\n",
    "    \n",
    "\n",
    "    return float(GR), float(JIS)\n",
    "\n",
    "def get_GR_JIS_MOGP_testing(topic_id):\n",
    "    \n",
    "    ## abrir CSV y devolver para GR la fila de promedios para todas las 5 corridas del topicId - es la ultima columna\n",
    "    ## idem JIS\n",
    "\n",
    "    # CSV:\n",
    "    # Topic_number  Run_1  Run_2  Run_3  Run_4  Run_5\n",
    "    #          1     0.96     1.00     1.00     0.94 \n",
    "    #         134    1.00     0.86     0.96     0.94 \n",
    "    #         ... \n",
    "\n",
    "    root = 'MOGP/'\n",
    "    file = 'Co3_MEAN_GLOBAL_RECALL_FROM_TESTING_ALL_TOPICS_LAST_GEN_ordered.csv'\n",
    "    \n",
    "    # GR LAST GEN - MEAN ALL RUN\n",
    "    df_GR = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "    gr_topicId = df_GR.loc[df_GR['Topic_number'] == topic_id]    \n",
    "    GR = gr_topicId[\"mean\"]\n",
    "    \n",
    "    file = 'Co3_MEAN_JACCARD_SIM_FROM_TESTING_ALL_TOPICS_LAST_GEN_ordered.csv'\n",
    "    \n",
    "    # GR LAST GEN - MEAN ALL RUN\n",
    "    df_JIS = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "    jis_topicId = df_JIS.loc[df_JIS['Topic_number'] == topic_id]\n",
    "    JIS = jis_topicId[\"mean\"]\n",
    "    \n",
    "\n",
    "    return float(GR), float(JIS)\n",
    "\n",
    "        \n",
    "def getPrecisionRecallMOGP(topic_id):\n",
    "\n",
    "    ## abrir CSV y devolver para precision la fila de promedios para todas las 5 corridas del topicId\n",
    "    ## idem Recall\n",
    "\n",
    "    # CSV:\n",
    "    # Topic_number  query_0  query_1  query_2  query_3  query_4  query_5  ... query_99\n",
    "    #          1     0.96     1.00     1.00     0.94     0.94     0.90          0.915\n",
    "    #         134    1.00     0.86     0.96     0.94     1.00     0.92          0.86\n",
    "    #         ... \n",
    "\n",
    "    root = 'MOGP/'\n",
    "    file = 'Co3_precision_allRun_lastGen_per_topic_TRAINING.csv'\n",
    "\n",
    "    # PRECISION LAST GEN - MEAN ALL RUN\n",
    "    df_P = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "\n",
    "    precision_topicId = df_P.loc[df_P['Topic_number'] == topic_id]\n",
    "    precision_topicId = precision_topicId.drop('Topic_number', 1) # elimina el numero de topico - devuelve arreglo limpio de promedios\n",
    "\n",
    "\n",
    "    file = 'Co3_recall_allRun_lastGen_per_topic_TRAINING.csv'\n",
    "    \n",
    "    # RECALL LAST GEN - MEAN ALL RUN\n",
    "    df_R = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "\n",
    "    recall_topicId = df_R.loc[df_R['Topic_number'] == topic_id]\n",
    "    recall_topicId = recall_topicId.drop('Topic_number', 1) # elimina el numero de topico - devuelve arreglo limpio de promedios\n",
    "\n",
    "    return precision_topicId.to_numpy(), recall_topicId.to_numpy()\n",
    "\n",
    "def getPrecisionRecallMOGP_testing(topic_id):\n",
    "\n",
    "    ## abrir CSV y devolver para precision la fila de promedios para todas las 5 corridas del topicId\n",
    "    ## idem Recall\n",
    "\n",
    "    # CSV:\n",
    "    # Topic_number  query_0  query_1  query_2  query_3  query_4  query_5  ... query_99\n",
    "    #          1     0.96     1.00     1.00     0.94     0.94     0.90          0.915\n",
    "    #         134    1.00     0.86     0.96     0.94     1.00     0.92          0.86\n",
    "    #         ... \n",
    "\n",
    "    root = 'MOGP/'\n",
    "    file = 'Co3_MEAN_PRECISION@10_FROM_TESTING_ALL_TOPICS_LAST_GEN_ordered.csv'\n",
    "\n",
    "    # PRECISION LAST GEN - MEAN ALL RUN\n",
    "    df_P = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores\n",
    "\n",
    "    precision_topicId = df_P.loc[df_P['Topic_number'] == topic_id]\n",
    "    precision_topicId = precision_topicId.drop('Topic_number', 1) # elimina el numero de topico - devuelve arreglo limpio de promedios\n",
    "\n",
    "    precision_topicId = precision_topicId[\"mean\"]\n",
    "    \n",
    "    \n",
    "    # EL RECALL esta almacenado como\n",
    "    # topic number  mean all run\n",
    "    #      1           0.703889\n",
    "    #      134         0.874884\n",
    "    ######\n",
    "    \n",
    "    \n",
    "    file = 'Co3_RECALL_FROM_TESTING_ALL_TOPICS_LAST_GEN_ordered.csv'\n",
    "    \n",
    "    # RECALL LAST GEN - MEAN ALL RUN\n",
    "    df_R = pd.read_csv(os.path.join(root, file), header=[0]).iloc[:, 1:] #el ultimo iloc elimina primer columna que son indices anteriores    \n",
    "    recall_topicId = df_R.loc[df_R['Topic_number'] == topic_id]\n",
    "    recall_topicId = recall_topicId.drop('Topic_number', 1) # elimina el numero de topico - devuelve arreglo limpio de promedios\n",
    "    \n",
    "    return precision_topicId.to_numpy(), recall_topicId.to_numpy()\n",
    "\n",
    "# metodos ceci\n",
    "methods = [MOGP, tf, idf, tf_ast, tf_ast_idf_ast, x_sqr, odds_ratio, information_gain, gain_ratio, fdd_05, fdd_10, fdd_1]\n",
    "\n",
    "\n",
    "# usados por Maiso\n",
    "#methods = [tf, idf, tf_ast,tf_ast_idf_ast , x_sqr, odds_ratio,tf_ast_igm,  information_gain, gain_ratio, tf_ast_igm_imp, gss, prob_based, sqrt_tf_ast_igm_imp,rf, idfec, tf_idf_ast, mutual_information , idfec_b, fdd_05,fdd_10]\n",
    "\n",
    "\n",
    "#testing = False\n",
    "\n",
    "topic_names = []\n",
    "#print(topics)\n",
    "for cat in topics:\n",
    "    topic_name = dict_topic_numb_name[cat]['topic_name']\n",
    "    topic_name = topic_name.replace(\"/\", \"-\")  \n",
    "    topic_names.append(topic_name)\n",
    "    print(topic_name)\n",
    "print(\"----------------------------------------------------------------- \\n\\n\")    \n",
    "\n",
    "\n",
    "categ2index = dict([(cat,idx) for idx,cat in enumerate(topic_names)])\n",
    "index2categ = dict([(idx,cat) for idx,cat in enumerate(topic_names)])\n",
    "\n",
    "# matrix to store metrics from training\n",
    "recalls = np.zeros(shape=(len(categ2index),len(methods)))   \n",
    "precisions = np.zeros((len(categ2index),len(methods))) \n",
    "f1_score = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "globalRecalls = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "jaccardIndexes = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "\n",
    "# matrix to store metrics from testing\n",
    "recalls_testing = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "precisions_testing = np.zeros((len(categ2index),len(methods)))\n",
    "f1_score_testing = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "globalRecalls_testing = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "jaccardIndexes_testing = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "\n",
    "\n",
    "# RECALL:\n",
    "#            FDD     TGF     TGF*\n",
    "# cat 1      x.xx    x.xx   x.xx\n",
    "# cat 2      x.xx    x.xx   x.xx\n",
    "# cat 3      x.xx    x.xx   x.xx\n",
    "# cat 4      x.xx    x.xx   x.xx\n",
    "\n",
    "# precision:\n",
    "#            FDD     TGF     TGF*\n",
    "# cat 1      x.xx    x.xx   x.xx\n",
    "# cat 2      x.xx    x.xx   x.xx\n",
    "# cat 3      x.xx    x.xx   x.xx\n",
    "# cat 4      x.xx    x.xx   x.xx\n",
    "\n",
    "\n",
    "i = np.array(vocab)\n",
    "\n",
    "\n",
    "for idx, category in enumerate(topics):            ##################################### CECI\n",
    "    t_testing = Texto.target(test, category_idx=category)\n",
    "    t_training = Texto.target(train, category_idx=category)   # vector de T o F si el doc es relev para el topico o no\n",
    "    \n",
    "    #     A = [i for i, x in enumerate(t_training) if x]\n",
    "    #     print(\"las posiciones de docus relevantes para el topico \", category)\n",
    "    #     print (A)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"TOPICO = \", category)\n",
    "    \n",
    "    # almacena un csv por topico, con las 100 queries de cada metodo\n",
    "    writer = open('queries-CSVs/{}_queries.csv'.format(category), \"w\")\n",
    "\n",
    "    a, b, c, d = get_abcd(m_train,t_training)   \n",
    "    #matrix_ = m_train\n",
    "    #target_ = t_training\n",
    "    #target_sin_categoria = Texto.target(train)\n",
    "\n",
    "    a_testing, b_testing, c_testing, d_testing = get_abcd(m_test,t_testing)\n",
    "    #matrix_testing = m_test                ############### VER DE BORRAR es m_test\n",
    "    #target_testing = t_testing\n",
    "    #target_sin_categoria_testing = Texto.target(test)\n",
    "    \n",
    "    recall = descr(a,b,c,d)\n",
    "    precision = discr(a,b,c,d)\n",
    "    \n",
    "    recall_testing = descr(a_testing, b_testing, c_testing, d_testing)\n",
    "    precision_testing = discr(a_testing, b_testing ,c_testing , d_testing)\n",
    "\n",
    "    #print(\"precision = discr = \", precision)\n",
    "    #print(\"recall = descr = \", recall)\n",
    "    \n",
    "    #a,b,c,d = get_abcd(m_train,t_training)\n",
    "    #FDD = fdd(a,b,c,d,1)                                ### POR QUE LLAMA A FDD CON BETA = 1 ???????????  \n",
    "    \n",
    "    f = open('queries-CSVs/{}_queries.csv'.format(category), \"w\")\n",
    "    \n",
    "    # create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header = [\"method\"] + [\"query_\"+str(i) for i in range(0,100)]\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for k in range(len(methods)):\n",
    "    \n",
    "        method = methods[k]\n",
    "        \n",
    "        print(\"topico= \", category, \" - Metodo= \", method.__name__)        \n",
    "        \n",
    "        sum_recall_value = 0.0\n",
    "        sum_recall_value_testing = 0.0\n",
    "        \n",
    "        sum_prec_value = 0.0\n",
    "        sum_prec_value_testing = 0.0                     \n",
    "        \n",
    "        gr_per_method = 0.0\n",
    "        gr_per_method_testing = 0.0\n",
    "        \n",
    "        jis_per_method = 0.0\n",
    "        jis_per_method_testing = 0.0\n",
    "        \n",
    "        queries_method_k = []        \n",
    "        \n",
    "        if method.__name__ == \"MOGP\":\n",
    "            # los valores de las metricas de MOGP ya estan calculados en CSVs\n",
    "            precisions_topicId, recalls_topicId = getPrecisionRecallMOGP(int(category))\n",
    "            precisions_topicId_testing, recalls_topicId_testing = getPrecisionRecallMOGP_testing(int(category))\n",
    "            \n",
    "            sum_prec_value = np.sum(precisions_topicId)\n",
    "            sum_prec_value_testing = np.sum(precisions_topicId_testing)\n",
    "            \n",
    "            sum_recall_value = np.sum(recalls_topicId)\n",
    "            sum_recall_value_testing = np.sum(recalls_topicId_testing)\n",
    "            \n",
    "            \n",
    "            #print(\"sum_prec_value \", sum_prec_value)\n",
    "            #print(\"sum_recall_value \", sum_recall_value)\n",
    "            \n",
    "            #print(\"sum_prec_value IN testing\", sum_prec_value_testing, precisions_topicId_testing)\n",
    "            #print(\"falta recall---------------------------------------------\")\n",
    "            \n",
    "            ######################################################\n",
    "            ##### FIXXX RECALL         !!!!!!!!!!!!!!!!!\n",
    "            ######################################################\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            best_100_words_with_k, scores_with_k = get_best_word_method(m_train, Texto.target(train), category, vocab, method)\n",
    "            print(scores_with_k) \n",
    "            \n",
    "            total_RR = []\n",
    "            RR1 = []\n",
    "            \n",
    "            total_RR_testing = []\n",
    "            RR2 = []\n",
    "            \n",
    "            for indx in range(1, len(best_100_words_with_k)+1):            \n",
    "                \n",
    "                # el arreglo de SCORES llega ordenado de menor a mayor\n",
    "                # con esto que sigue me quedo con el score mas chico, porque lo ordeno de MAYOR a menor (el -1) y desp me quedo con los del final - FAIL\n",
    "                # index_topword = np.argsort(scores_with_k)[-1][-1*indx]                        \n",
    "                \n",
    "                \n",
    "                index_topword = scores_with_k[-1*indx]                        \n",
    "                   \n",
    "              \n",
    "                #recall_value = recall[0,index_topword]\n",
    "                #precision_value = precision[0,index_topword]                    \n",
    "                \n",
    "                \n",
    "                #top_word = (i[np.argsort(scores_with_k)])[-1][-1*indx]\n",
    "                top_word = (i[index_topword])\n",
    "                \n",
    "                queries_method_k.append(top_word)\n",
    "\n",
    "                # estan comentados F1 ya se calculan aparte. comprobado que da igual\n",
    "                #RR: relevantes_recuperados por cada termino-vectores de Bool - tantas filas como docus en train o test\n",
    "                precision_value, recall_value , _ , RR1 = get_ir_metrics(top_word, m_train, m_tfIdf_train, Texto.target(train), category, vocab)\n",
    "                \n",
    "                sum_recall_value+= recall_value\n",
    "                sum_prec_value+= precision_value\n",
    "                \n",
    "                \n",
    "                # total de Relevantes Recuperados para GR en TRAIN\n",
    "                if len(total_RR)==0:\n",
    "                    total_RR = RR1\n",
    "                else:            \n",
    "                    total_RR = (total_RR | RR1)            \n",
    "                #print(\"termino= \", top_word, \" total_RR \\t\" ,np.sum(total_RR) )\n",
    "                \n",
    "                \n",
    "                \n",
    "                # metricas para TESTING\n",
    "                precision_value_testing, recall_value_testing , f1_value_testing, RR2 = get_ir_metrics(top_word, m_test, m_tfIdf_test, Texto.target(test), category, vocab)                                                                    \n",
    "                \n",
    "                # total de Relevantes Recuperados para GR en TEST\n",
    "                if len(total_RR_testing)==0:\n",
    "                    total_RR_testing = RR2\n",
    "                else:            \n",
    "                    total_RR_testing = (total_RR_testing | RR2)                      \n",
    "                #print(\"termino= \", top_word, \" total_RR_TESTING \\t\" ,np.sum(total_RR_testing) )                    \n",
    "\n",
    "                sum_recall_value_testing+= recall_value_testing\n",
    "                sum_prec_value_testing+= precision_value_testing\n",
    "                \n",
    "                top_word = None\n",
    "                \n",
    "            \n",
    "            #almaceno las queries en CSVs segun el metodo                \n",
    "            line = [method.__name__] + queries_method_k\n",
    "\n",
    "            writer.writerow(line)\n",
    "            \n",
    "\n",
    "            gr_per_method, jis_per_method = get_ir_poblational_metrics(queries_method_k, m_train, total_RR, Texto.target(train), category, vocab)\n",
    "            gr_per_method_testing, jis_per_method_testing = get_ir_poblational_metrics(queries_method_k, m_test, total_RR_testing, Texto.target(test), category, vocab)\n",
    "        \n",
    "        if method.__name__ == \"MOGP\":\n",
    "            #print (\"mean RECALL \", sum_recall_value/100)\n",
    "            #print (\"mean PRECISION \", sum_prec_value/100)\n",
    "            \n",
    "            recalls[idx,k] = sum_recall_value/100\n",
    "            precisions[idx,k] = sum_prec_value/100             # CON MOGP SON 100 queries\n",
    "            globalRecalls[idx,k], jaccardIndexes[idx,k] = get_GR_JIS_MOGP(int(category))                 # TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR  TERMINAR   \n",
    "            \n",
    "            recalls_testing[idx,k] = sum_recall_value_testing\n",
    "            precisions_testing[idx,k] = sum_prec_value_testing             # YA SON PROMEDIOS\n",
    "            globalRecalls_testing[idx,k], jaccardIndexes_testing[idx,k] = get_GR_JIS_MOGP_testing(int(category))\n",
    "\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            recalls[idx,k] = sum_recall_value/len(best_100_words_with_k)\n",
    "            recalls_testing[idx,k] = sum_recall_value_testing/len(best_100_words_with_k)\n",
    "            \n",
    "            precisions[idx,k] = sum_prec_value/len(best_100_words_with_k)\n",
    "            precisions_testing[idx,k] = sum_prec_value_testing/len(best_100_words_with_k)\n",
    "            \n",
    "            globalRecalls[idx,k] = gr_per_method\n",
    "            globalRecalls_testing[idx,k] = gr_per_method_testing\n",
    "            \n",
    "            jaccardIndexes[idx,k] = jis_per_method\n",
    "            jaccardIndexes_testing[idx,k] = jis_per_method_testing\n",
    "                                                   \n",
    "            \n",
    "            \n",
    "\n",
    "    # se queda con una top word en RECALL y otra en PREC para cada topico - WHY? creo que no se usa\n",
    "    top_word_r = (i[np.argsort(recall)])[-1][-1]\n",
    "    top_word_p = (i[np.argsort(precision)])[-1][-1]\n",
    "    #index_topword = np.argsort(FDD)[-1][-1]\n",
    "    \n",
    "    #recall_value = recall[0,index_topword]\n",
    "    #precision_value = precision[0,index_topword]\n",
    "    \n",
    "    # agrega a la tabla el PROMEDIO de las precisiones (o recalls) de todas las consultsa del topico    \n",
    "    #recalls[idx,-1] = recall_value    \n",
    "    #precisions[idx,-1] = precision_value\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "from scipy.stats import sem, t\n",
    "from scipy import mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# testing = False\n",
    "\n",
    "confidence = 0.95\n",
    "\n",
    "# promedia metricas por TODOS LOS TOPICOS\n",
    "r = np.average(recalls,axis=0)\n",
    "p = np.average(precisions,axis=0)\n",
    "f1scores = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "mask = (precisions+recalls)!=0\n",
    "f1scores[mask] = 2.0 * ((precisions[mask] * recalls[mask]) / (precisions[mask]+recalls[mask]))\n",
    "f1 = np.average(f1scores,axis=0)\n",
    "\n",
    "gr = np.average(globalRecalls,axis=0)\n",
    "jis = np.average(jaccardIndexes,axis=0)\n",
    "\n",
    "deltas_f1score = np.zeros(shape=(len(methods)))\n",
    "deltas_recall = np.zeros(shape=(len(methods)))\n",
    "deltas_precision = np.zeros(shape=(len(methods)))\n",
    "deltas_gr = np.zeros(shape=(len(methods)))\n",
    "deltas_jis = np.zeros(shape=(len(methods)))\n",
    "\n",
    "for columna in range(len(recalls[0,:])):\n",
    "    datos = recalls[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_recall[columna] = h\n",
    "    \n",
    "    datos = precisions[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_precision[columna] = h\n",
    "    \n",
    "    datos = f1scores[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_f1score[columna] = h\n",
    "    \n",
    "    # NUEVO GR Y JIS\n",
    "    datos = globalRecalls[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_gr[columna] = h\n",
    "    \n",
    "    datos = jaccardIndexes[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_jis[columna] = h\n",
    "\n",
    "#for cate in range(len(methods)):\n",
    "#    print('method: {:15} recall: {:5,.4f} precision {:5,.4f} f1-score: {:5,.4f}'.format(names[cate],r[cate],p[cate],f1[cate]))\n",
    "#print('method: {:15} recall: {:5,.4f} precision {:5,.4f} f1-score: {:5,.4f}'.format('FDD',r[cate+1],p[cate+1],f1[cate+1]))\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [15, 15]\n",
    "fig,ax = plt.subplots(5)\n",
    "\n",
    "\n",
    "## PLOT PRECISION\n",
    "label = [f'{value:4.3f}' for value in p]\n",
    "for i in range(len(p)):\n",
    "  ax[0].text(x =i+0.1 , y = p[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[0].bar(np.arange(len(methods)), height=p,tick_label=names)\n",
    "ax[0].errorbar(np.arange(len(methods)),p,yerr=deltas_precision,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT RECALL\n",
    "label = [f'{value:4.3f}' for value in r]\n",
    "for i in range(len(r)):\n",
    "  ax[1].text(x =i+0.1 , y = r[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[1].bar(np.arange(len(methods)), height=r,tick_label=names)\n",
    "ax[1].errorbar(np.arange(len(methods)),r,yerr=deltas_recall,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT F1\n",
    "label = [f'{value:4.3f}' for value in f1]\n",
    "for i in range(len(f1)):  \n",
    "  ax[2].text(x =i+0.1 , y = f1[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[2].bar(np.arange(len(methods)), height=f1,tick_label=names)\n",
    "ax[2].errorbar(np.arange(len(methods)),f1,yerr=deltas_f1score,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT GLOBAL RECALL\n",
    "label = [f'{value:4.3f}' for value in gr]\n",
    "for i in range(len(gr)):  \n",
    "  ax[3].text(x =i+0.1 , y = gr[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[3].bar(np.arange(len(methods)), height = gr,tick_label=names)\n",
    "ax[3].errorbar(np.arange(len(methods)),gr,yerr=deltas_gr,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT JACCARD INDEX SIM\n",
    "label = [f'{value:4.3f}' for value in jis]\n",
    "for i in range(len(jis)):  \n",
    "  ax[4].text(x =i+0.1 , y = jis[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[4].bar(np.arange(len(methods)), height= jis ,tick_label=names)\n",
    "ax[4].errorbar(np.arange(len(methods)),jis,yerr=deltas_jis,fmt='.k',capsize=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[1].set_ylabel('$recall$ $(avg)$',fontsize=15)\n",
    "ax[0].set_ylabel('$precision$ $(avg)$',fontsize=15)\n",
    "ax[2].set_ylabel('$F_1-score$ $(avg)$',fontsize=15)\n",
    "ax[3].set_ylabel('$G. Recall$ $(avg)$',fontsize=15)\n",
    "ax[4].set_ylabel('$J. S. Index$ $(avg)$',fontsize=15)\n",
    "\n",
    "\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[1].set_ylim(0,1)\n",
    "ax[2].set_ylim(0,1)\n",
    "ax[3].set_ylim(0,1)\n",
    "ax[4].set_ylim(0,1)\n",
    "\n",
    "# fig.suptitle('Testing' if testing else 'Training')\n",
    "\n",
    "\n",
    "for plot in ax:\n",
    "\n",
    "    for tick in plot.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(10) \n",
    "    for tick in plot.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(10) \n",
    "\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "\n",
    "\n",
    "# horizontal line in MOGP value\n",
    "ax[0].axhline(y=p[-12], c='k',ls=':')\n",
    "ax[1].axhline(y=r[-12], c='k',ls=':')\n",
    "ax[2].axhline(y=f1[-12], c='k',ls=':')\n",
    "ax[3].axhline(y=gr[-12], c='k',ls=':')\n",
    "ax[4].axhline(y=jis[-12], c='k',ls=':')\n",
    "\n",
    "#fig.savefig(f'performanceComparisonDMOZ{\"Testing\" if testing else \"Training\"}.pdf')\n",
    "fig.savefig(f'performanceComparisonDMOZ_Training.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# renombro a nombres usados en plot de mas arriba para no introducir bugs, ya que el calculo es el mismo\n",
    "recalls = recalls_testing\n",
    "precisions = precisions_testing\n",
    "globalRecalls = globalRecalls_testing\n",
    "jaccardIndexes = jaccardIndexes_testing\n",
    "\n",
    "# promedia metricas por TODOS LOS TOPICOS\n",
    "r = np.average(recalls,axis=0)\n",
    "p = np.average(precisions,axis=0)\n",
    "f1scores = np.zeros(shape=(len(categ2index),len(methods)))\n",
    "mask = (precisions+recalls)!=0\n",
    "f1scores[mask] = 2.0 * ((precisions[mask] * recalls[mask]) / (precisions[mask]+recalls[mask]))\n",
    "f1 = np.average(f1scores,axis=0)\n",
    "\n",
    "gr = np.average(globalRecalls,axis=0)\n",
    "jis = np.average(jaccardIndexes,axis=0)\n",
    "\n",
    "deltas_f1score = np.zeros(shape=(len(methods)))\n",
    "deltas_recall = np.zeros(shape=(len(methods)))\n",
    "deltas_precision = np.zeros(shape=(len(methods)))\n",
    "deltas_gr = np.zeros(shape=(len(methods)))\n",
    "deltas_jis = np.zeros(shape=(len(methods)))\n",
    "\n",
    "for columna in range(len(recalls[0,:])):\n",
    "    datos = recalls[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_recall[columna] = h\n",
    "    \n",
    "    datos = precisions[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_precision[columna] = h\n",
    "    \n",
    "    datos = f1scores[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_f1score[columna] = h\n",
    "    \n",
    "    # NUEVO GR Y JIS\n",
    "    datos = globalRecalls[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_gr[columna] = h\n",
    "    \n",
    "    datos = jaccardIndexes[:,columna]\n",
    "    n = len(datos)\n",
    "    std_err = sem(datos)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    deltas_jis[columna] = h\n",
    "\n",
    "#for cate in range(len(methods)):\n",
    "#    print('method: {:15} recall: {:5,.4f} precision {:5,.4f} f1-score: {:5,.4f}'.format(names[cate],r[cate],p[cate],f1[cate]))\n",
    "#print('method: {:15} recall: {:5,.4f} precision {:5,.4f} f1-score: {:5,.4f}'.format('FDD',r[cate+1],p[cate+1],f1[cate+1]))\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [15, 15]\n",
    "fig,ax = plt.subplots(5)\n",
    "\n",
    "\n",
    "## PLOT PRECISION\n",
    "label = [f'{value:4.3f}' for value in p]\n",
    "for i in range(len(p)):\n",
    "  ax[0].text(x =i+0.1 , y = p[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[0].bar(np.arange(len(methods)), height=p,tick_label=names)\n",
    "ax[0].errorbar(np.arange(len(methods)),p,yerr=deltas_precision,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT RECALL\n",
    "label = [f'{value:4.3f}' for value in r]\n",
    "for i in range(len(r)):\n",
    "  ax[1].text(x =i+0.1 , y = r[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[1].bar(np.arange(len(methods)), height=r,tick_label=names)\n",
    "ax[1].errorbar(np.arange(len(methods)),r,yerr=deltas_recall,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT F1\n",
    "label = [f'{value:4.3f}' for value in f1]\n",
    "for i in range(len(f1)):  \n",
    "  ax[2].text(x =i+0.1 , y = f1[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[2].bar(np.arange(len(methods)), height=f1,tick_label=names)\n",
    "ax[2].errorbar(np.arange(len(methods)),f1,yerr=deltas_f1score,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT GLOBAL RECALL\n",
    "label = [f'{value:4.3f}' for value in gr]\n",
    "for i in range(len(gr)):  \n",
    "  ax[3].text(x =i+0.1 , y = gr[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[3].bar(np.arange(len(methods)), height = gr,tick_label=names)\n",
    "ax[3].errorbar(np.arange(len(methods)),gr,yerr=deltas_gr,fmt='.k',capsize=5)\n",
    "\n",
    "## PLOT JACCARD INDEX SIM\n",
    "label = [f'{value:4.3f}' for value in jis]\n",
    "for i in range(len(jis)):  \n",
    "  ax[4].text(x =i+0.1 , y = jis[i]+0.02, s = label[i], size = 11, rotation=90)\n",
    "\n",
    "ax[4].bar(np.arange(len(methods)), height= jis ,tick_label=names)\n",
    "ax[4].errorbar(np.arange(len(methods)),jis,yerr=deltas_jis,fmt='.k',capsize=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[1].set_ylabel('$recall$ $(avg)$',fontsize=15)\n",
    "ax[0].set_ylabel('$precision$ $(avg)$',fontsize=15)\n",
    "ax[2].set_ylabel('$F_1-score$ $(avg)$',fontsize=15)\n",
    "ax[3].set_ylabel('$G. Recall$ $(avg)$',fontsize=15)\n",
    "ax[4].set_ylabel('$J. S. Index$ $(avg)$',fontsize=15)\n",
    "\n",
    "\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[1].set_ylim(0,1)\n",
    "ax[2].set_ylim(0,1)\n",
    "ax[3].set_ylim(0,1)\n",
    "ax[4].set_ylim(0,1)\n",
    "\n",
    "# fig.suptitle('Testing' if testing else 'Training')\n",
    "\n",
    "\n",
    "for plot in ax:\n",
    "\n",
    "    for tick in plot.xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(10) \n",
    "    for tick in plot.yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(10) \n",
    "\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "\n",
    "\n",
    "# horizontal line in MOGP value\n",
    "ax[0].axhline(y=p[-12], c='k',ls=':')\n",
    "ax[1].axhline(y=r[-12], c='k',ls=':')\n",
    "ax[2].axhline(y=f1[-12], c='k',ls=':')\n",
    "ax[3].axhline(y=gr[-12], c='k',ls=':')\n",
    "ax[4].axhline(y=jis[-12], c='k',ls=':')\n",
    "\n",
    "fig.savefig(f'performanceComparisonDMOZ_Testing.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING\n",
    "abrir archivos del dir queries-CSVs para levantar las queries generadas en el entrenamiento para testear con la matriz de testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in topics:\n",
    "    print(\"topic \", t)\n",
    "    \n",
    "    df = pd.read_csv (r'queries-CSVs/{}_queries.csv'.format(t))\n",
    "    print (df[\"query_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DMOZ weighting methods.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
